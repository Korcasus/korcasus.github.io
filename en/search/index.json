[{"content":" Info\nThis article was originally written in Korean and has been translated using ChatGPT.\nOverview Hive is a data warehouse software that allows for reading and writing large volumes of data stored in Hadoop using SQL syntax.\nIn simple terms, it can be thought of as a large-scale data processing database operating on Hadoop.\nBy default, it supports HDFS and offers an open DataSourcing feature that allows access to Hbase or other DBMS. It also supports Kafka, which is widely used as a Message Queue. In this series, I will document the configuration for integrating Hive with Kafka, as well as the process of reading and writing messages.\nIn this article, the focus is on the necessary configurations and essential information for use.\nIf you are interested in usage examples, please read the next article.\nThis was written based on the official document\nAlthough it is not stated in the document, I intend to detail additional information required for use and examples that I personally tested.\nThis was done in Hive version 3.x.\nInstall In fact, no separate installation is required.\nAs you may know if you\u0026rsquo;ve looked at the official documentation, it\u0026rsquo;s already built into Hive.\nWhile it might be hard to imagine, you can integrate with Kafka using SQL.\nCreate Table To integrate Hive and Kafka, a linkage must be created.\nJust as in Spring, you can easily create a connection using Binder and Binding, in Hive, you can do this using StorageHandler.\nYou can specify StorageHandler when defining a table. Additionally, with some more configurations, you can use it seamlessly.\nHowever, there are a few things to be cautious of when creating a table in Hive that integrates with Kafka.\nThe first thing is that you must create it as an EXTERNAL table.\nSince the method we will be using involves Hive accessing and using the data stored in Kafka, you need to add the EXTERNAL keyword.\nThe second thing is that you must specify the KafkaStorageHandler.\nWhen defining the table, it should include STORED BY 'org.apache.hadoop.hive.kafka.KafkaStorageHandler'.\nOnly with this specified can you integrate and use Kafka.\nThe third thing is that you need to configure the table.\nUsing the TBLPROPERTIES keyword, you can set the properties for the table.\nWithin this, you must write the configuration necessary for integrating with Kafka.\nBy applying all three points and writing the DDL, it can be crafted as shown in the example below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 CREATE EXTERNAL TABLE kafka_table ( `timestamp` TIMESTAMP, `page` STRING, `newPage` BOOLEAN, `added` INT, `deleted` BIGINT, `delta` DOUBLE ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES ( \u0026#34;kafka.topic\u0026#34; = \u0026#34;test-topic\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;localhost:9092\u0026#34; ); Kafka metadata Messages stored in Kafka consist not only of the Payload but also have four pieces of metadata: Key, Partition, Offset, and Timestamp.\nThese data can also be used in Hive.\nThe previously mentioned KafkaStorageHandler, used when defining the table, automatically registers these metadata as columns.\n__key (byte array) __partition (int32) __offset (int64) __timestamp (int64) I will elaborate on the usage methods in the next article where I\u0026rsquo;ve written real-use examples.\nPrecautions When Using Avro Format If you are using the Avro format through the Confluent Connector, you need to remove 5 bytes from the message.\nOf these 5 bytes, 1 byte is the magic byte, and 4 bytes correspond to the schema id of the schema registry.\nThis can be resolved by setting \u0026quot;avro.serde.type\u0026quot;=\u0026quot;skip\u0026quot; and \u0026quot;avro.serde.skip.bytes\u0026quot;=\u0026quot;5\u0026quot;.\nIf you are using avro, it would be beneficial to thoroughly refer to the guide for detailed information.\nSupport for the confluent avro format seems to be added in Hive version 4.0.\nIf you are using version 3.x, it would be good to be cautious. (Because I had a hard time with it\u0026hellip;)\nSerde(Serializer, Deserializer) If you\u0026rsquo;ve ever sent data from an Application to Kafka, you\u0026rsquo;ve probably had experience specifying a serializer and deserializer.\nSimilarly, Hive also offers various serializers and deserializers.\nSupported Serializers and Deserializers description org.apache.hadoop.hive.serde2.JsonSerDe JSON org.apache.hadoop.hive.serde2.OpenCSVSerde CSV org.apache.hadoop.hive.serde2.avro.AvroSerDe AVRO org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe binary org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe Plain Text The method of specification is simpler than you might think.\nYou can easily apply it by adding it to TBLPROPERTIES.\nFor instance, if the data format you wish to read from Kafka is JSON or if you want to send data in JSON format, you can add it as shown below.\n\u0026quot;kafka.serde.class\u0026quot; = \u0026quot;org.apache.hadoop.hive.serde2.JsonSerDe\u0026quot;\nAuthentication Kafka also offers a feature that requires authentication to grant permissions to read and write data.\nThe authentication details needed for using Kafka can also be provided through Hive.\nIn my case, I didn\u0026rsquo;t add any authentication and used it, so I will document it as an instance without authentication. If you are in a situation where you need authentication, please refer to and proceed with the official documentation.\nIf you haven\u0026rsquo;t added the TBLPROPERTIES value for authentication, an error occurs when you attempt to connect.\nYou might think, \u0026ldquo;If I haven\u0026rsquo;t set it up, shouldn\u0026rsquo;t it naturally be usable without authentication?\u0026rdquo; (That\u0026rsquo;s what I thought.)\nBy default, Hive tries to authenticate using an id and password.\nTherefore, you have to add a configuration to use it without authentication.\nThe official documentation didn\u0026rsquo;t provide a specific solution for this problem, so it was a challenging aspect for me.\nTo get straight to the point, you can resolve this by adding TBLPROPERTY.\nIn the case of a producer : \u0026quot;kafka.producer.security.protocol\u0026quot; = \u0026quot;PLAINTEXT\u0026quot; In the case of a consumer : \u0026quot;kafka.consumer.security.protocol\u0026quot; = \u0026quot;PLAINTEXT\u0026quot; If you want to delve deeper into authentication, it would be helpful to refer to this document.\nAs I\u0026rsquo;ll mention later, if you set kafka. as a prefix when configuring, you can pass the configuration value to Kafka.\nKey Serializer The Serializers I explained earlier were Value Serializers.\nThough more are provided than you might expect, conversely, only the Byte Serializer is supported for Key Serializer.\nEven if you want to change and use a different Serializer, it\u0026rsquo;s not possible to do so.\nI\u0026rsquo;m not sure why it was implemented this way, but you can understand the details by looking at the code.\nAs you might be curious, I\u0026rsquo;m also providing a link.\nproducer consumer Additionally Beyond what has been written so far, there may be additional configurations you\u0026rsquo;d like to add when using Kafka.\nAlthough it\u0026rsquo;s not described in the official documentation, you can pass configuration values to be used in Kafka.\nIn the case of a producer “kafka.producer.{property}” = “{value}” In the case of a consumer “kafka.consumer.{property}” = “{value}” By adding to TBLPROPERTIES, the configuration values are passed to Kafka, allowing you to apply your desired settings. If you\u0026rsquo;re curious about how the configuration values are passed or the implemented code, it would be helpful to refer to the link below.\nproducer consumer Summary Until now, we\u0026rsquo;ve looked into the necessary configurations and helpful information for reading and writing to Kafka from Hive.\nIn the next article, I will document the process of reading messages from Kafka in Hive.\nThank you.\n","date":"2022-12-01T22:44:01+09:00","permalink":"https://korcasus.github.io/en/p/hive-kafka-integration1/","title":"Hive Kafka Integration"}]