[{"content":"How to Build Fat Jar The method varies depending on the type of build tool you are using.\nFor Maven, you should use the maven-assembly-plugin,\nwhile for Gradle, you need to use the Shadow Plugin.\nIn most cases, using one of these two plugins will solve the problem. However, Spring Boot requires additional handling due to its different execution method.\nBoot-Jar This is the officially recommended approach in Spring.\nAccording to the explanation, Java does not provide a method to load nested JAR files, which can be a problem in environments where you need to run without extracting them.\nTherefore, you should use a Shaded Jar (Fat Jar). It collects all classes from all JAR files, packages them into a single JAR file.\nHowever, as it becomes challenging to see the libraries that exist in the actual application, Spring provides an alternative method.\nIf you\u0026rsquo;re curious about how this is implemented in detail, please refer to the official documentation.\nConfiguration 1. Adding Modules implementation 'org.springframework.boot:spring-boot-loader'\nThe mentioned modules help in creating Spring Boot as an Executable Jar or War.\n2. Modify Manifest Because it should be executed through JarLauncher, Manifest needs to be modified.\nThe contents should exist in META-INF/MANIFEST.MF and should be recorded during the build.\nThe official documentation provides the following guidance:\n1 2 Main-Class: org.springframework.boot.loader.JarLauncher Start-Class: com.mycompany.project.MyApplication In my case, I applied it as follows:\n1 2 3 4 5 6 jar { manifest { attributes \u0026#34;Main-Class\u0026#34;: \u0026#34;org.springframework.boot.loader.JarLauncher\u0026#34; attributes \u0026#34;Start-Class\u0026#34;: \u0026#34;com.mycompany.team.batch.SparkSpringBatchClient\u0026#34; } } Issues When executed using the java -jar xxx.jar command, it works as expected.\nHowever, problems occur when running it within Spark.\nThe issue stemmed from a dependency version mismatch.\nAt my current workplace, we\u0026rsquo;re running Spark version 2.4, and the bundled Gson version was older than what the application required.\nConsequently, the application failed to run due to the utilization of the outdated Gson library.\nI explored options for forcibly using the latest version of Gson.\nThe initial approach involved specifying the dependency version to use.\n(For your information, we are already using a relatively up-to-date Gson version without employing this method.)\nHowever, it didn\u0026rsquo;t work due to Spark\u0026rsquo;s preference for its embedded libraries.\nThe second method is outlined in the official Spark documentation. spark.driver.userClassPathFirst\nspark.executor.userClassPathFirst\nBoth of these variables have a default value of False.\nThe documentation specifies that the user-provided --jars option is given higher priority.\nThis can result in added complexity, particularly when managing an increasing number of jars, and it might introduce issues with experimental functionality.\nSince it didn\u0026rsquo;t deliver the intended behavior, and it wasn\u0026rsquo;t our original intent, we have ceased its usage.\nTo resolve this issue, you should utilize relocation (renaming) of dependencies, which is supported by the Shadow Plugin.\nShadow Plugin It consolidates the project\u0026rsquo;s dependency classes and resources into a single JAR.\nThis offers two distinct advantages:\nIt generates a deployable executable JAR. It bundles and relocates common dependencies from the library to prevent classpath conflicts. Referring to a more comprehensive description of the benefits would be beneficial(written in korean).\nAdd Option For Spring To build a Spring Application with the Shadow Plugin, you need additional options.\nThe following content is being added.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import com.github.jengelman.gradle.plugins.shadow.transformers.* shadowJar { // Required for Spring mergeServiceFiles() append \u0026#39;META-INF/spring.handlers\u0026#39; append \u0026#39;META-INF/spring.schemas\u0026#39; append \u0026#39;META-INF/spring.tooling\u0026#39; transform(PropertiesFileTransformer) { paths = [\u0026#39;META-INF/spring.factories\u0026#39; ] mergeStrategy = \u0026#34;append\u0026#34; } } Detailed information can be found in the Git Issue.\nRelocating Packages This is a feature used when duplicate dependencies exist in the classpath, preventing the intended version from being used. It is a common occurrence in Hadoop and Spark.\nThis idea is simple. It involves identifying conflicting dependencies within the project and then changing their names (paths).\nThis prevents issues from arising since different named dependencies are used.\nThe implementation method is simple.\n1 2 3 4 // Relocating a Package shadowJar { relocate \u0026#39;junit.framework\u0026#39;, \u0026#39;shadow.junit\u0026#39; } Change junit.framework to shadow.junit and apply it as follows, as needed.\n1 2 relocate \u0026#39;com.google.gson\u0026#39;, \u0026#39;shadow.google.gson\u0026#39; relocate \u0026#39;com.fasterxml.jackson\u0026#39;, \u0026#39;shadow.fasterxml.jackson\u0026#39; If you wish to examine relocation in the Shadow Plugin, please refer to the link.\nModify the classpath. In reality, I had hoped that everything would run smoothly up to this point.\nHowever, when I attempt to execute java -jar project-all.jar, I can detect an anomaly.\nAlthough Spring itself started, I observed that the Main Application did not execute.\nWhen I investigated, the root cause was not immediately apparent.\nFortunately, through a process of trial and error, I was able to identify the issue, and it was related to the following link\nAt first, it was built with only the runtimeClassPath by default, but when I included the compileClassPath as follows, the problem was resolved.\n1 configurations = [project.configurations.compileClasspath, project.configurations.productionRuntimeClasspath] As problem resolution was the primary focus, I didn\u0026rsquo;t investigate why including compileClassPath was necessary for it to function correctly.\nConclusion I\u0026rsquo;ve explored how to build a Fat Jar (Shaded Jar) when using Spring.\nWhile Boot Jar is the recommended method by Spring and offers the advantage of simplicity, it can be challenging to resolve Dependency conflicts in situations like using a Hadoop cluster.\nThe usage of the Shadow Plugin is a commonly employed method.\nHowever, when applying it to Spring, you will need to write additional scripts. Nevertheless, this approach can help avoid the problems experienced with Boot Jar.\nWhen dealing with Spark or Hadoop clusters, it is advisable to use the Shadow Plugin.\nHowever, if you do not encounter these issues, it is recommended to use it based on your specific circumstances.\nReference Spring Official Guide : https://docs.spring.io/spring-boot/docs/current/reference/html/executable-jar.html#appendix.executable-jar.alternatives offical document translated in korean Article : https://wordbe.tistory.com/entry/Spring-Boot-2-Executable-JAR-스프링-부트-실행 Spring fat jar Git Issue for shadow plugin : https://github.com/spring-projects/spring-boot/issues/1828#issuecomment-231104288 StackOverflow, which was helpful for gaining an overall understanding : https://stackoverflow.com/questions/51206959/classnotfound-when-submit-a-spring-boot-fat-jar-to-spark ","date":"2023-02-20T23:16:51+09:00","permalink":"https://korcasus.github.io/en/p/spring-fat-jar-for-spark/","title":"Spring Fat-jar for Spark"},{"content":" Info\nThis article was originally written in Korean and has been translated using ChatGPT.\nOverview In contrast to the previous discussions, I\u0026rsquo;ll delve into the process of transmitting messages from Hive to Kafka.\nViewed from Kafka\u0026rsquo;s standpoint, Hive takes on the role of a Producer.\nDrawback While transmitting from Hive to Kafka, core functionalities are catered for, making it suitable for most scenarios.\nYet, there are a few unsupported features, including the Header.\nIf the need arises to incorporate a Header, considering an alternative application over Hive is advised.\nKafkaWritable Kafka Record official document Considerations for Kafka metadata When transmitting a message from the Producer to Kafka, you can incorporate associated metadata.\nIf the metadata is omitted, Kafka is understood to generate it automatically.\nThis principle holds true for Hive as well. Since __timestamp, __partition, and __offset are attributes tracked by Kafka, writing in the mentioned manner allows Kafka to automatically populate these fields.\n__timestamp : -1 __partition : null __offset : -1 However, the available value range for __key is limited, making the choices restricted, but careful configuration is advised.\nKafka utilizes the key for determining the partition, which can impact message distribution. Additionally, the application of the key depends on the cleanup.policy configuration.\nIn Kafka, values are assigned to the same partition based on the key. Therefore, when configured as log.cleanup.policy = compact, it\u0026rsquo;s advantageous to input the key as a byte array.\nInteger to byte array : cast(cast(1 AS STRING) AS BINARY) String to byte array : cast(\u0026lsquo;1\u0026rsquo; AS BINARY) JSON test case I\u0026rsquo;ll proceed to transmit a message in JSON format to Kafka.\nCreating a topic I\u0026rsquo;m creating a topic to store test messages.\nBelow is an example that leverages a script bundled with the Kafka installation to establish a topic.\n./kafka-topics.sh --create --topic \u0026quot;${토픽토픽}\u0026quot; --replication-factor 1 --partitions 1 --bootstrap-server \u0026quot;{servers}\u0026quot;\nCreating a table Having set up a topic for transmission to Kafka, we\u0026rsquo;ll now establish a table responsible for the actual transmission process.\nEnable the Kafka Handler by designating the KafkaStorageHandler.\nThe configuration for TBLPROPERTIES aligns closely with the guidelines previously mentioned.\nBut, given its role in transmitting messages, set it as a producer instead of a consumer.\nThe example provided below can serve as a reference.\n1 2 3 4 5 6 7 8 9 10 11 CREATE EXTERNAL TABLE test_export_to_kafka( `id` STRING, `value` STRING ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES( \u0026#34;kafka.serde.class\u0026#34; = \u0026#34;org.apache.hadoop.hive.serde2.JsonSerDe\u0026#34;, \u0026#34;kafka.topic\u0026#34; = \u0026#34;${토픽토픽}\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;${servers}\u0026#34;, \u0026#34;kafka.producer.security.protocol\u0026#34; = \u0026#34;PLAINTEXT\u0026#34; ); Sending test data We will proceed to insert data into the table we created.\nIt\u0026rsquo;s essential to provide values for the columns explicitly mentioned in the table, which are id and value.\nRegarding Kafka Metadata, you can include them in the following order: key, partition, offset, and timestamp.\ninsert into test_export_to_kafka values ('1', '1', null, null, -1, -1);\nTest case for ONLY LONG TYPE This time, we will attempt to send data in PLAIN TEXT format, rather than JSON.\nCreating a topic It is similar to the JSON test case I explained earlier.\nWe are creating a topic to store test messages.\nThe example below creates the topic using a script provided with Kafka installation.\n./kafka-topics.sh --create --topic \u0026quot;${topic}\u0026quot; --replication-factor 1 --partitions 1 --bootstrap-server \u0026quot;{servers}\u0026quot;\nCreating a table Kafka Handler and TBLPROPERTIES are configured in the same manner as previously described in the test case.\nHowever, this time, as we are using the PLAIN TEXT format, we will create only one column.\n1 2 3 4 5 6 7 8 9 10 CREATE EXTERNAL TABLE test_export_to_kafka_long( id bigint ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES( \u0026#34;kafka.serde.class\u0026#34; = \u0026#34;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\u0026#34;, \u0026#34;kafka.topic\u0026#34; = \u0026#34;${topic}\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;${servers}\u0026#34;, \u0026#34;kafka.producer.security.protocol\u0026#34; = \u0026#34;PLAINTEXT\u0026#34; ); Sending test data We will proceed to insert data into the table we created.\nIt is mandatory to include a value for the id column, as specified in the table.\nFor the remaining columns, you can add Kafka Metadata information as done previously.\ninsert into test_export_to_kafka_long values (1, null, null, -1, -1);\nTesting messages with the same key being placed in the same partition When creating an application to be used as a Producer, you have the option to specify a Partitioner that determines the partition where Kafka will store the message.\nHowever, in Hive, there is no separate provision to specify a Partitioner.\nIt should be assumed that the Default Partitioner is in use.\nIf the message keys are identical, they should be placed in the same partition.\nHowever, since this is the first attempt to transmit through Hive, it was necessary to verify whether they are indeed directed to the same partition.\nTests were conducted to ascertain this.\nFollowing the transmission of multiple messages with different keys, we proceed to send a message with the same key as the previous one but with different content.\nYou can access detailed information through the query provided below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 CREATE EXTERNAL TABLE test_export_to_kafka_partition( id INT, value STRING ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES( \u0026#34;kafka.serde.class\u0026#34; = \u0026#34;org.apache.hadoop.hive.serde2.JsonSerDe\u0026#34;, \u0026#34;kafka.topic\u0026#34; = \u0026#34;${topic}\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;${servers}\u0026#34;, \u0026#34;kafka.producer.security.protocol\u0026#34; = \u0026#34;PLAINTEXT\u0026#34; ); ./kafka-topics.sh --create --topic ${topic} --bootstrap-server {servers} --config \u0026#34;cleanup.policy=compact\u0026#34; --partitions 4 --replication-factor 2 INSERT INTO TABLE test_export_to_kafka_partition VALUES (1, \u0026#39;1\u0026#39;, CAST(\u0026#39;1\u0026#39; AS BINARY), NULL, -1, -1), (\u0026#34;2\u0026#34;, \u0026#34;2\u0026#34;, CAST(\u0026#39;2\u0026#39; AS binary), -1, -1, -1), (\u0026#34;3\u0026#34;, \u0026#34;3\u0026#34;, CAST(\u0026#39;3\u0026#39; AS binary), -1, -1, -1), (\u0026#34;4\u0026#34;, \u0026#34;4\u0026#34;, CAST(\u0026#39;4\u0026#39; AS binary), -1, -1, -1); CREATE TEMPORARY TABLE test_partition(id INT, value STRING); INSERT INTO TABLE test_partition VALUES (2, \u0026#39;2\u0026#39;), (3, \u0026#39;3\u0026#39;), (4, \u0026#39;4\u0026#39;); INSERT INTO table test_export_to_kafka_partition SELECT id, value, CAST(CAST(id AS string) AS binary) AS `_key`, NULL AS `_partition`, -1, -1 FROM test_partition; Even though you didn\u0026rsquo;t provide a result screen, the test confirmed that data with the same key value is stored in the same partition.\nConclusion I have confirmed through this article that it is possible to transmit Hive Table data to Kafka.\nIn fact, there are several methods to transfer data, and there are other good alternatives available. (For example, you can connect to HiveServer2 via JDBC or use Spark.)\nWhile I didn\u0026rsquo;t explicitly mention it in the feature description, you might have noticed that there are some limitations. If you execute queries that involve JOINs with other tables and attempt to send data immediately, errors may occur, as you pointed out. To overcome this issue, you should create temporary tables to store the data and then write queries that use these temporary tables for data transmission.\nThere may be other constraints that I haven\u0026rsquo;t identified.\nDespite these constraints, a significant advantage is that you don\u0026rsquo;t need to create a separate application.\nI hope this information serves as an opportunity for you to realize that this method can be one of the choices.\nThank you for reading this far.\n","date":"2022-12-01T22:45:04+09:00","permalink":"https://korcasus.github.io/en/p/hive-kafka-integration3/","title":"Write message to kafka using hive"},{"content":" Info\nThis article was originally written in Korean and has been translated using ChatGPT.\nOverview In our previous post, we delved into the configurations necessary for integrating Kafka with Hive.\nMoving forward, I\u0026rsquo;ll elaborate on the processes of reading from and writing to Kafka.\nIn this piece, we\u0026rsquo;ll kick off by pulling data from Kafka.\nYou can think of it as leveraging Hive in the role of a Kafka Consumer for better clarity.\nOne distinction to note is that it doesn\u0026rsquo;t get assigned as a Consumer Group, unlike typical Consumers.\nJSON This tutorial proceeds under the assumption that the data being read from Kafka is formatted in JSON.\nSample data I\u0026rsquo;ll walk you through using the following data structure as our reference example. Feel free to expand by adding more fields based on your requirements.\n1 2 3 4 { \u0026#34;id\u0026#34;: 12345678, \u0026#34;value:\u0026#34;: \u0026#34;hello\u0026#34; } Table Definition Let\u0026rsquo;s define a table mirroring the structure of the JSON schema.\nThe \u0026lsquo;id\u0026rsquo; field carries an integer value, while the \u0026lsquo;value\u0026rsquo; field is a string.\nWithin Hive, integer values can be represented using the INT or BIGINT data types. Strings, on the other hand, are characterized as STRING.\nDrawing from the provided information, the DDL query can be constructed as follows.\n1 2 3 4 5 6 7 8 9 10 11 CREATE EXTERNAL TABLE test_export_to_kafka( `id` BIGINT, `value` STRING ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES( \u0026#34;kafka.serde.class\u0026#34; = \u0026#34;org.apache.hadoop.hive.serde2.JsonSerDe\u0026#34;, \u0026#34;kafka.topic\u0026#34; = \u0026#34;etl.hive.catalog.used\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;{broker servers}\u0026#34;, \u0026#34;kafka.consumer.security.protocol\u0026#34; = \u0026#34;PLAINTEXT\u0026#34; ); The table name, test_export_to_kafka, can be adjusted to your preference. Columns within the table should be outlined in alignment with the JSON structure being ingested.\nEnsure that column names mirror the JSON keys, and column data types are consistent with the corresponding JSON values.\nGiven that Hive offers a broader spectrum of data types compared to JSON, I\u0026rsquo;d recommend referring to this documentation to pick the most suitable type.\nBy assigning \u0026quot;kafka.serde.class\u0026quot; = \u0026quot;org.apache.hadoop.hive.serde2.JsonSerDe\u0026quot;, you\u0026rsquo;re enabling Hive to deserialize JSON data.\nNo other particulars in TBLPROPERTIES demand attention.\nSimply adjust kafka.topic to the desired Kafka topic name.\nREAD 1 2 SELECT * FROM test_export_to_kafka If everything has been configured correctly, you should be able to view the messages within the topic.\nOn the other hand, if there\u0026rsquo;s a misconfiguration, you might encounter error messages or see columns rendered as NULL.\nEnsure that your setup doesn\u0026rsquo;t deviate from the guidelines we\u0026rsquo;ve discussed up to this point.\nTEXT This tutorial focuses on reading PLAIN TEXT, as opposed to structured data such as JSON.\nSample data Given it\u0026rsquo;s PLAIN TEXT, the example is straightforward.\nWe\u0026rsquo;ll utilize basic numerical values for demonstration purposes.\n1 2 3 1234 5678 91011 Table Definition 1 2 3 4 5 6 7 8 9 10 CREATE EXTERNAL TABLE test_export_to_kafka( `id` BIGINT ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES( \u0026#34;kafka.serde.class\u0026#34; = \u0026#34;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\u0026#34;, \u0026#34;kafka.topic\u0026#34; = \u0026#34;etl.hive.catalog.used\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;{servers}\u0026#34;, \u0026#34;kafka.consumer.security.protocol\u0026#34; = \u0026#34;PLAINTEXT\u0026#34; ); This aligns closely with the JSON test case we touched on previously.\nThe distinctions lie in the distinct designations for Serde and the columns.\nFor the Serde configuration, switch to \u0026quot;kafka.serde.class\u0026quot; = \u0026quot;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\u0026quot;.\nREAD 1 2 SELECT * FROM test_export_to_kafka If everything has been configured properly, you should be able to view the messages from the topic.\nHowever, should there be a misconfiguration, you might either see error messages or columns displaying as NULL.\nPlease ensure your configurations align with the guidelines discussed up to now.\nUtilizing Kafka Metadata In Kafka, alongside the primary messages, associated metadata is also stored.\nWhen shaping a table, this metadata seamlessly gets incorporated as columns. Consequently, when Hive fetches a message, it concurrently retrieves this accompanying metadata.\nGiven that metadata is configured as columns, it opens up avenues for their application within SQL conditional statements.\nFor illustration, you might opt to pull data entered post a designated timestamp, or exclusively data residing in a particular partition.\n\u0026ldquo;For fields like __partition and __offset, their employment of integer types makes them particularly user-friendly.\nex1) __partition = 0 ex2) __offset \u0026gt; 5000 Yet, with __timestamp, it\u0026rsquo;s captured as int64 due to its reliance on the unix_timestamp.\nThis diverges from the typical time notation we\u0026rsquo;re accustomed to, necessitating a conversion and resulting in a less-than-convenient experience.\nFor ease in deploying within conditions, it\u0026rsquo;s advisable to modify and utilize it as illustrated below.\nThe function to employ here would be unix_timestamp, as demonstrated in the subsequent example.\nunix_timestamp('2022-03-08 00:00:00.000', 'yyyy-MM-dd HH:mm:ss.SSS') * 1000\nFor the first parameter, specify the time you intend to transform. For the second parameter, indicate the time format corresponding to what you specified in the first. With the function in place, it\u0026rsquo;s possible to transform a specific time into unix_timestamp, paving the way for comparison activities involving __timestamp.\nYet, bear in mind that the __timestamp fetched via Hive operates on a microsecond scale. As a result, some additional computations are required to align the units, which is why a multiplication by 1000 was executed. Here\u0026rsquo;s an illustrative example.\nex) unix_timestamp('2022-03-08 00:00:00.000', 'yyyy-MM-dd HH:mm:ss.SSS') * 1000 \u0026lt; __timestamp\nSupplementary Example\n1 2 3 4 5 6 7 8 9 -- print to UTC timestamp select FROM_UTC_TIMESTAMP(`__timestamp`, \u0026#39;JST\u0026#39;) from test_kafka_product_received limit 1; -- print timestamp, unixtime to timestamp select current_timestamp(), from_unixtime(unix_timestamp(), \u0026#39;yyyy-MM-dd HH:mm:ss.SSS\u0026#39;); -- print converted unixtime select unix_timestamp(\u0026#39;2022-03-07 14:00:00.000\u0026#39;, \u0026#39;yyyy-MM-dd HH:mm:ss.SSS\u0026#39;); select unix_timestamp(\u0026#39;2022-03-07 06:11:41\u0026#39;, \u0026#39;yyyy-MM-dd HH:mm:ss\u0026#39;); 정리 With the insights from this piece, you\u0026rsquo;re now equipped to fetch JSON or PLAIN-TEXT data from Kafka utilizing Hive.\nLeveraging metadata for filtering can enable effortless extraction of Kafka messages that align with specific criteria.\nA noted limitation is the inability to join this with other tables housed within Hive (I faced authentication-related challenges during my attempts).\nSuch constraints might curtail its full potential, but with further enhancements, its utility could significantly rise.\nIn the subsequent post, I\u0026rsquo;ll delve into the procedure of transmitting data from Hive to Kafka.\nMany thanks.\n","date":"2022-12-01T22:45:01+09:00","permalink":"https://korcasus.github.io/en/p/hive-kafka-integration2/","title":"Read message from kafka using hive"},{"content":" Info\nThis article was originally written in Korean and has been translated using ChatGPT.\nOverview Hive is a data warehouse software that allows for reading and writing large volumes of data stored in Hadoop using SQL syntax.\nIn simple terms, it can be thought of as a large-scale data processing database operating on Hadoop.\nBy default, it supports HDFS and offers an open DataSourcing feature that allows access to Hbase or other DBMS. It also supports Kafka, which is widely used as a Message Queue. In this series, I will document the configuration for integrating Hive with Kafka, as well as the process of reading and writing messages.\nIn this article, the focus is on the necessary configurations and essential information for use.\nIf you are interested in usage examples, please read the next article.\nThis was written based on the official document\nAlthough it is not stated in the document, I intend to detail additional information required for use and examples that I personally tested.\nThis was done in Hive version 3.x.\nInstall In fact, no separate installation is required.\nAs you may know if you\u0026rsquo;ve looked at the official documentation, it\u0026rsquo;s already built into Hive.\nWhile it might be hard to imagine, you can integrate with Kafka using SQL.\nCreate Table To integrate Hive and Kafka, a linkage must be created.\nJust as in Spring, you can easily create a connection using Binder and Binding, in Hive, you can do this using StorageHandler.\nYou can specify StorageHandler when defining a table. Additionally, with some more configurations, you can use it seamlessly.\nHowever, there are a few things to be cautious of when creating a table in Hive that integrates with Kafka.\nThe first thing is that you must create it as an EXTERNAL table.\nSince the method we will be using involves Hive accessing and using the data stored in Kafka, you need to add the EXTERNAL keyword.\nThe second thing is that you must specify the KafkaStorageHandler.\nWhen defining the table, it should include STORED BY 'org.apache.hadoop.hive.kafka.KafkaStorageHandler'.\nOnly with this specified can you integrate and use Kafka.\nThe third thing is that you need to configure the table.\nUsing the TBLPROPERTIES keyword, you can set the properties for the table.\nWithin this, you must write the configuration necessary for integrating with Kafka.\nBy applying all three points and writing the DDL, it can be crafted as shown in the example below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 CREATE EXTERNAL TABLE kafka_table ( `timestamp` TIMESTAMP, `page` STRING, `newPage` BOOLEAN, `added` INT, `deleted` BIGINT, `delta` DOUBLE ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES ( \u0026#34;kafka.topic\u0026#34; = \u0026#34;test-topic\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;localhost:9092\u0026#34; ); Kafka metadata Messages stored in Kafka consist not only of the Payload but also have four pieces of metadata: Key, Partition, Offset, and Timestamp.\nThese data can also be used in Hive.\nThe previously mentioned KafkaStorageHandler, used when defining the table, automatically registers these metadata as columns.\n__key (byte array) __partition (int32) __offset (int64) __timestamp (int64) I will elaborate on the usage methods in the next article where I\u0026rsquo;ve written real-use examples.\nPrecautions When Using Avro Format If you are using the Avro format through the Confluent Connector, you need to remove 5 bytes from the message.\nOf these 5 bytes, 1 byte is the magic byte, and 4 bytes correspond to the schema id of the schema registry.\nThis can be resolved by setting \u0026quot;avro.serde.type\u0026quot;=\u0026quot;skip\u0026quot; and \u0026quot;avro.serde.skip.bytes\u0026quot;=\u0026quot;5\u0026quot;.\nIf you are using avro, it would be beneficial to thoroughly refer to the guide for detailed information.\nSupport for the confluent avro format seems to be added in Hive version 4.0.\nIf you are using version 3.x, it would be good to be cautious. (Because I had a hard time with it\u0026hellip;)\nSerde(Serializer, Deserializer) If you\u0026rsquo;ve ever sent data from an Application to Kafka, you\u0026rsquo;ve probably had experience specifying a serializer and deserializer.\nSimilarly, Hive also offers various serializers and deserializers.\nSupported Serializers and Deserializers description org.apache.hadoop.hive.serde2.JsonSerDe JSON org.apache.hadoop.hive.serde2.OpenCSVSerde CSV org.apache.hadoop.hive.serde2.avro.AvroSerDe AVRO org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe binary org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe Plain Text The method of specification is simpler than you might think.\nYou can easily apply it by adding it to TBLPROPERTIES.\nFor instance, if the data format you wish to read from Kafka is JSON or if you want to send data in JSON format, you can add it as shown below.\n\u0026quot;kafka.serde.class\u0026quot; = \u0026quot;org.apache.hadoop.hive.serde2.JsonSerDe\u0026quot;\nAuthentication Kafka also offers a feature that requires authentication to grant permissions to read and write data.\nThe authentication details needed for using Kafka can also be provided through Hive.\nIn my case, I didn\u0026rsquo;t add any authentication and used it, so I will document it as an instance without authentication. If you are in a situation where you need authentication, please refer to and proceed with the official documentation.\nIf you haven\u0026rsquo;t added the TBLPROPERTIES value for authentication, an error occurs when you attempt to connect.\nYou might think, \u0026ldquo;If I haven\u0026rsquo;t set it up, shouldn\u0026rsquo;t it naturally be usable without authentication?\u0026rdquo; (That\u0026rsquo;s what I thought.)\nBy default, Hive tries to authenticate using an id and password.\nTherefore, you have to add a configuration to use it without authentication.\nThe official documentation didn\u0026rsquo;t provide a specific solution for this problem, so it was a challenging aspect for me.\nTo get straight to the point, you can resolve this by adding TBLPROPERTY.\nIn the case of a producer : \u0026quot;kafka.producer.security.protocol\u0026quot; = \u0026quot;PLAINTEXT\u0026quot; In the case of a consumer : \u0026quot;kafka.consumer.security.protocol\u0026quot; = \u0026quot;PLAINTEXT\u0026quot; If you want to delve deeper into authentication, it would be helpful to refer to this document.\nAs I\u0026rsquo;ll mention later, if you set kafka. as a prefix when configuring, you can pass the configuration value to Kafka.\nKey Serializer The Serializers I explained earlier were Value Serializers.\nThough more are provided than you might expect, conversely, only the Byte Serializer is supported for Key Serializer.\nEven if you want to change and use a different Serializer, it\u0026rsquo;s not possible to do so.\nI\u0026rsquo;m not sure why it was implemented this way, but you can understand the details by looking at the code.\nAs you might be curious, I\u0026rsquo;m also providing a link.\nproducer consumer Additionally Beyond what has been written so far, there may be additional configurations you\u0026rsquo;d like to add when using Kafka.\nAlthough it\u0026rsquo;s not described in the official documentation, you can pass configuration values to be used in Kafka.\nIn the case of a producer “kafka.producer.{property}” = “{value}” In the case of a consumer “kafka.consumer.{property}” = “{value}” By adding to TBLPROPERTIES, the configuration values are passed to Kafka, allowing you to apply your desired settings. If you\u0026rsquo;re curious about how the configuration values are passed or the implemented code, it would be helpful to refer to the link below.\nproducer consumer Summary Until now, we\u0026rsquo;ve looked into the necessary configurations and helpful information for reading and writing to Kafka from Hive.\nIn the next article, I will document the process of reading messages from Kafka in Hive.\nThank you.\n","date":"2022-12-01T22:44:01+09:00","permalink":"https://korcasus.github.io/en/p/hive-kafka-integration1/","title":"Hive Kafka Integration"}]