[{"content":" Info\nThis article was originally written in Korean and has been translated using ChatGPT.\nIntroduction In Kubernetes, batch-type applications are catered for.\nThey can be executed as Job or CronJob.\nYet, the platform lacks features for managing execution histories or setting dependencies among various jobs.\nTo ascertain if Argo Workflow and Argo Event fully address these needs, I installed and performed some tests.\nInstall Argo-Event Considering future usage, the installation is done cluster-wide, rather than at the namespace level.\nnamespace ÏÉùÏÑ± 1 kubectl create namespace argo-events Deploy Argo Events, SA, ClusterRoles, Sensor Controller, EventBus Controller and EventSource Controller. 1 2 3 kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/manifests/install.yaml # Install with a validating admission controller kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/manifests/install-validating-webhook.yaml Deploy Eventbus 1 kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/eventbus/native.yaml Argo-workflow Controller And Server 1 2 kubectl create namespace argo kubectl apply -n argo -f https://github.com/argoproj/argo-workflows/releases/download/v3.3.0/install.yaml When you follow the getting-started guide for installation, it gets confined to the namespace.\nSuch a setup might hinder the convenience of using argo-event, so it\u0026rsquo;s advisable to opt for a cluster-wide installation unless there\u0026rsquo;s a compelling reason otherwise.\nInstall CLI 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Download the binary curl -sLO https://github.com/argoproj/argo-workflows/releases/download/v3.3.0/argo-darwin-amd64.gz # Unzip gunzip argo-darwin-amd64.gz # Make binary executable chmod +x argo-darwin-amd64 # Move binary to path mv ./argo-darwin-amd64 /usr/local/bin/argo # Test installation argo version Port-Forwarding (local) 1 kubectl -n argo port-forward deployment/argo-server 2746:2746 For a Remote Cluster setup, configure the Routing by following the instructions in this link.\nFor local setups, opt for the Load Balancer method.\nAuthorization By adapting the approach from this link, you can retrieve an existing token and utilize it for login.\nConcept Architecture Event Source It\u0026rsquo;s a resource designed to process external events.\nSensor The sensor accepts the Event Dependency Set as its input, while the Trigger stipulates the output.\nFunctioning as an Event Dependency Manager, it processes events relayed via the EventBus and subsequently initiates the Trigger.\nEventbus It serves as the TransportLayer, bridging EventSources and Sensor.\nEventSources are responsible for publishing events, whereas the Sensor subscribes to these events and initiates the Trigger.\nTrigger Once the Sensor resolves the Event Dependencies, it signifies the resource or workload that gets executed.\nFeature tested calendar webhook-workflow webhook-k8s-object cross-namespace webhook-auth RBAC Configuration Operating on the assumption that we\u0026rsquo;re utilizing argo-workflow and argo-events, let me elucidate.\nFor argo-workflow, one must provide permissions to inspect the status and logs of the executed pod.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: workflow-role namespace: argo-events rules: - apiGroups: - \u0026#34;\u0026#34; resources: - pods verbs: - get - watch - patch - apiGroups: - \u0026#34;\u0026#34; resources: - pods/log verbs: - get - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: workflow-role-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: workflow-role subjects: - kind: ServiceAccount name: default When intending to utilize Argo-Workflow as a Trigger within Argo Events, the following setup is essential.\nGiven that the object spawned by the Trigger is a Workflow, requisite permissions are needed to access that specific api-resource.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 apiVersion: v1 kind: ServiceAccount metadata: name: operate-workflow-sa namespace: argo-events --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: operate-workflow-role namespace: argo-events rules: - apiGroups: - argoproj.io verbs: - \u0026#34;*\u0026#34; resources: - workflows - workflowtemplates - cronworkflows - clusterworkflowtemplates --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: operate-workflow-role-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: operate-workflow-role subjects: - kind: ServiceAccount name: operate-workflow-sa webhook-workflow Webhooks are used as EventSources, and the Sensor uses argo-workflow as a Trigger.\nAccordingly, to operate, you need to pre-install argo-workflow cluster wide.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: argoproj.io/v1alpha1 kind: EventSource metadata: name: webhook spec: service: ports: - port: 12000 targetPort: 12000 webhook: example: port: \u0026#34;12000\u0026#34; endpoint: /example method: POST 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 apiVersion: argoproj.io/v1alpha1 kind: Sensor metadata: name: webhook spec: template: serviceAccountName: operate-workflow-sa dependencies: - name: test-dep eventSourceName: webhook eventName: example triggers: - template: name: webhook-workflow-trigger k8s: operation: create source: resource: apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: webhook- spec: entrypoint: whalesay arguments: parameters: - name: message value: hello world templates: - name: whalesay inputs: parameters: - name: message container: image: docker/whalesay:latest command: [cowsay] args: [\u0026#34;{{inputs.parameters.message}}\u0026#34;] parameters: - src: dependencyName: test-dep dataKey: body dest: spec.arguments.parameters.0.value calendar This EventSource can execute both Timer and CronJob tasks.\n1 2 3 4 5 6 7 8 9 apiVersion: argoproj.io/v1alpha1 kind: EventSource metadata: name: calendar spec: calendar: example-with-interval: interval: 10s # schedule: \u0026#34;30 * * * *\u0026#34; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 apiVersion: argoproj.io/v1alpha1 kind: Sensor metadata: name: calendar spec: template: serviceAccountName: operate-workflow-sa dependencies: - name: test-dep eventSourceName: calendar eventName: example-with-interval triggers: - template: name: calendar-workflow-trigger k8s: operation: create source: resource: apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: calendar-workflow- spec: entrypoint: whalesay arguments: parameters: - name: message value: hello world templates: - name: whalesay inputs: parameters: - name: message container: image: docker/whalesay:latest command: [cowsay] args: [\u0026#34;{{inputs.parameters.message}}\u0026#34;] parameters: - src: dependencyName: test-dep dataKey: eventTime dest: spec.arguments.parameters.0.value retryStrategy: steps: 3 webhook-k8s-object This setup resembles the Webhook-Workflow. However, what distinguishes it is that the Sensor triggers a k8s object.\nIt\u0026rsquo;s capable of executing Custom Resources, including Pod, Deployment, Job, and CronJob.\nThe available operations are Create, Update, Patch, and Delete.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: argoproj.io/v1alpha1 kind: EventSource metadata: name: k8s-webhook spec: service: ports: - port: 12000 targetPort: 12000 webhook: example: port: \u0026#34;12000\u0026#34; endpoint: /example method: POST 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion: argoproj.io/v1alpha1 kind: Sensor metadata: name: k8s-webhook spec: template: serviceAccountName: operate-k8s-sa dependencies: - name: test-dep eventSourceName: k8s-webhook eventName: example triggers: - template: name: webhook-pod-trigger k8s: operation: create source: resource: apiVersion: v1 kind: Pod metadata: generateName: hello-world- spec: containers: - name: hello-container args: - \u0026#34;hello-world\u0026#34; command: - cowsay image: \u0026#34;docker/whalesay:latest\u0026#34; parameters: - src: dependencyName: test-dep dataKey: body dest: spec.containers.0.args.0 To deploy a Kubernetes Object, you\u0026rsquo;ll need supplementary RBAC permissions.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion: v1 kind: ServiceAccount metadata: name: operate-k8s-sa namespace: argo-events --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: operate-k8s-role namespace: argo-events rules: - apiGroups: - \u0026#34;\u0026#34; verbs: - \u0026#34;*\u0026#34; resources: - pods --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: operate-k8s-role-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: operate-k8s-role subjects: - kind: ServiceAccount name: operate-k8s-sa cross-namespace The RBAC configurations used until now are restricted to individual namespaces. This poses a challenge as every user needs to be familiar with its usage, potentially causing user inconvenience.\nFor cross-namespace functionality, all definitions that were previously set as Role need to be converted to ClusterRole.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion: v1 kind: ServiceAccount metadata: name: operate-k8s-cluster-sa namespace: argo-events --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: operate-k8s-clusterrole namespace: argo-events rules: - apiGroups: - \u0026#34;\u0026#34; verbs: - \u0026#34;*\u0026#34; resources: - pods --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: operate-k8s-clusterrole-binding namespace: argo-events roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: operate-k8s-clusterrole subjects: - kind: ServiceAccount name: operate-k8s-cluster-sa namespace: argo-events The provided ClusterRole example is configured solely for pod creation. If other resources are necessary, you can incorporate them and then proceed to use the role.\nwebhook-auth For security reasons, it may be necessary to incorporate an authentication token when making a Webhook call.\nThis approach entails generating an authentication token and subsequently registering it as a Secret, a type of k8s object.\nBy designating the name of the Secret object, it becomes available for authentication purposes within the Webhook.\n1 2 3 echo -n \u0026#39;af3qqs321f2ddwf1e2e67dfda3fs\u0026#39; \u0026gt; ./token.txt kubectl create secret generic my-webhook-token --from-file=my-token=./token.txt 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: argoproj.io/v1alpha1 kind: EventSource metadata: name: secret-webhook spec: webhook: example: port: \u0026#34;12000\u0026#34; endpoint: /example method: POST authSecret: name: my-webhook-token key: my-token 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion: argoproj.io/v1alpha1 kind: Sensor metadata: name: k8s-webhook spec: template: serviceAccountName: operate-k8s-sa dependencies: - name: test-dep eventSourceName: secret-webhook eventName: example triggers: - template: name: webhook-pod-trigger k8s: operation: create source: resource: apiVersion: v1 kind: Pod metadata: generateName: hello-world- spec: containers: - name: hello-container args: - \u0026#34;hello-world\u0026#34; command: - cowsay image: \u0026#34;docker/whalesay:latest\u0026#34; parameters: - src: dependencyName: test-dep dataKey: body dest: spec.containers.0.args.0 Summary From the tests we\u0026rsquo;ve conducted, it\u0026rsquo;s evident that the emphasis was placed on assessing functional capabilities rather than operational aspects.\nGiven that all necessary resources for Batch can be precisely defined using yaml, it enabled seamless GitOps operations.\nMoreover, with Argo Event\u0026rsquo;s extensive support for various Event Sources, we\u0026rsquo;ve verified that both Webhook and Cron can be utilized to their full potential.\nIn the upcoming article, I\u0026rsquo;ll delve into the operational requirements and discuss our approach to evaluating them.\n","date":"2023-09-10T16:46:32+09:00","permalink":"https://korcasus.github.io/en/p/argo-events-getting-start/","title":"Argo Events Getting Start"},{"content":" Info\nThis article was originally written in Korean and has been translated using ChatGPT.\nIntroduction Given that ES operates as a search engine, it excels in swiftly locating data fitting specific conditions.\nBut, the preconditioning for such capabilities demands an indexing stage, which in turn, consumes significant resources.\nOwing to these limitations, extracting pertinent data from vast datasets becomes challenging.\nIn light of this, alternative data retrieval methods become necessary, with Hadoop often emerging as a primary consideration.\nIf the task of transmitting data to Elasticsearch falls on you or a team member, it\u0026rsquo;s feasible to efficiently store it in HDFS.\nHowever, faced with the constraint of exclusive Elasticsearch usage without control over data transfer, what steps should be taken?\nClearly, the initial step would be to contemplate data extraction from Elasticsearch.\nIn addressing this scenario, I intend to highlight an apt methodology.\nThe procedure for ETLing data from Elasticsearch via a library in Spark will be detailed at an introductory \u0026lsquo;Getting Started\u0026rsquo; level.\nObjective Perform ETL on a specific index from ES to Hive.\nRequired Basic knowledge of Spark is essential. Basic knowledge of Elasticsearch is essential. Environment Scala : 2.12 Spark : 3.x Java : 8 ElasticSearch : 7.x Dependency elasticsearch-hadoop elasticsearch-spark-30_2.12 calcite-core If deploying ElasticSearch version 8, consulting the Release Note for the most suitable version is advised.\nFor the elastic-* library, utilizing version 7.17.0 or above is preferable.\nAn essential patch was incorporated in the 7.17.0 release, warranting this recommendation.\nI remember utilizing the calcite-core during the deserialization process.\nOmitting it can lead to complications when forming a Dataframe.\nGetting Started Connect If you adhere to the guide, you can leverage Spark SQL.\nBut, as Spark SQL deals with structured data, discrepancies in data structures can pose challenges.\nI won\u0026rsquo;t delve into the earlier versions (Spark version \u0026lt; 3.x).\nStarting with Spark version 3, the use of SparkSession becomes the norm.\nDuring the session creation, it\u0026rsquo;s imperative to provide the details of the desired Elasticsearch connection.\nFor a comprehensive understanding, refer to the Configuration documentation.\nTo establish a connection with ES, these two entries are mandatory.\nes.nodes es.port When authentication is deemed necessary or if an HTTPS connection isn\u0026rsquo;t mandated, apply the settings below.\nes.net.http.auth.user es.net.http.auth.pass es.net.ssl For authentication specifics, refer to the provided link.\nShould you operate ES within constrained settings like a WAN environment, there are essential parameters to incorporate.\nOnce this parameter is integrated, it exclusively employs the nodes delineated in es.nodes, bypassing all others.\nes.nodes.wan.only For in-depth details, please refer to the link.\nWhen incorporating all the aforementioned configuration details, the resultant setting appears as below.\n1 2 3 4 5 6 7 8 9 sparkSession = SparkSession.builder() .master(sparkConfiguration.getMaster()) .appName(sparkConfiguration.getAppName()) .config(\u0026#34;spark.es.nodes\u0026#34;, \u0026#34;es-nodes\u0026#34;) .config(\u0026#34;spark.es.port\u0026#34;, \u0026#34;es-port\u0026#34;) .config(\u0026#34;spark.es.net.http.auth.user\u0026#34;, \u0026#34;user\u0026#34;) .config(\u0026#34;spark.es.net.http.auth.pass\u0026#34;, \u0026#34;password\u0026#34;) .config(\u0026#34;spark.es.nodes.wan.only\u0026#34;, true) .config(\u0026#34;spark.es.net.ssl\u0026#34;, true) ElasticSearch Hadoop offers four distinct avenues for Spark integration.\nNative RDD Spark Streaming Spark SQL Spark Structured Streaming I\u0026rsquo;ve categorized the aforementioned four based on their nature.\nStructured vs Unstructured Streaming vs Non-Streaming(Batch) If streaming is on the agenda, deliberation on Spark Streaming or Spark Structured Streaming might be apt.\nIn other instances, Native RDD or Spark SQL emerges as viable options.\nGiven the project\u0026rsquo;s objective to utilize ES data within Hive, we opted for Spark SQL, capable of embracing a structured format.\nFrom version 1.5 onwards, the Dataframe can be instantiated as depicted below. (For guidance on versions preceding 1.5, consult the respective documentation.)\n1 val df = sql.read.format(\u0026#34;es\u0026#34;).load(\u0026#34;spark/index\u0026#34;) Although Java examples exist, the lion\u0026rsquo;s share of the explanations leans towards Scala.\nGiven our commitment to maintenance, we opted for a Java Project. Consequently, forthcoming examples will be showcased in Java.\n1 2 3 Dataset\u0026lt;Row\u0026gt; df = sparkSession.read() .format(\u0026#34;es\u0026#34;) .load(\u0026#34;template_data\u0026#34;); You have the capability to instantiate a DataFrame object using the aforementioned method.\nShould the need arise to craft a Java DataFrame object, the procedure can be executed as delineated below.\n1 2 3 4 Map\u0026lt;String, String\u0026gt; cfg = Maps.newHashMap(); // cfg serves as an object designated for incorporating configuration. cfg.put(\u0026#34;es.read.metadata\u0026#34;, \u0026#34;true\u0026#34;); Dataset\u0026lt;Row\u0026gt; df = JavaEsSparkSQL.esDF(sparkSession, \u0026#34;template_data\u0026#34;, cfg); Read from ES Through the methodologies outlined thus far, you\u0026rsquo;ve gained access to the DataFrame.\nIt\u0026rsquo;s time to fetch and utilize the data contained within.\nInitially, we\u0026rsquo;ll ascertain the kind of data stored in the DataFrame.\nEmploying the printSchema() function, as demonstrated below, reveals the inherent structure of the DataFrame.\n1 2 3 4 5 6 7 df.printSchema(); root |-- _class: string (nullable = true) |-- column1: long (nullable = true) |-- column2: string (nullable = true) ... The DataFrame\u0026rsquo;s schema is formulated using the mapping specifications designated within the index.\nIf you\u0026rsquo;ve walked through this process firsthand, you\u0026rsquo;d discern the absence of metadata, such as the document id.\nWhile this aspect may appear perplexing, it can be readily addressed by integrating the specified configuration value.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Dataset\u0026lt;Row\u0026gt; df = sparkSession.read() .format(\u0026#34;es\u0026#34;) .option(\u0026#34;es.read.metadata\u0026#34;, \u0026#34;true\u0026#34;) .load(\u0026#34;template_data\u0026#34;); df.printSchema(); root |-- _class: string (nullable = true) |-- column1: long (nullable = true) |-- column2: string (nullable = true) ... |-- _metadata: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true) There might be another lingering query on your mind: how are array fields addressed? ü§î\nThe crux lies in the inability to designate an array type within the mapping.\nAs evident from the guide, it emphasizes declaring the data type associated with the element.\nCould there be potential parsing issues due to misconfiguration of the DataFrame Schema? This question naturally ensues.\nThis hurdle is surmountable through appropriate configuration.\nI, too, encountered this challenge and turned to this specific configuration for resolution.\n1 2 3 4 5 Dataset\u0026lt;Row\u0026gt; df = sparkSession.read() .format(\u0026#34;es\u0026#34;) .option(\u0026#34;es.read.metadata\u0026#34;, \u0026#34;true\u0026#34;) .option(\u0026#34;es.read.field.as.array.include\u0026#34;, \u0026#34;productMapping,sellerMapping\u0026#34;) .load(\u0026#34;template_data\u0026#34;); I\u0026rsquo;ve delineated which fields warrant parsing as arrays.\nOn the flip side, fields that should abstain from array parsing can also be identified.\nThis is facilitated by adjusting the es.read.field.as.array.exclude parameter.\nGiven that the data types between ES stored data and mapping harmonize, no discrepancies will arise during the ES data parsing phase.\nThough direct DataFrame manipulation is feasible, establishing a temporary view empowers you to employ SQL, as exemplified below.\n1 2 df.createTempView(\u0026#34;tmp_front\u0026#34;); Dataset\u0026lt;Row\u0026gt; filtered_df = sparkSession.sql(\u0026#34;select _metadata, \u0026#34; + String.join(\u0026#34;,\u0026#34;, columns) + \u0026#34; from tmp_front\u0026#34;); Customize Field It\u0026rsquo;s not always a straightforward approach to utilize data from ES in its original form; transformations might be imperative at times.\nConsider a scenario where a field comprises a serialized Json string; deserialization may precede its utilization as a Map.\nIndeed, this illustration stems from my personal encounters. üòÑ\nPrior to embarking on the processing aspect, let\u0026rsquo;s touch upon the fundamental workings of Spark.\nOur crafted program is termed a \u0026lsquo;Driver\u0026rsquo; within Spark\u0026rsquo;s context, assuming responsibilities such as supervising a myriad of workers, orchestrating the operational program executions, and monitoring tasks.\nData fetching and distributed tasks are designated to Workers (or Executors). With the collaborative efforts of numerous Workers, even voluminous datasets can be tackled efficiently in a truncated timeframe.\nWhile it\u0026rsquo;s feasible for the Driver to manage the data churned out by Workers, this approach would undeniably decelerate operations.\nThe DataFrames sculpted via Spark SQL reside in a distributed fashion amongst Spark\u0026rsquo;s Workers.\nThus, for adept field processing, it\u0026rsquo;s imperative to facilitate function executions across an array of Workers.\nFor this purpose, implementation through the Serializable Interface is recommended.\nYet, retrofitting an existing Class to adopt the Serializable trait may pose challenges.\nA viable alternative is formulating a static function to address this.\nWhen outlining the process of registering a column post-Deserialize, the sequence would be as follows.\nDevelop a UDF (User Defined Function) as a static function for field manipulation. Register this UDF with SparkSession. Incorporate it as a new column in the DataFrame. If you translate the aforementioned three steps into code, it would look like the following.\n1 2 3 4 5 6 7 8 9 10 11 12 13 public static UDF1\u0026lt;String, HashMap\u0026lt;String, String\u0026gt;\u0026gt; jsonStringToMap = jsonStr -\u0026gt; { ObjectMapper mapper = new ObjectMapper(); return mapper.readValue(jsonStr, new TypeReference\u0026lt;HashMap\u0026lt;String, String\u0026gt;\u0026gt;() {}); }; @Override public RepeatStatus execute(StepContribution contribution, ChunkContext chunkContext) throws Exception { sparkSession.udf().register(\u0026#34;jsonStringToMap\u0026#34;, jsonStringToMap, DataTypes.createMapType(DataTypes.StringType, DataTypes.StringType)); df.createTempView(\u0026#34;tmp_front\u0026#34;); Dataset\u0026lt;Row\u0026gt; filtered_df = sparkSession.sql(\u0026#34;select _metadata, \u0026#34; + String.join(\u0026#34;,\u0026#34;, columns) + \u0026#34; from tmp_front\u0026#34;); filtered_df = filtered_df.withColumn(\u0026#34;dataFieldMap\u0026#34;, functions.callUDF(\u0026#34;jsonStringToMap\u0026#34;, filtered_df.col(\u0026#34;dataFields\u0026#34;))); } With the ability to invoke the UDF function on each Worker, distributed DataFrames can be processed in parallel.\nConclusion So far, we\u0026rsquo;ve explored the process of integrating Elasticsearch data into Hive using Spark.\nWhile I\u0026rsquo;ve detailed only the Spark-SQL approach tailored to my requirements among the four available strategies, the methods for the others are fairly analogous.\nI recommend selecting the method that aligns best with your specific context.\nI embarked on this journey solely based on the guide documentation, and perhaps due to my limited expertise, I faced some challenges. ü´†\nI hope this documentation serves as a helpful reference for those undertaking similar tasks.\nAppreciate your understanding.\nReference Official Documentation for Spark-SQL Integration : https://www.elastic.co/guide/en/elasticsearch/hadoop/7.17/spark.html#spark-sql Official Documentation for Configuration : https://www.elastic.co/guide/en/elasticsearch/hadoop/7.17/configuration.html Dependency Elasticsearch-spark-30 : https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-30 Elasticsearch-hadoop : https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-hadoop ","date":"2023-08-11T00:09:54+09:00","permalink":"https://korcasus.github.io/en/p/elasticsearch-spark/","title":"Elasticsearch Spark"},{"content":"How to Build Fat Jar The method varies depending on the type of build tool you are using.\nFor Maven, you should use the maven-assembly-plugin,\nwhile for Gradle, you need to use the Shadow Plugin.\nIn most cases, using one of these two plugins will solve the problem. However, Spring Boot requires additional handling due to its different execution method.\nBoot-Jar This is the officially recommended approach in Spring.\nAccording to the explanation, Java does not provide a method to load nested JAR files, which can be a problem in environments where you need to run without extracting them.\nTherefore, you should use a Shaded Jar (Fat Jar). It collects all classes from all JAR files, packages them into a single JAR file.\nHowever, as it becomes challenging to see the libraries that exist in the actual application, Spring provides an alternative method.\nIf you\u0026rsquo;re curious about how this is implemented in detail, please refer to the official documentation.\nConfiguration 1. Adding Modules implementation 'org.springframework.boot:spring-boot-loader'\nThe mentioned modules help in creating Spring Boot as an Executable Jar or War.\n2. Modify Manifest Because it should be executed through JarLauncher, Manifest needs to be modified.\nThe contents should exist in META-INF/MANIFEST.MF and should be recorded during the build.\nThe official documentation provides the following guidance:\n1 2 Main-Class: org.springframework.boot.loader.JarLauncher Start-Class: com.mycompany.project.MyApplication In my case, I applied it as follows:\n1 2 3 4 5 6 jar { manifest { attributes \u0026#34;Main-Class\u0026#34;: \u0026#34;org.springframework.boot.loader.JarLauncher\u0026#34; attributes \u0026#34;Start-Class\u0026#34;: \u0026#34;com.mycompany.team.batch.SparkSpringBatchClient\u0026#34; } } Issues When executed using the java -jar xxx.jar command, it works as expected.\nHowever, problems occur when running it within Spark.\nThe issue stemmed from a dependency version mismatch.\nAt my current workplace, we\u0026rsquo;re running Spark version 2.4, and the bundled Gson version was older than what the application required.\nConsequently, the application failed to run due to the utilization of the outdated Gson library.\nI explored options for forcibly using the latest version of Gson.\nThe initial approach involved specifying the dependency version to use.\n(For your information, we are already using a relatively up-to-date Gson version without employing this method.)\nHowever, it didn\u0026rsquo;t work due to Spark\u0026rsquo;s preference for its embedded libraries.\nThe second method is outlined in the official Spark documentation. spark.driver.userClassPathFirst\nspark.executor.userClassPathFirst\nBoth of these variables have a default value of False.\nThe documentation specifies that the user-provided --jars option is given higher priority.\nThis can result in added complexity, particularly when managing an increasing number of jars, and it might introduce issues with experimental functionality.\nSince it didn\u0026rsquo;t deliver the intended behavior, and it wasn\u0026rsquo;t our original intent, we have ceased its usage.\nTo resolve this issue, you should utilize relocation (renaming) of dependencies, which is supported by the Shadow Plugin.\nShadow Plugin It consolidates the project\u0026rsquo;s dependency classes and resources into a single JAR.\nThis offers two distinct advantages:\nIt generates a deployable executable JAR. It bundles and relocates common dependencies from the library to prevent classpath conflicts. Referring to a more comprehensive description of the benefits would be beneficial(written in korean).\nAdd Option For Spring To build a Spring Application with the Shadow Plugin, you need additional options.\nThe following content is being added.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import com.github.jengelman.gradle.plugins.shadow.transformers.* shadowJar { // Required for Spring mergeServiceFiles() append \u0026#39;META-INF/spring.handlers\u0026#39; append \u0026#39;META-INF/spring.schemas\u0026#39; append \u0026#39;META-INF/spring.tooling\u0026#39; transform(PropertiesFileTransformer) { paths = [\u0026#39;META-INF/spring.factories\u0026#39; ] mergeStrategy = \u0026#34;append\u0026#34; } } Detailed information can be found in the Git Issue.\nRelocating Packages This is a feature used when duplicate dependencies exist in the classpath, preventing the intended version from being used. It is a common occurrence in Hadoop and Spark.\nThis idea is simple. It involves identifying conflicting dependencies within the project and then changing their names (paths).\nThis prevents issues from arising since different named dependencies are used.\nThe implementation method is simple.\n1 2 3 4 // Relocating a Package shadowJar { relocate \u0026#39;junit.framework\u0026#39;, \u0026#39;shadow.junit\u0026#39; } Change junit.framework to shadow.junit and apply it as follows, as needed.\n1 2 relocate \u0026#39;com.google.gson\u0026#39;, \u0026#39;shadow.google.gson\u0026#39; relocate \u0026#39;com.fasterxml.jackson\u0026#39;, \u0026#39;shadow.fasterxml.jackson\u0026#39; If you wish to examine relocation in the Shadow Plugin, please refer to the link.\nModify the classpath. In reality, I had hoped that everything would run smoothly up to this point.\nHowever, when I attempt to execute java -jar project-all.jar, I can detect an anomaly.\nAlthough Spring itself started, I observed that the Main Application did not execute.\nWhen I investigated, the root cause was not immediately apparent.\nFortunately, through a process of trial and error, I was able to identify the issue, and it was related to the following link\nAt first, it was built with only the runtimeClassPath by default, but when I included the compileClassPath as follows, the problem was resolved.\n1 configurations = [project.configurations.compileClasspath, project.configurations.productionRuntimeClasspath] As problem resolution was the primary focus, I didn\u0026rsquo;t investigate why including compileClassPath was necessary for it to function correctly.\nConclusion I\u0026rsquo;ve explored how to build a Fat Jar (Shaded Jar) when using Spring.\nWhile Boot Jar is the recommended method by Spring and offers the advantage of simplicity, it can be challenging to resolve Dependency conflicts in situations like using a Hadoop cluster.\nThe usage of the Shadow Plugin is a commonly employed method.\nHowever, when applying it to Spring, you will need to write additional scripts. Nevertheless, this approach can help avoid the problems experienced with Boot Jar.\nWhen dealing with Spark or Hadoop clusters, it is advisable to use the Shadow Plugin.\nHowever, if you do not encounter these issues, it is recommended to use it based on your specific circumstances.\nReference Spring Official Guide : https://docs.spring.io/spring-boot/docs/current/reference/html/executable-jar.html#appendix.executable-jar.alternatives offical document translated in korean Article : https://wordbe.tistory.com/entry/Spring-Boot-2-Executable-JAR-Ïä§ÌîÑÎßÅ-Î∂ÄÌä∏-Ïã§Ìñâ Spring fat jar Git Issue for shadow plugin : https://github.com/spring-projects/spring-boot/issues/1828#issuecomment-231104288 StackOverflow, which was helpful for gaining an overall understanding : https://stackoverflow.com/questions/51206959/classnotfound-when-submit-a-spring-boot-fat-jar-to-spark ","date":"2023-02-20T23:16:51+09:00","permalink":"https://korcasus.github.io/en/p/spring-fat-jar-for-spark/","title":"Spring Fat-jar for Spark"},{"content":" Info\nThis article was originally written in Korean and has been translated using ChatGPT.\nOverview In contrast to the previous discussions, I\u0026rsquo;ll delve into the process of transmitting messages from Hive to Kafka.\nViewed from Kafka\u0026rsquo;s standpoint, Hive takes on the role of a Producer.\nDrawback While transmitting from Hive to Kafka, core functionalities are catered for, making it suitable for most scenarios.\nYet, there are a few unsupported features, including the Header.\nIf the need arises to incorporate a Header, considering an alternative application over Hive is advised.\nKafkaWritable Kafka Record official document Considerations for Kafka metadata When transmitting a message from the Producer to Kafka, you can incorporate associated metadata.\nIf the metadata is omitted, Kafka is understood to generate it automatically.\nThis principle holds true for Hive as well. Since __timestamp, __partition, and __offset are attributes tracked by Kafka, writing in the mentioned manner allows Kafka to automatically populate these fields.\n__timestamp : -1 __partition : null __offset : -1 However, the available value range for __key is limited, making the choices restricted, but careful configuration is advised.\nKafka utilizes the key for determining the partition, which can impact message distribution. Additionally, the application of the key depends on the cleanup.policy configuration.\nIn Kafka, values are assigned to the same partition based on the key. Therefore, when configured as log.cleanup.policy = compact, it\u0026rsquo;s advantageous to input the key as a byte array.\nInteger to byte array : cast(cast(1 AS STRING) AS BINARY) String to byte array : cast(\u0026lsquo;1\u0026rsquo; AS BINARY) JSON test case I\u0026rsquo;ll proceed to transmit a message in JSON format to Kafka.\nCreating a topic I\u0026rsquo;m creating a topic to store test messages.\nBelow is an example that leverages a script bundled with the Kafka installation to establish a topic.\n./kafka-topics.sh --create --topic \u0026quot;${ÌÜ†ÌîΩÌÜ†ÌîΩ}\u0026quot; --replication-factor 1 --partitions 1 --bootstrap-server \u0026quot;{servers}\u0026quot;\nCreating a table Having set up a topic for transmission to Kafka, we\u0026rsquo;ll now establish a table responsible for the actual transmission process.\nEnable the Kafka Handler by designating the KafkaStorageHandler.\nThe configuration for TBLPROPERTIES aligns closely with the guidelines previously mentioned.\nBut, given its role in transmitting messages, set it as a producer instead of a consumer.\nThe example provided below can serve as a reference.\n1 2 3 4 5 6 7 8 9 10 11 CREATE EXTERNAL TABLE test_export_to_kafka( `id` STRING, `value` STRING ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES( \u0026#34;kafka.serde.class\u0026#34; = \u0026#34;org.apache.hadoop.hive.serde2.JsonSerDe\u0026#34;, \u0026#34;kafka.topic\u0026#34; = \u0026#34;${ÌÜ†ÌîΩÌÜ†ÌîΩ}\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;${servers}\u0026#34;, \u0026#34;kafka.producer.security.protocol\u0026#34; = \u0026#34;PLAINTEXT\u0026#34; ); Sending test data We will proceed to insert data into the table we created.\nIt\u0026rsquo;s essential to provide values for the columns explicitly mentioned in the table, which are id and value.\nRegarding Kafka Metadata, you can include them in the following order: key, partition, offset, and timestamp.\ninsert into test_export_to_kafka values ('1', '1', null, null, -1, -1);\nTest case for ONLY LONG TYPE This time, we will attempt to send data in PLAIN TEXT format, rather than JSON.\nCreating a topic It is similar to the JSON test case I explained earlier.\nWe are creating a topic to store test messages.\nThe example below creates the topic using a script provided with Kafka installation.\n./kafka-topics.sh --create --topic \u0026quot;${topic}\u0026quot; --replication-factor 1 --partitions 1 --bootstrap-server \u0026quot;{servers}\u0026quot;\nCreating a table Kafka Handler and TBLPROPERTIES are configured in the same manner as previously described in the test case.\nHowever, this time, as we are using the PLAIN TEXT format, we will create only one column.\n1 2 3 4 5 6 7 8 9 10 CREATE EXTERNAL TABLE test_export_to_kafka_long( id bigint ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES( \u0026#34;kafka.serde.class\u0026#34; = \u0026#34;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\u0026#34;, \u0026#34;kafka.topic\u0026#34; = \u0026#34;${topic}\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;${servers}\u0026#34;, \u0026#34;kafka.producer.security.protocol\u0026#34; = \u0026#34;PLAINTEXT\u0026#34; ); Sending test data We will proceed to insert data into the table we created.\nIt is mandatory to include a value for the id column, as specified in the table.\nFor the remaining columns, you can add Kafka Metadata information as done previously.\ninsert into test_export_to_kafka_long values (1, null, null, -1, -1);\nTesting messages with the same key being placed in the same partition When creating an application to be used as a Producer, you have the option to specify a Partitioner that determines the partition where Kafka will store the message.\nHowever, in Hive, there is no separate provision to specify a Partitioner.\nIt should be assumed that the Default Partitioner is in use.\nIf the message keys are identical, they should be placed in the same partition.\nHowever, since this is the first attempt to transmit through Hive, it was necessary to verify whether they are indeed directed to the same partition.\nTests were conducted to ascertain this.\nFollowing the transmission of multiple messages with different keys, we proceed to send a message with the same key as the previous one but with different content.\nYou can access detailed information through the query provided below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 CREATE EXTERNAL TABLE test_export_to_kafka_partition( id INT, value STRING ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES( \u0026#34;kafka.serde.class\u0026#34; = \u0026#34;org.apache.hadoop.hive.serde2.JsonSerDe\u0026#34;, \u0026#34;kafka.topic\u0026#34; = \u0026#34;${topic}\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;${servers}\u0026#34;, \u0026#34;kafka.producer.security.protocol\u0026#34; = \u0026#34;PLAINTEXT\u0026#34; ); ./kafka-topics.sh --create --topic ${topic} --bootstrap-server {servers} --config \u0026#34;cleanup.policy=compact\u0026#34; --partitions 4 --replication-factor 2 INSERT INTO TABLE test_export_to_kafka_partition VALUES (1, \u0026#39;1\u0026#39;, CAST(\u0026#39;1\u0026#39; AS BINARY), NULL, -1, -1), (\u0026#34;2\u0026#34;, \u0026#34;2\u0026#34;, CAST(\u0026#39;2\u0026#39; AS binary), -1, -1, -1), (\u0026#34;3\u0026#34;, \u0026#34;3\u0026#34;, CAST(\u0026#39;3\u0026#39; AS binary), -1, -1, -1), (\u0026#34;4\u0026#34;, \u0026#34;4\u0026#34;, CAST(\u0026#39;4\u0026#39; AS binary), -1, -1, -1); CREATE TEMPORARY TABLE test_partition(id INT, value STRING); INSERT INTO TABLE test_partition VALUES (2, \u0026#39;2\u0026#39;), (3, \u0026#39;3\u0026#39;), (4, \u0026#39;4\u0026#39;); INSERT INTO table test_export_to_kafka_partition SELECT id, value, CAST(CAST(id AS string) AS binary) AS `_key`, NULL AS `_partition`, -1, -1 FROM test_partition; Even though you didn\u0026rsquo;t provide a result screen, the test confirmed that data with the same key value is stored in the same partition.\nConclusion I have confirmed through this article that it is possible to transmit Hive Table data to Kafka.\nIn fact, there are several methods to transfer data, and there are other good alternatives available. (For example, you can connect to HiveServer2 via JDBC or use Spark.)\nWhile I didn\u0026rsquo;t explicitly mention it in the feature description, you might have noticed that there are some limitations. If you execute queries that involve JOINs with other tables and attempt to send data immediately, errors may occur, as you pointed out. To overcome this issue, you should create temporary tables to store the data and then write queries that use these temporary tables for data transmission.\nThere may be other constraints that I haven\u0026rsquo;t identified.\nDespite these constraints, a significant advantage is that you don\u0026rsquo;t need to create a separate application.\nI hope this information serves as an opportunity for you to realize that this method can be one of the choices.\nThank you for reading this far.\n","date":"2022-12-01T22:45:04+09:00","permalink":"https://korcasus.github.io/en/p/hive-kafka-integration3/","title":"Write message to kafka using hive"},{"content":" Info\nThis article was originally written in Korean and has been translated using ChatGPT.\nOverview In our previous post, we delved into the configurations necessary for integrating Kafka with Hive.\nMoving forward, I\u0026rsquo;ll elaborate on the processes of reading from and writing to Kafka.\nIn this piece, we\u0026rsquo;ll kick off by pulling data from Kafka.\nYou can think of it as leveraging Hive in the role of a Kafka Consumer for better clarity.\nOne distinction to note is that it doesn\u0026rsquo;t get assigned as a Consumer Group, unlike typical Consumers.\nJSON This tutorial proceeds under the assumption that the data being read from Kafka is formatted in JSON.\nSample data I\u0026rsquo;ll walk you through using the following data structure as our reference example. Feel free to expand by adding more fields based on your requirements.\n1 2 3 4 { \u0026#34;id\u0026#34;: 12345678, \u0026#34;value:\u0026#34;: \u0026#34;hello\u0026#34; } Table Definition Let\u0026rsquo;s define a table mirroring the structure of the JSON schema.\nThe \u0026lsquo;id\u0026rsquo; field carries an integer value, while the \u0026lsquo;value\u0026rsquo; field is a string.\nWithin Hive, integer values can be represented using the INT or BIGINT data types. Strings, on the other hand, are characterized as STRING.\nDrawing from the provided information, the DDL query can be constructed as follows.\n1 2 3 4 5 6 7 8 9 10 11 CREATE EXTERNAL TABLE test_export_to_kafka( `id` BIGINT, `value` STRING ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES( \u0026#34;kafka.serde.class\u0026#34; = \u0026#34;org.apache.hadoop.hive.serde2.JsonSerDe\u0026#34;, \u0026#34;kafka.topic\u0026#34; = \u0026#34;etl.hive.catalog.used\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;{broker servers}\u0026#34;, \u0026#34;kafka.consumer.security.protocol\u0026#34; = \u0026#34;PLAINTEXT\u0026#34; ); The table name, test_export_to_kafka, can be adjusted to your preference. Columns within the table should be outlined in alignment with the JSON structure being ingested.\nEnsure that column names mirror the JSON keys, and column data types are consistent with the corresponding JSON values.\nGiven that Hive offers a broader spectrum of data types compared to JSON, I\u0026rsquo;d recommend referring to this documentation to pick the most suitable type.\nBy assigning \u0026quot;kafka.serde.class\u0026quot; = \u0026quot;org.apache.hadoop.hive.serde2.JsonSerDe\u0026quot;, you\u0026rsquo;re enabling Hive to deserialize JSON data.\nNo other particulars in TBLPROPERTIES demand attention.\nSimply adjust kafka.topic to the desired Kafka topic name.\nREAD 1 2 SELECT * FROM test_export_to_kafka If everything has been configured correctly, you should be able to view the messages within the topic.\nOn the other hand, if there\u0026rsquo;s a misconfiguration, you might encounter error messages or see columns rendered as NULL.\nEnsure that your setup doesn\u0026rsquo;t deviate from the guidelines we\u0026rsquo;ve discussed up to this point.\nTEXT This tutorial focuses on reading PLAIN TEXT, as opposed to structured data such as JSON.\nSample data Given it\u0026rsquo;s PLAIN TEXT, the example is straightforward.\nWe\u0026rsquo;ll utilize basic numerical values for demonstration purposes.\n1 2 3 1234 5678 91011 Table Definition 1 2 3 4 5 6 7 8 9 10 CREATE EXTERNAL TABLE test_export_to_kafka( `id` BIGINT ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES( \u0026#34;kafka.serde.class\u0026#34; = \u0026#34;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\u0026#34;, \u0026#34;kafka.topic\u0026#34; = \u0026#34;etl.hive.catalog.used\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;{servers}\u0026#34;, \u0026#34;kafka.consumer.security.protocol\u0026#34; = \u0026#34;PLAINTEXT\u0026#34; ); This aligns closely with the JSON test case we touched on previously.\nThe distinctions lie in the distinct designations for Serde and the columns.\nFor the Serde configuration, switch to \u0026quot;kafka.serde.class\u0026quot; = \u0026quot;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\u0026quot;.\nREAD 1 2 SELECT * FROM test_export_to_kafka If everything has been configured properly, you should be able to view the messages from the topic.\nHowever, should there be a misconfiguration, you might either see error messages or columns displaying as NULL.\nPlease ensure your configurations align with the guidelines discussed up to now.\nUtilizing Kafka Metadata In Kafka, alongside the primary messages, associated metadata is also stored.\nWhen shaping a table, this metadata seamlessly gets incorporated as columns. Consequently, when Hive fetches a message, it concurrently retrieves this accompanying metadata.\nGiven that metadata is configured as columns, it opens up avenues for their application within SQL conditional statements.\nFor illustration, you might opt to pull data entered post a designated timestamp, or exclusively data residing in a particular partition.\n\u0026ldquo;For fields like __partition and __offset, their employment of integer types makes them particularly user-friendly.\nex1) __partition = 0 ex2) __offset \u0026gt; 5000 Yet, with __timestamp, it\u0026rsquo;s captured as int64 due to its reliance on the unix_timestamp.\nThis diverges from the typical time notation we\u0026rsquo;re accustomed to, necessitating a conversion and resulting in a less-than-convenient experience.\nFor ease in deploying within conditions, it\u0026rsquo;s advisable to modify and utilize it as illustrated below.\nThe function to employ here would be unix_timestamp, as demonstrated in the subsequent example.\nunix_timestamp('2022-03-08 00:00:00.000', 'yyyy-MM-dd HH:mm:ss.SSS') * 1000\nFor the first parameter, specify the time you intend to transform. For the second parameter, indicate the time format corresponding to what you specified in the first. With the function in place, it\u0026rsquo;s possible to transform a specific time into unix_timestamp, paving the way for comparison activities involving __timestamp.\nYet, bear in mind that the __timestamp fetched via Hive operates on a microsecond scale. As a result, some additional computations are required to align the units, which is why a multiplication by 1000 was executed. Here\u0026rsquo;s an illustrative example.\nex) unix_timestamp('2022-03-08 00:00:00.000', 'yyyy-MM-dd HH:mm:ss.SSS') * 1000 \u0026lt; __timestamp\nSupplementary Example\n1 2 3 4 5 6 7 8 9 -- print to UTC timestamp select FROM_UTC_TIMESTAMP(`__timestamp`, \u0026#39;JST\u0026#39;) from test_kafka_product_received limit 1; -- print timestamp, unixtime to timestamp select current_timestamp(), from_unixtime(unix_timestamp(), \u0026#39;yyyy-MM-dd HH:mm:ss.SSS\u0026#39;); -- print converted unixtime select unix_timestamp(\u0026#39;2022-03-07 14:00:00.000\u0026#39;, \u0026#39;yyyy-MM-dd HH:mm:ss.SSS\u0026#39;); select unix_timestamp(\u0026#39;2022-03-07 06:11:41\u0026#39;, \u0026#39;yyyy-MM-dd HH:mm:ss\u0026#39;); Ï†ïÎ¶¨ With the insights from this piece, you\u0026rsquo;re now equipped to fetch JSON or PLAIN-TEXT data from Kafka utilizing Hive.\nLeveraging metadata for filtering can enable effortless extraction of Kafka messages that align with specific criteria.\nA noted limitation is the inability to join this with other tables housed within Hive (I faced authentication-related challenges during my attempts).\nSuch constraints might curtail its full potential, but with further enhancements, its utility could significantly rise.\nIn the subsequent post, I\u0026rsquo;ll delve into the procedure of transmitting data from Hive to Kafka.\nMany thanks.\n","date":"2022-12-01T22:45:01+09:00","permalink":"https://korcasus.github.io/en/p/hive-kafka-integration2/","title":"Read message from kafka using hive"},{"content":" Info\nThis article was originally written in Korean and has been translated using ChatGPT.\nOverview Hive is a data warehouse software that allows for reading and writing large volumes of data stored in Hadoop using SQL syntax.\nIn simple terms, it can be thought of as a large-scale data processing database operating on Hadoop.\nBy default, it supports HDFS and offers an open DataSourcing feature that allows access to Hbase or other DBMS. It also supports Kafka, which is widely used as a Message Queue. In this series, I will document the configuration for integrating Hive with Kafka, as well as the process of reading and writing messages.\nIn this article, the focus is on the necessary configurations and essential information for use.\nIf you are interested in usage examples, please read the next article.\nThis was written based on the official document\nAlthough it is not stated in the document, I intend to detail additional information required for use and examples that I personally tested.\nThis was done in Hive version 3.x.\nInstall In fact, no separate installation is required.\nAs you may know if you\u0026rsquo;ve looked at the official documentation, it\u0026rsquo;s already built into Hive.\nWhile it might be hard to imagine, you can integrate with Kafka using SQL.\nCreate Table To integrate Hive and Kafka, a linkage must be created.\nJust as in Spring, you can easily create a connection using Binder and Binding, in Hive, you can do this using StorageHandler.\nYou can specify StorageHandler when defining a table. Additionally, with some more configurations, you can use it seamlessly.\nHowever, there are a few things to be cautious of when creating a table in Hive that integrates with Kafka.\nThe first thing is that you must create it as an EXTERNAL table.\nSince the method we will be using involves Hive accessing and using the data stored in Kafka, you need to add the EXTERNAL keyword.\nThe second thing is that you must specify the KafkaStorageHandler.\nWhen defining the table, it should include STORED BY 'org.apache.hadoop.hive.kafka.KafkaStorageHandler'.\nOnly with this specified can you integrate and use Kafka.\nThe third thing is that you need to configure the table.\nUsing the TBLPROPERTIES keyword, you can set the properties for the table.\nWithin this, you must write the configuration necessary for integrating with Kafka.\nBy applying all three points and writing the DDL, it can be crafted as shown in the example below.\n1 2 3 4 5 6 7 8 9 10 11 12 13 CREATE EXTERNAL TABLE kafka_table ( `timestamp` TIMESTAMP, `page` STRING, `newPage` BOOLEAN, `added` INT, `deleted` BIGINT, `delta` DOUBLE ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES ( \u0026#34;kafka.topic\u0026#34; = \u0026#34;test-topic\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;localhost:9092\u0026#34; ); Kafka metadata Messages stored in Kafka consist not only of the Payload but also have four pieces of metadata: Key, Partition, Offset, and Timestamp.\nThese data can also be used in Hive.\nThe previously mentioned KafkaStorageHandler, used when defining the table, automatically registers these metadata as columns.\n__key (byte array) __partition (int32) __offset (int64) __timestamp (int64) I will elaborate on the usage methods in the next article where I\u0026rsquo;ve written real-use examples.\nPrecautions When Using Avro Format If you are using the Avro format through the Confluent Connector, you need to remove 5 bytes from the message.\nOf these 5 bytes, 1 byte is the magic byte, and 4 bytes correspond to the schema id of the schema registry.\nThis can be resolved by setting \u0026quot;avro.serde.type\u0026quot;=\u0026quot;skip\u0026quot; and \u0026quot;avro.serde.skip.bytes\u0026quot;=\u0026quot;5\u0026quot;.\nIf you are using avro, it would be beneficial to thoroughly refer to the guide for detailed information.\nSupport for the confluent avro format seems to be added in Hive version 4.0.\nIf you are using version 3.x, it would be good to be cautious. (Because I had a hard time with it\u0026hellip;)\nSerde(Serializer, Deserializer) If you\u0026rsquo;ve ever sent data from an Application to Kafka, you\u0026rsquo;ve probably had experience specifying a serializer and deserializer.\nSimilarly, Hive also offers various serializers and deserializers.\nSupported Serializers and Deserializers description org.apache.hadoop.hive.serde2.JsonSerDe JSON org.apache.hadoop.hive.serde2.OpenCSVSerde CSV org.apache.hadoop.hive.serde2.avro.AvroSerDe AVRO org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe binary org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe Plain Text The method of specification is simpler than you might think.\nYou can easily apply it by adding it to TBLPROPERTIES.\nFor instance, if the data format you wish to read from Kafka is JSON or if you want to send data in JSON format, you can add it as shown below.\n\u0026quot;kafka.serde.class\u0026quot; = \u0026quot;org.apache.hadoop.hive.serde2.JsonSerDe\u0026quot;\nAuthentication Kafka also offers a feature that requires authentication to grant permissions to read and write data.\nThe authentication details needed for using Kafka can also be provided through Hive.\nIn my case, I didn\u0026rsquo;t add any authentication and used it, so I will document it as an instance without authentication. If you are in a situation where you need authentication, please refer to and proceed with the official documentation.\nIf you haven\u0026rsquo;t added the TBLPROPERTIES value for authentication, an error occurs when you attempt to connect.\nYou might think, \u0026ldquo;If I haven\u0026rsquo;t set it up, shouldn\u0026rsquo;t it naturally be usable without authentication?\u0026rdquo; (That\u0026rsquo;s what I thought.)\nBy default, Hive tries to authenticate using an id and password.\nTherefore, you have to add a configuration to use it without authentication.\nThe official documentation didn\u0026rsquo;t provide a specific solution for this problem, so it was a challenging aspect for me.\nTo get straight to the point, you can resolve this by adding TBLPROPERTY.\nIn the case of a producer : \u0026quot;kafka.producer.security.protocol\u0026quot; = \u0026quot;PLAINTEXT\u0026quot; In the case of a consumer : \u0026quot;kafka.consumer.security.protocol\u0026quot; = \u0026quot;PLAINTEXT\u0026quot; If you want to delve deeper into authentication, it would be helpful to refer to this document.\nAs I\u0026rsquo;ll mention later, if you set kafka. as a prefix when configuring, you can pass the configuration value to Kafka.\nKey Serializer The Serializers I explained earlier were Value Serializers.\nThough more are provided than you might expect, conversely, only the Byte Serializer is supported for Key Serializer.\nEven if you want to change and use a different Serializer, it\u0026rsquo;s not possible to do so.\nI\u0026rsquo;m not sure why it was implemented this way, but you can understand the details by looking at the code.\nAs you might be curious, I\u0026rsquo;m also providing a link.\nproducer consumer Additionally Beyond what has been written so far, there may be additional configurations you\u0026rsquo;d like to add when using Kafka.\nAlthough it\u0026rsquo;s not described in the official documentation, you can pass configuration values to be used in Kafka.\nIn the case of a producer ‚Äúkafka.producer.{property}‚Äù = ‚Äú{value}‚Äù In the case of a consumer ‚Äúkafka.consumer.{property}‚Äù = ‚Äú{value}‚Äù By adding to TBLPROPERTIES, the configuration values are passed to Kafka, allowing you to apply your desired settings. If you\u0026rsquo;re curious about how the configuration values are passed or the implemented code, it would be helpful to refer to the link below.\nproducer consumer Summary Until now, we\u0026rsquo;ve looked into the necessary configurations and helpful information for reading and writing to Kafka from Hive.\nIn the next article, I will document the process of reading messages from Kafka in Hive.\nThank you.\n","date":"2022-12-01T22:44:01+09:00","permalink":"https://korcasus.github.io/en/p/hive-kafka-integration1/","title":"Hive Kafka Integration"}]