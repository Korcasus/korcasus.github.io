[{"content":"How to Build Fat Jar Build Tool을 어떤 종류를 사용하는지에 따라 방법이 달라집니다.\nMaven은 maven-assembly-plugin\nGradle Shadow Plugin 을 사용해야합니다.\n대부분은 위의 2개 Plugin중 하나만 사용하면 해결되지만, Spring Boot는 구동방식이 달라서, 별도의 처리가 필요합니다.\nBoot-Jar Spring에서 공식적으로 권장하는 방법입니다.\n설명에 따르면 Java는 nested-jar파일을 load하는 방법을 제공하지않습니다. 이때문에 압축 풀지않고, 실행해야하는 환경이라면 문제가 될 수 있습니다.\n이 때문에 Shaded Jar(Fat Jar)를 사용해야합니다. 모든 Jars파일로부터 모든 Class를 모아서 Pacakage하여 하나의 Jar파일로 만들어 줍니다.\n실제로 Application에 존재하는 library를 보기에 힘들어지기때문에, Spring은 다른 방법을 제공합니다.\n어떤방식으로 구현되었는지가 궁금하다면 상세한 내용은 공식문서를 참고해주세요.\nConfiguration 1. Module 추가 implementation 'org.springframework.boot:spring-boot-loader'\n위의 모듈은 Spring boot 를 Excutable Jar 또는 War로 만들수 있게 도와줍니다.\n2. Manifest 변경 JarLauncher를 통해 실행되어야하기 때문에, Manifest 변경이 필요합니다.\nMETA-INF/MANIFEST.MF 에 내용이 존재하는데. Build하며 기록되어야 합니다.\n공식문서에는 아래와같이 가이드 하고 있습니다.\n1 2 Main-Class: org.springframework.boot.loader.JarLauncher Start-Class: com.mycompany.project.MyApplication 저의 경우에는 아래와 같이 적용하였습니다.\n1 2 3 4 5 6 jar { manifest { attributes \u0026#34;Main-Class\u0026#34;: \u0026#34;org.springframework.boot.loader.JarLauncher\u0026#34; attributes \u0026#34;Start-Class\u0026#34;: \u0026#34;com.mycompany.team.batch.SparkSpringBatchClient\u0026#34; } } 문제점 java -jar xxx.jar 명령어로 실행할 경우, 정상동작합니다.\n하지만 Spark에서 실행할 경우, 문제가 발생합니다.\n이유는 Dependency version 때문이었습니다.\n재직중인 회사에서 운영중인 Spark는 2.4버전을 사용 중인데, 여기에 포함되어있는 gson 버전이 Application에서 사용해야할 버전보다 낮았습니다. 이때문에 구버전의 Gson으로 사용되면서 Application구동 실패 하였습니다.\n강제로 최신버전의 Gson을 사용할 수 있는 방안을 찾아보았습니다.\n첫번째로는 사용할 Dependency버전을 명시하는 방법이였습니다.\n(참고로 이 방법을 사용하지않아도 비교적 최신 gson을 사용합니다.) 하지만 spark에 내장되어있는 library를 우선으로 사용하기때문에 실패하였습니다.\n두번째 방법은 Spark공식문서에서 제공하는 방법입니다. (참고)\nspark.driver.userClassPathFirst\nspark.executor.userClassPathFirst\n위의 2개 변수는 기본값이 False로 되어있습니다.\n문서에 따르면, 사용자가 옵션으로 추가한 --jars 를 우선시 사용한다고 되어있습니다.\n시험 기능인 문제도 있고, 추가해야할 jar이 증가하면 관리할 포인트가 증가할 수 있습니다. 의도한대로 동작하지도 않았고, 의도했던 바가 아니라 더이상 사용하지 않았습니다.\n이를 해결하려면 relocate(임의로 이름변경?)을 사용해야하는데, Shadow Plugin에서는 지원합니다.\nShadow Plugin 프로젝트 내에 있는 Dependency Classs와 Resource를 하나의 Jar로 만들어 줍니다.\n이로 인해 얻을수 있는 장점은 2가지입니다.\n실행가능한 Jar배포를 만든다. Library에 있는 공통 Dependency를 bundling, relocating하여 classpath conflict를 회피한다. 장점을 좀더 잘 풀어쓴글을 참고하면 좋겠습니다.\nAdd Option For Spring Shadow Plugin으로 Spring Application을 Build하려면 추가 옵션이 필요합니다.\n아래의 내용을 추가합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import com.github.jengelman.gradle.plugins.shadow.transformers.* shadowJar { // Required for Spring mergeServiceFiles() append \u0026#39;META-INF/spring.handlers\u0026#39; append \u0026#39;META-INF/spring.schemas\u0026#39; append \u0026#39;META-INF/spring.tooling\u0026#39; transform(PropertiesFileTransformer) { paths = [\u0026#39;META-INF/spring.factories\u0026#39; ] mergeStrategy = \u0026#34;append\u0026#34; } } 상세한 내용은 Git Issue를 읽어보면 알 수 있습니다.\nRelocating Packages Classpath에 동일한 Dependency가 존재하여, 의도하고자한 버전을 사용하지 못하는 상황일때 사용하는 기능입니다. Hadoop, Spark에서 빈번히 발생합니다.\n이 아이디어는 간단합니다. 충돌나는 Dependency를 Project에서 찾아낸뒤, 이름(경로)을 바꿔줍니다. 다른 이름의 Dependency를 사용하기때문에 문제가 생기지 않습니다.\n구현방법은 간단합니다.\n1 2 3 4 // Relocating a Package shadowJar { relocate \u0026#39;junit.framework\u0026#39;, \u0026#39;shadow.junit\u0026#39; } juni.framework를 shadow.junit 으로 변경해 줍니다.\n필요에 맞게 아래처럼 적용합니다.\n1 2 relocate \u0026#39;com.google.gson\u0026#39;, \u0026#39;shadow.google.gson\u0026#39; relocate \u0026#39;com.fasterxml.jackson\u0026#39;, \u0026#39;shadow.fasterxml.jackson\u0026#39; Shadow Plugin에서 relocation을 살펴보고싶다면 링크를 참고해주세요.\nClasspath 변경 사실 여기까지 진행하고 나서, 잘 실행하는것을 기대했습니다.\n하지만 java -jar project-all.jar 로 실행해보면, 이상함을 느낄 수 있습니다. Spring자체는 실행되었지만, Main Application이 실행되지 않은 것을 확인 할 수 있습니다.\n찾아보니, 원인을 알기 쉽지 않았습니다.\n다행히 try and error 거치며 알아냈는데, 아래의 링크와 관련있었습니다. https://imperceptiblethoughts.com/shadow/configuration/dependencies/\n기본으로 runtimeClassPath만 포함되서 Build되었고, 아래처럼 compileClassPath를 포함하니 해결되었습니다.\n1 configurations = [project.configurations.compileClasspath, project.configurations.productionRuntimeClasspath] 문제해결이 우선이라, 왜 compileClassPath도 포함해야만 정상동작하는지는 확인해보지 않았습니다.\nConclusion Spring을 사용할때, 어떻게 Fat Jar(Shaded Jar)을 빌드할 수 있는지 살펴보았습니다.\nBoot-Jar은 스프링에서 권장하는 방법이고 간단하다는 장점이 있지만, Dependency 충돌하는 상황이라면 해결할기 어렵다는 문제가 있습니다.\n이런경우는 하둡 클러스터를 사용하는경우에 해당됩니다.\nShadow Plugin을 사용하는 방법은 일반적으로 많이 사용하는 방법입니다.\n하지만 스프링에 적용하려면 스크립트를 추가로 작성해주어야 하지만, Boot-Jar에서 겪었던 문제를 회피할 수 있습니다.\nSpark나 하둡 클러스터를 사용해야하는 경우라면 Shadow Plugin을 사용하는 것을 권장합니다.\n하지만 이와 같은 문제를 부딪힌 상황이 아니라면, 본인의 상황에 맞게 사용하는 것을 권장합니다.\nReference Spring Official Guide : https://docs.spring.io/spring-boot/docs/current/reference/html/executable-jar.html#appendix.executable-jar.alternatives 공식문서 번역 Article : https://wordbe.tistory.com/entry/Spring-Boot-2-Executable-JAR-스프링-부트-실행 Spring fat jar Git Issue for shadow plugin : https://github.com/spring-projects/spring-boot/issues/1828#issuecomment-231104288 전반적인 이해에 도움을 주었던 StackOverflow : https://stackoverflow.com/questions/51206959/classnotfound-when-submit-a-spring-boot-fat-jar-to-spark ","date":"2023-02-20T23:16:51+09:00","permalink":"https://korcasus.github.io/p/spring-fat-jar-for-spark/","title":"Spring Fat-jar for Spark"},{"content":"개요 앞서 설명해왔던 내용과는 반대로 Hive에서 Kafka에 메시지를 전송하는 과정을 설명하겠습니다.\nKafka 입장에서는 Hive가 Producer 역할을 수행하게 됩니다.\n단점 Hive에서 Kafka로 전송할때 필수적인 기능들은 제공하다보니, 일반적인 경우에는 사용하는데 문제 없습니다.\n하지만 일부 기능을 제공하지않는데, 그 중 하나가 Header입니다.\nHeader도 만들어주어야 하는 상황이라면, Hive 말고 다른 Application을 사용하는 것을 권장합니다.\nKafkaWritable Kafka Record 공식문서 kafka metadata 주의사항 Producer에서 Kafka로 메시지를 전송할때는 메타 데이터들을 포함시켜 전송할 수 있습니다.\n포함시키지 않을 경우, Kafka에서 자동으로 생성해주는 것으로 알고 있습니다.\nHive에서도 마찬가지 입니다. __timestamp, __partition, __offset은 kafka에서 기록하는 정보이므로 아래처럼 작성하면 Kafka에서 자동으로 생성해 줄 수 있습니다.\n__timestamp : -1 __partition : null __offset : -1 하지만 __key는 사용할 수 있는 값의 범위는 한정적이다보니 선택지가 제한적이지만, 신중하게 설정해주는 것이 좋습니다.\nKafka는 key를 partition을 결정하는데 사용하므로 메시지 분산에 영향을 미칠수 있습니다. 그리고 cleanup.policy 설정값에 따라 key 활용여부가 결정됩니다.\nKafka에서 key값을 기준으로 동일한 partition에 넣어주는 역할을 수행하고 있으며 , log.cleanup.policy = compact 로 설정할경우 key를 활용하기 때문에 byte array로 넣어주는게 좋습니다.\nInteger to byte array : cast(cast(1 AS STRING) AS BINARY) String to byte array : cast(\u0026lsquo;1\u0026rsquo; AS BINARY) JSON 테스트 케이스 JSON 포맷 메시지를 Kafka에 전송해보도록 하겠습니다.\n토픽 생성 테스트 메시지를 저장해둘 토픽을 생성합니다.\n아래의 예시는 Kafka 설치시 함께 제공되는 스크립트를 사용하여 topic을 만듭니다.\n./kafka-topics.sh --create --topic \u0026quot;${토픽토픽}\u0026quot; --replication-factor 1 --partitions 1 --bootstrap-server \u0026quot;{서버서버}\u0026quot;\n테이블 생성 Kafka에 전송할 topic을 만들어보았으니, 이제는 실제 전송역할을 수행해줄 table을 생성해보겠습니다. Kafka Handler를 사용할 수 있도록 KafkaStorageHandler를 지정해줍니다.\nTBLPROPERTIES에는 앞서 설명드렸던 글과 비슷하게 진행합니다.\n다만, 메시지 전송하는 역할이니 consumer가 아닌 producer로 지정해줍니다.\n아래의 예시를 참고하면 좋겠습니다.\n1 2 3 4 5 6 7 8 9 10 11 CREATE EXTERNAL TABLE test_export_to_kafka( `id` STRING, `value` STRING ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES( \u0026#34;kafka.serde.class\u0026#34; = \u0026#34;org.apache.hadoop.hive.serde2.JsonSerDe\u0026#34;, \u0026#34;kafka.topic\u0026#34; = \u0026#34;${토픽토픽}\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;${서버서버}\u0026#34;, \u0026#34;kafka.producer.security.protocol\u0026#34; = \u0026#34;PLAINTEXT\u0026#34; ); 테스트 데이터 전송 만들어준 테이블에 데이터를 입력해보도록 하겠습니다. 테이블에 명시한 컬럼인 id, value에 대한 값을 필수로 추가합니다. 그 외에는 Kafka MetataData 이며 key, partition, offset, timestamp 순서대로 생각하고 넣어주시면 됩니다.\ninsert into test_export_to_kafka values ('1', '1', null, null, -1, -1);\nONLY LONG TYPE 테스트 케이스 이번에는 JSON이 아닌 PLAIN TEXT Format으로 데이터를 전송해보도록하겠습니다.\n토픽 생성 앞서 설명드렸던 JSON 테스트 케이스와 비슷합니다.\n테스트 메시지를 저장해둘 토픽을 생성합니다.\n아래의 예시는 Kafka 설치시 함께 제공되는 스크립트를 사용하여 topic을 만듭니다.\n./kafka-topics.sh --create --topic \u0026quot;${토픽토픽}\u0026quot; --replication-factor 1 --partitions 1 --bootstrap-server \u0026quot;{서버서버}\u0026quot;\n테이블 생성 Kafka Handler와 TBLPROPERTIES는 앞서 설명드렸던 테스트 케이스와 동일하게 설정합니다.\n다만 이번엔 PLAIN TEXT Format인 만큼, 컬럼은 하나만 만들어 주겠습니다.\n1 2 3 4 5 6 7 8 9 10 CREATE EXTERNAL TABLE test_export_to_kafka_long( id bigint ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES( \u0026#34;kafka.serde.class\u0026#34; = \u0026#34;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\u0026#34;, \u0026#34;kafka.topic\u0026#34; = \u0026#34;${토픽토픽}\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;${서버서버}\u0026#34;, \u0026#34;kafka.producer.security.protocol\u0026#34; = \u0026#34;PLAINTEXT\u0026#34; ); 테스트 데이터 전송 만들어둔 테이블에 데이터를 입력해보도록 하겠습니다.\n테이블에 명시한 컬럼인 id에 대한 값을 필수로 추가합니다. 그 외에는 Kafka MetaData에 대한 정보로 이전과 동일하게 추가해줍니다. insert into test_export_to_kafka_long values (1, null, null, -1, -1);\n동일한 key를 가진 메시지가 동일한 partition에 들어가는 테스트 Producer로 사용할 Application을 만들때 보면, Kafka에 전송할 Message가 어떤 Partition에 저장될지 Partitioner를 지정할 수 있습니다.\n하지만 Hive에서는 별도로 Partitioner를 지정할 수 없습니다.\nDefault Partitioner를 사용하는것으로 생각하고 진행하셔야 합니다.\nMessage의 Key가 동일하다면, 동일한 Partition에 들어가야 합니다.\n하지만 Hive를 통해 전송해보는 것은 처음이기 때문에 동일한 Partition에 전송되는지 확인이 필요하였습니다.\n이를 확인하기 위해 테스트를 진행하였습니다.\n각기 다른 Key를 가진 여러개의 메시지를 전송한 뒤, 앞서 보낸 메시지와 동일한 Key를 가지지만 다른 내용으로 메시지를 전송합니다.\n자세한 내용은 아래의 쿼리를 통해 확인할 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 CREATE EXTERNAL TABLE test_export_to_kafka_partition( id INT, value STRING ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES( \u0026#34;kafka.serde.class\u0026#34; = \u0026#34;org.apache.hadoop.hive.serde2.JsonSerDe\u0026#34;, \u0026#34;kafka.topic\u0026#34; = \u0026#34;${토픽토픽}\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;${서버서버}\u0026#34;, \u0026#34;kafka.producer.security.protocol\u0026#34; = \u0026#34;PLAINTEXT\u0026#34; ); ./kafka-topics.sh --create --topic ${토픽토픽} --bootstrap-server {서버서버} --config \u0026#34;cleanup.policy=compact\u0026#34; --partitions 4 --replication-factor 2 INSERT INTO TABLE test_export_to_kafka_partition VALUES (1, \u0026#39;1\u0026#39;, CAST(\u0026#39;1\u0026#39; AS BINARY), NULL, -1, -1), (\u0026#34;2\u0026#34;, \u0026#34;2\u0026#34;, CAST(\u0026#39;2\u0026#39; AS binary), -1, -1, -1), (\u0026#34;3\u0026#34;, \u0026#34;3\u0026#34;, CAST(\u0026#39;3\u0026#39; AS binary), -1, -1, -1), (\u0026#34;4\u0026#34;, \u0026#34;4\u0026#34;, CAST(\u0026#39;4\u0026#39; AS binary), -1, -1, -1); CREATE TEMPORARY TABLE test_partition(id INT, value STRING); INSERT INTO TABLE test_partition VALUES (2, \u0026#39;2\u0026#39;), (3, \u0026#39;3\u0026#39;), (4, \u0026#39;4\u0026#39;); INSERT INTO table test_export_to_kafka_partition SELECT id, value, CAST(CAST(id AS string) AS binary) AS `_key`, NULL AS `_partition`, -1, -1 FROM test_partition; 결과화면은 첨부하지 않았지만 테스트를 진행하니 동일한 Key값을 가진경우, 동일한 Partition에 저장되는 것을 확인할 수 있었습니다.\n정리 이번 글을 통해 Hive Table 데이터를 Kafka로 전송가능하다는 것을 확인할 수 있었습니다.\n사실 이렇게 데이터를 전송하는게 유일한 방법도 아니고, 다른 좋은 방법들도 존재합니다.\n(예를 들면 HiveServer2에 JDBC로 연결할 수도 있고, Spark를 사용할 수도 있습니다.)\n설명한 내용들을 읽으셨다면 제약이 많다는 것을 느끼셨을 수 있습니다.\n기능 설명에 작성하진 않았지만, 단순히 전송하는 경우라면 문제없지만 다른 테이블과 JOIN해서 바로 전송하게끔 쿼리를 실행한다면 에러가 발생할 수 있습니다.\n이 문제를 회피하기위해서는 임시 테이블을 만들어 데이터를 저장한 뒤, 임시테이블을 사용해 데이터 전송하게끔 쿼리 작성해야합니다.\n이 외에도 제가 파악하지 못한 제약이 있을수도 있습니다.\n이처럼 많은 제약이 존재하지만, 별도로 Application을 만들 필요가 없다는게 큰 장점 입니다.\n이러한 방법도 여러 선택지중 하나로 사용할 수 있다는 것을 알게되는 계기가 되었으면 합니다.\n여기까지 읽어주셔서 감사합니다.\n","date":"2022-12-01T22:45:04+09:00","permalink":"https://korcasus.github.io/p/hive-kafka-integration-3/","title":"Hive Kafka Integration 3"},{"content":"개요 이전 글에서 Kafka와 Hive 연동에 사용하는 설정들에 대해서 알아보았습니다.\n이제부터는 Kafka에서 데이터를 읽어들이고, 쓰는 작업에 대해서 설명하려고 합니다. 이번 글에서는 Kafka에 있는 데이터들을 읽어들이는 것부터 진행해보도록 하겠습니다.\nHive를 Kafka Consumer로 사용하는 방벙리라 생각하시면 이해하기 쉬우실 겁니다.\n차이점이 있다면, 일반 Consumer와는 다르게 Consumer Group으로 지정되지 않습니다.\nJSON Kafka로부터 읽어들일 데이터들이 json format 일 경우를 가정하고, 진행하는 예제입니다.\nSample data 아래와 같은 형태의 데이터를 예시를 기준으로 설명드리도록 하겠습니다.\n필요에 따라 더 많은 필드들을 추가하실 수 있습니다.\n1 2 3 4 { \u0026#34;id\u0026#34;: 12345678, \u0026#34;value:\u0026#34;: \u0026#34;hello\u0026#34; } 테이블 정의 json 스키마와 동일한 구조를 가지는 테이블을 정의해보도록 하겠습니다. id 필드는 정수값을 가지고, value 필드는 문자열을 가지고 있습니다. Hive에서 정수값은 INT 또는 BIGINT Data Type으로 정의할 수 있습니다. 그리고 문자열은 STRING으로 정의합니다.\n이 정보들을 토대로 DDL쿼리를 만들어보면 아래와 같습니다.\n1 2 3 4 5 6 7 8 9 10 11 CREATE EXTERNAL TABLE test_export_to_kafka( `id` BIGINT, `value` STRING ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES( \u0026#34;kafka.serde.class\u0026#34; = \u0026#34;org.apache.hadoop.hive.serde2.JsonSerDe\u0026#34;, \u0026#34;kafka.topic\u0026#34; = \u0026#34;etl.hive.catalog.used\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;{broker서버}\u0026#34;, \u0026#34;kafka.consumer.security.protocol\u0026#34; = \u0026#34;PLAINTEXT\u0026#34; ); test_export_to_kafka은 테이블명으로, 원하는대로 사용할 수 있습니다.\n테이블에서 사용하는 컬럼들은 읽어들일 json 구조대로 작성해주면 됩니다.\n컬럼명은 json key와 동일하게 작성하고, 컬럼의 타입은 json value의 데이터 타입과 동일하게 맞춰주면 됩니다.\nhive 데이터 타입은 json보다 다양하므로, 문서를 참고해 적절한 타입을 선택해서 사용하는 것을 권장합니다. \u0026quot;kafka.serde.class\u0026quot; = \u0026quot;org.apache.hadoop.hive.serde2.JsonSerDe\u0026quot;으로 지정해줌으로서, hive가 json데이터를 deserialize할 수 있게 되었습니다.\n그 외에 특별히 TBLPROPERTIES에서 살펴봐야할 내용은 없습니다.\nkafka.topic만 사용하고 싶은 kafka topic명으로 변경해주면 됩니다.\nREAD 1 2 SELECT * FROM test_export_to_kafka 정상적으로 설정되었다면, topic에 들어있는 메시지가 출력되는것을 확인하실 수 있습니다.\n하지만 설정이 잘못되었다면 에러메시지가 출력되거나, 각 컬럼이 NULL로 출력 될 것입니다.\n여기까지 설명했던 내용과 다르게 설정한 내용이 있지않은지 확인해주세요.\nTEXT JSON처럼 구조화된 데이터가 아니라, PLAIN TEXT를 읽어오는 예제입니다.\nSample data PLAIN TEXT라, 예제도 특별할 것은 없습니다.\n단순히 숫자값을 예시로 사용해보도록 하겠습니다.\n1 2 3 1234 5678 91011 테이블 정의 1 2 3 4 5 6 7 8 9 10 CREATE EXTERNAL TABLE test_export_to_kafka( `id` BIGINT ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES( \u0026#34;kafka.serde.class\u0026#34; = \u0026#34;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\u0026#34;, \u0026#34;kafka.topic\u0026#34; = \u0026#34;etl.hive.catalog.used\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;{서버서버}\u0026#34;, \u0026#34;kafka.consumer.security.protocol\u0026#34; = \u0026#34;PLAINTEXT\u0026#34; ); 앞서 설명했던 JSON 테스트 케이스와 유사합니다.\n다른점이 있다면 Serde와 컬럼을 다르게 지정해준 것입니다.\nSerde의 경우 \u0026quot;kafka.serde.class\u0026quot; = \u0026quot;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\u0026quot; 로 변경하면 됩니다.\nREAD 1 2 SELECT * FROM test_export_to_kafka 정상적으로 설정되었다면, topic에 들어있는 메시지가 출력되는것을 확인하실 수 있습니다.\n하지만 설정이 잘못되었다면 에러메시지가 출력되거나, 각 컬럼이 NULL로 출력 될 것입니다.\n여기까지 설명했던 내용과 다르게 설정한 내용이 있지않은지 확인해주세요.\nKafka Metadata 활용 Kafka에는 메시지와 함께 메타 데이터들이 함께 기록됩니다.\n테이블을 정의할때 메타 데이터들도 자동으로 컬럼으로 추가됩니다. Hive에서 메시지를 읽어올때, 메타 데이터도 함께 가져오게 됩니다.\n메타 데이터가 컬럼으로 정의되다보니, SQL 조건문으로 활용할 수 있습니다.\n예를 들면 특정 시간 이후로 유입된 데이터만 가져오던가, 특정 파티션에 있는 데이터만 가져올 수 있습니다.\n__partition와 __offset의 경우에는 정수 타입을 사용하기 때문에 사용하기 편리합니다.\nex1) __partition = 0 ex2) __offset \u0026gt; 5000 하지만 __timestamp의 경우에는 unix_timestamp를 사용하기에 int64로 기록됩니다.\n이는 일상에서 사용하는 시간 표기방식과 다르고, 변환이 필요하기에 사용하기 불편합니다. 편하게 조건문으로 활용하기 위해서는 아래처럼 변경해서 사용하는것을 추천합니다.\nunix_timestamp라는 함수를 사용 하면 되는데 아래 예제를 보겠습니다.\nunix_timestamp('2022-03-08 00:00:00.000', 'yyyy-MM-dd HH:mm:ss.SSS') * 1000\n첫번째 인자는 변환하고 싶은 시간을 작성합니다. 두번째 인자는 첫번째 인자에 입력한 시간포맷을 작성합니다. 함수를 사용하여 특정 시간을 unix_timestamp로 변경 할 수 있게 되었고, __timestamp 비교 연산을 할 수 있게되었습니다. 하지만 hive로 통해 읽어들인 __timestamp는 마이크로초 단위이기 때문에 단위를 맞춰주는 추가연산이 필요합니다.\n이때문에 1000을 곱하였습니다. 아래는 사용 예제입니다.\nex) unix_timestamp('2022-03-08 00:00:00.000', 'yyyy-MM-dd HH:mm:ss.SSS') * 1000 \u0026lt; __timestamp\n추가 예제\n1 2 3 4 5 6 7 8 9 -- print to UTC timestamp select FROM_UTC_TIMESTAMP(`__timestamp`, \u0026#39;JST\u0026#39;) from test_kafka_product_received limit 1; -- print timestamp, unixtime to timestamp select current_timestamp(), from_unixtime(unix_timestamp(), \u0026#39;yyyy-MM-dd HH:mm:ss.SSS\u0026#39;); -- print converted unixtime select unix_timestamp(\u0026#39;2022-03-07 14:00:00.000\u0026#39;, \u0026#39;yyyy-MM-dd HH:mm:ss.SSS\u0026#39;); select unix_timestamp(\u0026#39;2022-03-07 06:11:41\u0026#39;, \u0026#39;yyyy-MM-dd HH:mm:ss\u0026#39;); 정리 이번 글을 통해 Hive에서 Kafka에 저장된 JSON 또는 PLAIN-TEXT를 읽어서 사용할 수 있게 되었습니다.\nmetadata를 필터링 용도로 사용한다면, Kafka에 원하는 조건을 가진 메시지를 손쉽게 추출해 볼 수 있겠습니다.\n아쉬운 점으로는 Hive에 저장되어있는 다른 테이블들과 join이 되지 않는다는 점입니다. (제가 시도했을때는 인증으로 인한 문제가 발생했습니다.)\n이 때문에 사용용도가 많이 제약되었지만, 개선된다면 활용도가 높을 것으로 보입니다.\n다음 글에서는 Hive데이터들을 Kafka로 전송하는 과정을 기록해보겠습니다.\n감사합니다.\n","date":"2022-12-01T22:45:01+09:00","permalink":"https://korcasus.github.io/p/hive-kafka-integration-2/","title":"Hive Kafka Integration 2"},{"content":"개요 Hive는 Hadoop에 저장된 대용량의 데이터들을 SQL 문법으로 읽고 쓸수있게 해주는 data warehouse software입니다.\n간단하게 Hadoop에서 동작하는 대용량 데이터 처리 Database라고 생각해 볼 수 있을것 같습니다.\n기본으로 HDFS를 지원하며, Hbase 또는 기타 DBMS에 접근할 수 있도록 개방적인 DataSourcing 기능을 제공하고 있습니다. Message Queue로 많이 활용되는 Kafka도 지원합니다. 이번 시리즈에서는 Hive와 Kafka 연동을 위한 설정부터 Message를 읽고 쓰는 과정까지 기록해보려고 합니다.\n이번 글에서는 사용할때 필요한 설정들과 알아야하는 정보들을 위주로 기록합니다. 사용 예시가 궁금하시다면 다음 글을 읽어주세요.\n공식문서를 토대로 작성하였습니다.\n문서에는 작성되어있지않으나, 사용하려면 추가로 알아야할 내용과 직접 테스트한 예제들을 상세히 기록하고자 합니다.\nHive 3.x 버전에서 진행하였습니다.\n설치 사실 별도의 설치는 필요하지않습니다.\n공식문서를 한번 살펴보셨다면 아시겠지만, Hive에 이미 내장 되어있습니다.\n어떻게 할 지 상상이 안될수도 있지만, SQL을 사용하면 Kafka에 연동을 할 수 있습니다.\n테이블 생성 Hive와 Kafka를 연동해주기 위해서는 연결고리를 만들어주어야합니다.\nSpring에서는 Binder와 Binding을 사용하면 손쉽게 연결고리를 만들 수 있듯이, Hive에서는 StorageHandler를 사용하여 만들 수 있습니다.\nStorageHandler는 table을 정의할 때 함께 명시하면 됩니다. 이 외에도 설정을 추가하면 완벽하게 사용하실수 있습니다.\n다만 Hive에서 Kafka와 연동된 테이블을 만들기 위해서는 몇가지 주의해야하는 사항이 있습니다.\n첫번째로는 EXTERNAL 테이블로 생성해야한다는 것입니다.\n우리가 사용할 방식은 Hive에서 Kafka에 접근하여 저장된 데이터들을 사용하는 것이기 때문에, EXTERNAL 키워드를 추가해주어야합니다.\n두번째로는 KafkaStorageHandler 를 명시해야합니다.\n테이블을 정의할 때 STORED BY 'org.apache.hadoop.hive.kafka.KafkaStorageHandler' 가 함께 적혀있어야 합니다.\n적혀 있어야만 Kafka를 연동하여 사용하실 수 있습니다.\n세번째로는 테이블설정을 해주어야합니다.\nTBLPROPERTIES 키워드 사용하면 테이블에 대한 Property를 설정할 수 있게됩니다. 여기에 Kafka 연동에 필요한 Configuration을 작성해야 합니다.\n세가지를 모두 적용하여 DDL을 작성해보면, 아래의 예시처럼 만들어질 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 CREATE EXTERNAL TABLE kafka_table ( `timestamp` TIMESTAMP, `page` STRING, `newPage` BOOLEAN, `added` INT, `deleted` BIGINT, `delta` DOUBLE ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES ( \u0026#34;kafka.topic\u0026#34; = \u0026#34;test-topic\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;localhost:9092\u0026#34; ); Kafka metadata Kafka에 저장된 Message는 Payload 외에도 Key, Partition, Offset, Timestamp 4개의 metadata가 존재합니다. 이 데이터들도 Hive에서 사용할 수 있습니다.\n앞서 테이블을 정의할 때 사용하였던 KafkaStorageHandler가 자동으로 metadata들을 컬럼으로 등록해 줍니다.\n__key (byte array) __partition (int32) __offset (int64) __timestamp (int64) 활용하는 방안은 실사용 예제를 작성한 다음글에서 자세히 설명드리도록 하겠습니다.\nAvro 포맷 사용시 주의사항 만약 Confluent Connector를 통해 Avro포맷을 사용한다면, 메시지에서 5-bytes를 제거해주어야합니다.\n이 5-bytes중 1-byte는 매직 byte, 4-byte는 schema registry의 schema id에 해당됩니다.\n이는 \u0026quot;avro.serde.type\u0026quot;=\u0026quot;skip\u0026quot; 그리고 \u0026quot;avro.serde.skip.bytes\u0026quot;=\u0026quot;5\u0026quot;를 설정해주면 해결할 수 있습니다.\navro를 사용할 경우, 상세한 내용은 가이드를 꼭 참고하면 좋겠습니다.\nconfluent avro format 지원은 Hive 4.0버전에 추가될 것으로 보입니다. 3.x 버전을 사용하시는 분이라면, 주의해주시면 좋을것 같습니다. (제가 삽질했던거라서요\u0026hellip;)\nSerde(Serializer, Deserializer) Application에서 Kafka로 데이터 전송을 해본 적 있다면, serializer, deserializer를 지정해보신 경험이 있으실 겁니다.\nHive에서도 마찬가지로 여러 serializer, deserializer를 제공하고있습니다.\nSupported Serializers and Deserializers description org.apache.hadoop.hive.serde2.JsonSerDe JSON 포맷 org.apache.hadoop.hive.serde2.OpenCSVSerde CSV 포맷 org.apache.hadoop.hive.serde2.avro.AvroSerDe AVRO 포맷 org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe binary..? org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe Plain Text 지정하는 방법은 생각보다 매우 간단합니다.\nTBLPROPERTIES에 추가하면 손쉽게 적용할 수 있습니다.\n예를 들면 Kafka에서 읽어들일 데이터포맷이 JSON의 경우이거나, 전송할 데이터가 JSON으로 하고 싶다면 아래처럼 추가하면 됩니다. \u0026quot;kafka.serde.class\u0026quot; = \u0026quot;org.apache.hadoop.hive.serde2.JsonSerDe\u0026quot;\n인증 Kafka에도 인증을 해야 데이터 읽고 쓸수 있는 권한을 주는 기능을 제공하고 있습니다.\nKafka 사용하는데 필요한 인증정보들 또한 Hive를 통해 제공할 수 있습니다.\n저의 경우에는 별도로 인증을 추가하지 않고 사용했다보니, 인증없이 사용한 경우로 기록하겠습니다.\n인증을 사용해야하는 상황이라면, 공식 문서를 읽어보고 진행해 주세요.\n인증을 위한 TBLPROPERTIES 값을 추가하지 않은 상태라면, 연결을 시도했을때 에러가 발생합니다.\n별도의 설정을 안해두었다면, 당연히 인증없이도 사용가능해야되는거 아닌가? 라고 생각할 수 있을텐데요. (제가 그랬습니다.) Hive는 기본값으로 id, password를 사용하여 인증을 시도하려 합니다.\n따라서 인증을 사용하지않는 설정을 추가해주어야만 사용이 가능한데요.\n공식문서에 이 문제를 해결하기 위한 방법을 따로 제시하지않아 많이 어려움을 겪었던 부분이었습니다. 결론부터 이야기하면 TBLPROPERTY를 추가하면 해결할 수 있습니다.\nproducer일 경우 : \u0026quot;kafka.producer.security.protocol\u0026quot; = \u0026quot;PLAINTEXT\u0026quot; consumer일 경우 : \u0026quot;kafka.consumer.security.protocol\u0026quot; = \u0026quot;PLAINTEXT\u0026quot; 인증에 상세히 알아보고 싶다면, 문서를 참고하시면 좋을것 같습니다.\n후에 서술하겠지만 설정할때 kafka.를 prefix로 지정해두면, kafka에 설정값이 전달할 수 있습니다.\nKey Serializer 앞서 설명드렸던 Serializer들은 Value Serializer 였습니다.\n예상하던것보다 많이 제공했지만 반대로 Key Serializer는 Byte Serializer밖에 지원하지 않습니다.\n다른 Serializer로 변경해서 사용하고 싶더라도 변경할 수 없게 되어 있습니다.\n왜 이렇게 구현한지는 모르겠지만, 자세한 내용은 코드를 보면 알 수 있습니다.\n궁금하실 수 있을 것 같아, 링크도 첨부합니다.\nproducer consumer 그외 지금까지 작성한 내용들 이외에도 Kafka를 사용할때, 추가하고 싶은 설정이 더 있을 수 있습니다.\n공식문서에서는 설명하진 않지만, Kafka에 사용할 설정값을 전달할 수 있습니다.\nproducer로 사용할 경우 “kafka.producer.{property}” = “{value}” consumer로 사용할 경우 “kafka.consumer.{property}” = “{value}” TBLPROPERTIES에 추가해주면, Kafka에 설정값들이 전달되며 원하는 설정을 적용할 수 있습니다. 어떻게 설정값이 전달될 수 있는지, 구현된 코드가 궁금하다면 아래의 링크를 참고하시면 좋겠습니다.\nproducer consumer 정리 여기까지 Hive에서 Kafka를 Read, Write하기 위해서 필요한 설정들과, 도움이 되는 정보들에 대해 알아 보았습니다.\n다음 글에서는 Kafka에 있는 Message들을 Hive에서 Read하는 과정을 기록해보겠습니다.\n감사합니다.\n","date":"2022-12-01T22:44:01+09:00","permalink":"https://korcasus.github.io/p/hive-kafka-integration-1/","title":"Hive Kafka Integration 1"},{"content":"들어가기에 앞서 안녕하세요.\nSynology DS220+ 구매한 뒤로 Bitwarden, WebDAV, \u0026hellip; 다양한 용도로 사용하고있습니다.\n이전까지는 DuckDNS와 함께 DSM 6.X 사용중이었습니다만, DSM 업데이트 이후로 만들어두었던 자동화 프로세스가 동작하지 않더라구요. 이번 기회에 갖고싶던 Domain을 CloudFlare에서 Domain 구매를 한 뒤, 새롭게 프로세스를 정리하였습니다.\n공식 가이드 대로 진행했던 과정을 순차적으로 정리하려고 합니다.\nDomain 구입과 DDNS 설정에 대한 내용은 기회가 되면 작성하도록 하겠습니다.\n사용한 환경 DSM 7.X Synology DS220+ Domain Provider : CloudFlare Let\u0026rsquo;s Encrypt란? HTTPS를 사용할 때는 CA(인증기관)에서 제공하는 인증서를 사용합니다.\n원하는 도메인으로 인증서를 발급하고자 할때, 업체마다 다른 가격을 지불해야합니다. Let\u0026rsquo;s Encrypt는 비영리 기관으로서 무료로 TLS인증서를 제공합니다.\n장점은 무료로 인증서를 제공받을 수 있지만,\n단점으로는 타업체는 유효기간이 1년인데 비해 짧은 3개월이라는 점입니다.\nacme.sh 는 뭔가요? Let\u0026rsquo;s Encrypt는 ACME 프로토콜을 사용하여 도메인 소유자인지 확인한 뒤에 인증서를 발급합니다. 인증서 발급을 위해서는 ACME 클라이언트 소프트웨어를 선택해야하는데요.\nCertBot 사용을 권장하지만, 상황에 따라 다른 소프트웨어를 선택할 수 있습니다. acme.sh는 bash를 사용해 인증서를 발급해 줄 수 있는 대체 소프트웨어입니다.\n본인의 선호도에 따라 선택할 수 있습니다.\n상세한 내용은 공식문서를 참고해주세요.\nCloudFlare API Token 발급 인증서 발급 절차에 대해 정확하게 알지는 못해도, 파악한 흐름은 아래와 같습니다.\n원하는 Domain의 인증서 발급 요청 (to Let\u0026rsquo;s Encrypt) 해당 Domain의 소유자임을 증명하기 위해 TXT Record값을 응답값으로 받음 CloudFlare API를 사용하여 TXT Record 값을 등록 TXT Record값을 확인한 후, 인증서 발급 위 과정에서 사용되는 CloudFlare API를 사용하기위해 Token 발급을 진행합니다.\nGlobal Key로도 사용가능하지만, 필요이상의 권한을 가지고 있기때문에 진행하진 않습니다.\ntoken을 발급하고자 하는 Domain을 선택하면, 아래의 같은 Home 화면이 나타납니다.\n빨간색 화살표를 클릭합니다.\nAPI Token 생성 버튼을 누르면, 아래의 같은 페이지가 나타나게됩니다.\n우리는 DNS 관련된 수정 권한을 Token을 발행해야하기 때문에, 빨간색 화살표의 버튼을 누릅니다.\n아래의 이미지에서 1번에 권한이 필요한 Domain을 선택합니다.\n그뒤에는 2번을 선택해 바로 생성하도록 합니다.\n정상적으로 진행되었다면, Token값을 확인할 수 있습니다.\n이 값은 더이상 볼 수 없기 때문에 잘 기억해주시기 바랍니다.\nACME 설치 및 인증서 발급 인증서 발급을 진행해줄 Client를 설치하도록 하겠습니다.\n아래의 명령어대로 설치를 진행하도록 합니다.\n계정은 root로 진행합니다.\n1 2 3 4 5 6 7 $ sudo su $ cd ~ $ wget https://github.com/acmesh-official/acme.sh/archive/master.tar.gz $ tar xvf master.tar.gz $ cd acme.sh-master/ $ ./acme.sh --install --nocron --home /usr/local/share/acme.sh --accountemail \u0026#34;email@gmailcom\u0026#34; $ source ~/.profile 이제 인증서 생성을 진행해 볼텐데요.\n아래의 환경 변수들을 만들어 줍니다.\n1 2 3 4 # 발급해둔 API Token export CF_Token=\u0026#34;MY_SECRET_TOKEN_SUCH_SECRET\u0026#34; # CloudFlare Email export CF_Email=\u0026#34;myemail@example.com\u0026#34; 아래의 명령어들을 실행해서 인증서를 생성해보겠습니다.\n1 2 3 4 5 6 $ cd /usr/local/share/acme.sh # 여기에 사용할 Domain을 지정 해줍니다. # Ex) *.my-domain.com $ export CERT_DOMAIN=\u0026#34;your-domain.tld\u0026#34; $ export CERT_DNS=\u0026#34;dns_cf\u0026#34; $ /usr/local/share/acme.sh/acme.sh --issue --home . -d \u0026#34;$CERT_DOMAIN\u0026#34; --dns \u0026#34;$CERT_DNS\u0026#34; 위의 마지막 명렁어를 실행했을때, 에러없이 마무리되었다면 정상적으로 된 것입니다.\n만약 email을 update하라는 문구가 출력된다면 아래처럼 진행해주세요.\n1 $ /usr/local/share/acme.sh//usr/local/share/acme.sh/acme.sh --register-account -m 이메일 --issue --home . -d \u0026#39;*.도메인\u0026#39; --dns \u0026#34;$CERT_DNS\u0026#34; 혹여나 문제가 생긴다면 댓글로 문의주세요.\n기본 인증서 교체 신규로 생성한 인증서를 기본 인증서로 교체해보겠습니다.\n1 2 3 4 5 $ cd /usr/local/share/acme.sh $ export SYNO_Username=\u0026#39;관리자 계정\u0026#39; $ export SYNO_Password=\u0026#39;관리자 비밀번호\u0026#39; $ export SYNO_Certificate=\u0026#34;인증서 설명\u0026#34; $ /usr/local/share/acme.sh/acme.sh --deploy --home . -d \u0026#34;$CERT_DOMAIN\u0026#34; --deploy-hook synology_dsm Success가 출력된다면, 정상적으로 마무리된 것입니다.\n시놀로지 제어판-\u0026gt; 보안 -\u0026gt; 인증서로 들어가서 잘 등록되었는지 확인해봅니다.\n또는 브라우저에서 DSM에 접속한 뒤 새로고침해서 인증서 정보를 확인해주세요.\n만약 OTP를 설정해두셨다면, 위 과정을 진행하면서 SYNO_TOTP_SECRET 값을 설정해주라는 출력과 함께 에러가 발생할 것입니다.\n관리자 계정으로 로그인 할때, OTP 인증을 거치게 설정을 해두었기때문에 당연하다고 볼 수 있습니다. 출력된 메시지대로 값을 지정해주는 것도 방법이겠지만, 중요한 정보이기 때문에 선뜻 작성하기엔 내키지 않습니다. (관리자 뚫리면 어차피 소용없지만, 괜히 그렇더라구요..;;)\n이를 대체할 수 있는 값이 있는데, SYNO_DID 입니다.\nDeviceID값을 의미합니다. 이 값을 얻기위해서는 DSM에 로그인시 Remember this device에 체크한 후, 관리자 계정으로 로그인하면 Cookie로 저장됩니다.\n개발자 모드로 들어가서 이 값을 복사한뒤, 환경변수로 지정해 줍니다.\n위의 설명한 방식은 http 프로토콜을 사용하여 localhost에 인증서 교체 요청을 합니다.\n특별히 문제될건 없지만, 혹~~~~~시나 무조건 https 프로토콜을 사용해야한다!!! 라고 생각하시는 분이 계시다면\n아래의 2개 환경변수를 지정해준뒤, --insecure argument를 추가한뒤 실행해주면 https 프로토콜을 사용하게 됩니다.\n1 2 3 4 # 앞서 지정한 환경변수도 필요로 합니다. $ export SYNO_Scheme=\u0026#34;https\u0026#34; $ export SYNO_Port=\u0026#34;5001\u0026#34; $ /usr/local/share/acme.sh/acme.sh --deploy --insecure --home . -d \u0026#34;$CERT_DOMAIN\u0026#34; --deploy-hook synology_dsm 인증서 갱신 스케줄러 설정 인증서 유효기간이 3개월이기 때문에, 3개월마다 갱신작업을 해주어야합니다.\n하지만 이 작업을 매번 수행하기엔\u0026hellip; 놀랍도록 귀찮습니다.\n만료일에 가까워지면 갱신해야되는 초조함은 덤 입니다.\n이 번거로운 일에서 벗어나기 위해 자동화를 하겠습니다.\n제어한 -\u0026gt; 서비스 -\u0026gt; 작업 스케줄러 -\u0026gt; 생성 -\u0026gt; 예약된 작업 -\u0026gt; 사용자 정의 스크립트를 순서대로 선택합니다.\n이미지로는 아래와 같습니다. 빨간원 숫자 순서대로 진행합니다.\n하나의 팝업창 나타날텐데요.\n이미지처럼 3가지 값을 확인해줍니다.\nDSM6 버전까지는 root로 진행해야 인증서 교체가 가능했는데요.\n7 버전에서 다른계정으로 실행해도 되는지 테스트 해보진 않았습니다.\n다음으로는 언제실행할지 스케줄을 지정할 차례입니다.\n이미지처럼 설정할 경우 매주 토요일 오전 1시에 갱신을 시도하려고 할겁니다.\n선호하는 요일, 시간을 지정해주세요.\n매달 실행할 수 있게 변경 가능하지만, 갱신은 만료 1개월 전부터 가능합니다.\n첫번째이자 마지막 갱신 시도하는 날에 모종의 이유로 실패할 경우, 위에서 진행하셨던걸 다시해주셔야합니다.\n그래서 안전하게 매주 실행하도록 설정했습니다. 다음으로는 실행할 스크립트를 지정하는 단계입니다.\n알림 설정해두신 분이라면, 실행 상세정보를 이메일로 보내기를 체크해주면 좋습니다.\n잘 실행되는걸 메일 오는것으로 확인할 수 있을 테니까요.\n저의경우에는 메일 대신에 텔레그램으로 메시지 오도록 변경했습니다. 이와 관련해서는 따로 포스트 하겠습니다.\n사용자 정의 스크립트는 이미지처럼 인증서를 갱신하는 스크립트를 넣어줍니다.\n간단합니다. 한줄의 커맨드만 입력해주면 됩니다.\n1 /usr/local/share/acme.sh/acme.sh --cron --home /usr/local/share/acme.sh/ 이제 설정 완료하시면 아래의 같은 팝업이 출력됩니다.\n사뿐히 확인버튼 눌러주시면 됩니다.\n알림 외에도 실행한 로그를 확인하고 싶으실수 있는데요.\n아래 이미지처럼 로그 경로를 설정할 수 있습니다. 저의경우에 log 디렉토리로 지정했습니다.\n이와같이 지정했을 경우 스케줄러 로그는 /volume1/log/synoscheduler 경로에 저장됩니다.\n우리가 실행한 작업의 로그는 /volume1/log/synoscheduler/${task_number}/${실행시간} 경로에서 확인하실수 있습니다.\ntask_number는 스케줄러가 몇번째로 만들어졌는지 의미하는 값으로 보입니다.\noutput.log는 실행하며 출력된 결과이고, script.log는 실행한 스크립트를 볼 수 있습니다.\n정리 왜 이렇게 까지 해야되나 궁금해 하실수도 있습니다.\n집에서만 사용하도록 하신경우라면 이 과정 전혀 필요없습니다. 왜냐하면 외부에서 접근이 불가능하니까요.\n하지만 외부에서 접근을 허용하는 경우라면 이야기가 달라집니다.\n설정을 해두신 시점 이후로, 수시로 타국가에서 접근을 시도합니다. 보안 허점을 찾으려고 하는 거죠. http 통신은 보안상 문제가 있기 때문에, 이 상황에서 http를 사용해서 외부에서 접근하면\u0026hellip; 더 이야기 안해도 아실거라 생각합니다.\n이러한 이유로 https를 사용하게된거고, 최소한의 방어수단을 갖춘 셈입니다.\n여튼 여기까지 진행하셨다면, CloudFlare의 Domain에 TLS인증서 기반의 https를 사용할 수 있는 환경을 만드신 겁니다. 진행하느라 고생 많으셨습니다.👏👏\n저도 이번에 새롭게 설정하다보니, 자동 갱신이 잘 되는지는 확인하진 못했습니다.\n2개월 뒤에 내용을 보충 하도록 하겠습니다.\n","date":"2022-11-21T22:23:00+09:00","permalink":"https://korcasus.github.io/p/automate-synology-lets-encrypt-certificate/","title":"Automate Synology Let's Encrypt Certificate"},{"content":"Hugo Blog 만들기-2 개요 지난번 글을 통해, Hugo로 Blog를 만들어 보았습니다.\n이번에는 블로그 운영하는데 있어, 필수라고 생각되는 내용들을 적용해보도록 하겠습니다.\nCustom 설정하기 Stack 테마를 Git SubModule로 등록하여 블로그를 만들었습니다.\n사용하는건 좋지만 입맛에 맞게 수정하려면 테마를 변경해 주어야합니다.\n하지만 직접 수정한다면 테마를 업데이트할때마다 충돌이 발생할테고, SubModule로 등록한 의미가 퇴색 됩니다. 이를 방지하기 위한 방법을 설명하겠습니다.\nthemes/hugo-theme-stack/layouts/partials/header.html 파일을 수정해야하는 상황을 가정해보겠습니다. 이 파일을 layouts/partials/header.html로 복사한뒤, 원하는 대로 수정합니다.\nbuild를 할 경우 복사해온 파일을 사용하게 됩니다.\nasset과 static도 동일합니다.\n테마에 사용된 동일한 디자인의 아이콘을 추가할때 위와같은 방법으로 진행할 수 있습니다.\n좀 더 세심히 수정해보고 싶으시다면, 이 글을 참고하시면 큰 도움이 될것 같습니다.\nGoogle 검색 노출 해당 글을 참고하였습니다 구글 검색 결과에 노출되기 위해서는, 구글 검색엔진에 우리 홈페이지도 검색되게해줘~~ 라고 요청을 해야합니다.\n이 작업은 홈페이지 소유자가 Google Search Console 요청할 수 있습니다.\n홈페이지 형태에 따라 소유자임을 증명하는 방법이 달라집니다.\n시작하기를 누르면 아래와 같은 페이지가 나타납니다. URL 접두어에 Github blog 주소를 작성해줍니다. 도메인을 만들어서 운영중이시라면 도메인으로 생성해주세요.\nURL 접두어로 신청했다면, 해당 페이지의 소유자인지 확인해 주어야 합니다. HTML파일을 다운로드하여, static 디렉토리에 추가해줍니다. build하면 자동으로 public 디렉토리에 포함됩니다.\n그리고 public 디렉토리에 sitemap.xml이 존재해야 합니다.\n만약 없다면, build를 한번 해주시면 됩니다.\n\u0026lt;계정\u0026gt;.github.io/sitemap.xml로 접속했을때, 어떤 결과들이 나온다면 정상적으로 된 겁니다.\n검색 노출되기까지는 시간이 좀 걸리니, 느긋하게 기다린뒤 Search Console에서 확인해 봅시다.\nGoogle Analytics 적용 Github 으로 블로그를 운영할 경우, 평균 방문자 수가 얼마인지 간단한 통계조차도 볼 수 없습니다. 이러한 지표는 블로그 운영하는데 있어 재미(?)를 느끼게 하는데 큰 역할을 한다고 생각합니다.\n이런 제약사항을 해결하고, 블로그에 접속하는 사용자들을 분석(?)해보고 싶어 Google Analytics 를 적용해보았습니다.\n해당 글을 참고하였습니다 진행하시기 전에, 광고차단을 해제해두시는것을 추천합니다. (적용되었는지 확인하는데 문제가 생길 수 있습니다.) Google Analytics 신청 상세한 신청 과정을 설명하자니, 이미 진행해버려 동일하게 설명하긴 어려울 것 같습니다.\n개인정보도 포함되어있구요.\n대신 저보다 더 상세히 작성해주신 분들도 많으니, 참고해서 신청해주시면 됩니다.\n애널리틱스 계정, 속성 및 앱 까지 모두 신청하셨다면 사전 준비는 모두 되었다고 생각하시면 됩니다.\n이제는 analytics가 적용된 script와 GA Tracking ID를 확인해 보겠습니다.\n애널리틱스를 적용할 속성 및 앱의 데이터스트림을 생성해 줍니다.\n생성해준 뒤 웹 스트림 세부정보를 들어가면 아래와 같은 이미지를 보실 수 있습니다.\n측정 ID 아래에 적힌 값이 GA Tracking ID를 의미합니다.\nscript는 빨간색 화살표를 클릭해보면 확인할 수 있습니다.\n클릭해보면 아래와 이미지의 섹션에서 확인할 수 있습니다.\n모자이크 한 부분이 analytics를 적용할 수 있는 script입니다.\nScript와 GA Tracking ID 잘 저장해주세요.\nInternal Template을 사용하여 Google Analytics 활성화 Hugo 공식 Guide에서는 config.yaml에 설정만 해준다면, 손쉽게 적용할수 있다고 안내 합니다. 설정 하는방법은 아래와 같습니다.\n1 googleAnalytics: G-MEASUREMENT_ID blog에 배포한 뒤 실시간 접속자 counting이 되는지 확인해 봅니다. 뒤에서 설명하겠지만, 홈 화면에서 실시간 접속자 수를 확인할 수 있습니다. 이 단계에서 잘 적용되었다면, Script 수동 설치는 진행하지 않아도 됩니다.\nScript 수동 설치 신청하며 복사해두었던 script를 layout에 추가하여 적용합니다.\nscript를 복사하며 잘 살펴보셨다면 아시겠지만 head 태그에 추가해야된다고 안내하고 있습니다.\n사용중인 테마의 head 태그 부분을 확인해보겠습니다. (stack 테마를 기준으로 설명합니다.)\n우선 블로그를 구성하는 가장 상위(?)의 html을 확인해보겠습니다. themes/hugo-theme-stack/layouts/_default/baseof.html 입니다.\n현 기준으로는 아래처럼 구성되어있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;{{ .Site.LanguageCode }}\u0026#34; dir=\u0026#34;{{ default `ltr` .Language.LanguageDirection }}\u0026#34;\u0026gt; \u0026lt;head\u0026gt; {{- partial \u0026#34;head/head.html\u0026#34; . -}} {{- block \u0026#34;head\u0026#34; . -}}{{ end }} \u0026lt;/head\u0026gt; \u0026lt;body class=\u0026#34;{{ block `body-class` . }}{{ end }}\u0026#34;\u0026gt; {{- partial \u0026#34;head/colorScheme\u0026#34; . -}} {{/* The container is wider when there\u0026#39;s any activated widget */}} {{- $hasWidget := false -}} {{- range .Site.Params.widgets -}} {{- if gt (len .) 0 -}} {{- $hasWidget = true -}} {{- end -}} {{- end -}} \u0026lt;div class=\u0026#34;container main-container flex on-phone--column {{ if $hasWidget }}extended{{ else }}compact{{ end }}\u0026#34;\u0026gt; {{- block \u0026#34;left-sidebar\u0026#34; . -}} {{ partial \u0026#34;sidebar/left.html\u0026#34; . }} {{- end -}} \u0026lt;main class=\u0026#34;main full-width\u0026#34;\u0026gt; {{- block \u0026#34;main\u0026#34; . }}{{- end }} \u0026lt;/main\u0026gt; {{- block \u0026#34;right-sidebar\u0026#34; . -}}{{ end }} \u0026lt;/div\u0026gt; {{ partial \u0026#34;footer/include.html\u0026#34; . }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; head 태그 안에 추가하기위해 head/head.html를 확인해보겠습니다.\nthemes/hugo-theme-stack/layouts/partials/head/head.html을 보시면 됩니다.\n아래처럼 만들어져 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \u0026lt;meta charset=\u0026#39;utf-8\u0026#39;\u0026gt; \u0026lt;meta name=\u0026#39;viewport\u0026#39; content=\u0026#39;width=device-width, initial-scale=1\u0026#39;\u0026gt; {{- $description := partialCached \u0026#34;data/description\u0026#34; . .RelPermalink -}} \u0026lt;meta name=\u0026#39;description\u0026#39; content=\u0026#39;{{ $description }}\u0026#39;\u0026gt; {{- $title := partialCached \u0026#34;data/title\u0026#34; . .RelPermalink -}} \u0026lt;title\u0026gt;{{ $title }}\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026#39;canonical\u0026#39; href=\u0026#39;{{ .Permalink }}\u0026#39;\u0026gt; {{- partial \u0026#34;head/style.html\u0026#34; . -}} {{- partial \u0026#34;head/script.html\u0026#34; . -}} {{- partial \u0026#34;head/opengraph/include.html\u0026#34; . -}} {{- range .AlternativeOutputFormats -}} \u0026lt;link rel=\u0026#34;{{ .Rel }}\u0026#34; type=\u0026#34;{{ .MediaType.Type }}\u0026#34; href=\u0026#34;{{ .Permalink | safeURL }}\u0026#34;\u0026gt; {{- end -}} {{ with .Site.Params.favicon }} \u0026lt;link rel=\u0026#34;shortcut icon\u0026#34; href=\u0026#34;{{ . }}\u0026#34; /\u0026gt; {{ end }} {{- template \u0026#34;_internal/google_analytics.html\u0026#34; . -}} {{- partial \u0026#34;head/custom.html\u0026#34; . -}} script를 추가해야되니까 head/script.html에 추가하면 좋을것으로 보이네요. 실제로 파일을 확인해보겠습니다. 경로는 themes/hugo-theme-stack/layouts/partials/head/head.html 입니다.\n열어보면 아시겠지만, 비어있습니다.\n무슨 내용이 있었다면 custom하며 보완해주어야겠지만, 비어있으므로 걱정할 필요가 없습니다.\nlaygouts/partials/head/script.html을 만들어, 복사해둔 스크립트를 추가해주면 됩니다.\n적용 확인 애널리틱스 홈 화면을 들어갑니다.\n정상 적용되었다면, 지난 30분 동안의 사용자에 counting 됩니다.\n정리 첫번째 글로 Hugo로 Static Site Generate, 테마적용, 댓글 시스템 연동까지 진행해보았고 이번 글을 통해 블로그를 고도화(Custom 방법, 구글 검색노출, 구글 analytics) 해보았습니다.\n다음 글은 애드센스와 github action 적용을 주제로 작성할 예정입니다.\n감사합니다.\n","date":"2022-11-15T20:23:45+09:00","permalink":"https://korcasus.github.io/p/hugo-blog-%EB%A7%8C%EB%93%A4%EA%B8%B0-2/","title":"Hugo Blog 만들기-2"},{"content":"Hugo Blog 만들기-1 Hugo란? Go로 구현된 빠르고 현대적인 Static Site Generator입니다.\nVisitor Request가 발생할때마다 동적으로 페이지를 생성하는 시스템과는 다르게, Content를 만들거나 업데이트할때 Build가 됩니다. 사전에 Build된 Page를 보여주기 때문에, viewer에게 최적의 경험을 제공해줄 수 있습니다. Hosting에도 제약이 없으며, CDN에도 문제없이 동작합니다.\nHugo는 Database가 필요없고 Ruby, Python or PHP와 같은 Expensive Runtime에 Dependency가 없습니다.\n특징을 정리해보면 아래와 같습니다.\nBuild가 극단적으로 빠르다. (페이지마다 1ms 이하) cross platform 지원 liveReload지원 강력한 테마 어디서든 Hosting할 수 있다. … 상세한 내용은 Hugo 문서를 참고해주세요.\nHugo 설치 Mac을 주력으로 사용하기 때문에, Mac 기준으로 설명하겠습니다.\n타 운영체제를 사용한다면, Reference를 참고해서 설치를 진행해주세요.\nPrerequisites Git Hugo module Git submodule … GoLang ≥ 1.18 Go를 사용하며, Git을 통해 Submodule을 관리하기때문에 위의 2개를 사전에 설치해주어야합니다.\n1 2 3 4 5 6 # Hombrew Package manager 사용 $ brew install hugo # 정상적으로 설치되었는지 확인 $ hugo version hugo v0.104.3+extended darwin/arm64 BuildDate=unknown Apple Silicon을 사용중이라면, 위처럼 darwin/arm64로 나타나는지 확인해줍니다.\nGithub repository 2개 생성 User Github Page로 사용할 Repository와 Markdown을 기록할 Repository를 구분하기 위한 용도로 나누어 사용하기 위해서 입니다.\n1개의 Repository로 통합해도 되지만, 작성해둔 content를 쉽게 copy해갈 수 있으므로 사전에 방지하기 위해 구분하여 사용하려 합니다.\n저의 경우에는 아래의 2개의 respository를 생성했습니다.\ntechnical-blog .github.io (ex. korcasus.github.io) Hugo로 프로젝트 만들기 Hugo를 사용해서 default project를 만들어줍니다.\n생성되는 프로젝트는 아래처럼 구성됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 $ hugo new site technical-blog $ tree . ├── archetypes ├── config.toml ├── content ├── data ├── layouts ├── public ├── static └── themes 자세하게 알아보고 사용하고 싶다면 Reference를 한번 읽어보는걸 권장합니다.\n블로그에 이쁜 옷을 입혀줄 차례입니다.\n본인이 직접 테마를 만들것이 아니라면, 공개된 테마를 사용하는게 좋습니다.\n다양한 테마가 존재하므로, 문서에서 찾아보고 본인에 맞는 테마를 사용하도록 합니다.\n저의 경우에는 Stack이 가장 이쁘고, 문서화가 잘 되어있다고 생각했습니다.\n그래서 Stack을 기준으로 블로그 구현한 것을 설명하겠습니다.\n1 2 3 4 5 $ cd technical-blog # 다른 테마의 경우 # git submodule add https://github.com/${테마Repository}.git themes/${테마이름} $ git submodule add https://github.com/CaiJimmy/hugo-theme-stack/ themes/hugo-theme-stack 위에처럼 Submodule로 추가해둔다면, 추후 테마 업데이트 하기가 쉬워집니다.\nHugo에 Module이라는 것도 있는데, 이 방법으로도 대체 가능합니다. 관심있다면 적용해보는것도 좋아보입니다.\n테마가 정상적으로 설치되었다면, 테마의 설정파일을 사용하기 위해 기본 config file을 복사해와야 합니다.\n${project}/themes/hugo-theme-stack/exampleSite/config.yaml 파일을 복사해 ${project}/config.yaml 에 옮겨줍니다.\nGithub Page로 사용하기위해서는 config.yaml의 일부를 변경해주어야 합니다.\n아래의 예시에 주석에 해당되는 부분을 변경해주도록 합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Github page로 사용할 URL로 변경해주어야합니다. baseurl: https://korcasus.github.io/ languageCode: en-us theme: hugo-theme-stack paginate: 5 title: Read Write languages: en: languageName: English title: Read Write weight: 1 disqusShortname: hugo-theme-stack ... 추가한 테마가 정상적으로 동작하는지 project 디렉토리에서 build 및 서버 실행을 해보도록 하겠습니다.\n이미 어느정도 blog를 만든뒤에 작성하는거라, 완전 아래와 동일하게 나오진 않지만 비슷하게 출력되었던 것으로 기억합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 $ hugo server -D Start building sites … hugo v0.104.3+extended darwin/arm64 BuildDate=unknown | EN -------------------+----- Pages | 41 Paginator pages | 2 Non-page files | 4 Static files | 0 Processed images | 18 Aliases | 18 Sitemaps | 1 Cleaned | 0 Built in 1494 ms Watching for changes in /Users/user/Workspace/technical-blog/{archetypes,assets,content,data,layouts,static,themes} Watching for config changes in /Users/user/Workspace/technical-blog/config.yaml, /Users/user/Workspace/technical-blog/themes/hugo-theme-stack/config.yaml Environment: \u0026#34;development\u0026#34; Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop Build된 결과를 보기 위해 http://localhost:1313 으로 접속했을때 페이지가 보인다면, 여기까지는 잘 진행되었다고 판단하시면 됩니다.\nGit Repo 연결 및 ShellScipt 작성 Hugo로 만들어진 Project를 개인(Privarte) Repository에 등록하고, Hugo로 Build된 결과를 Github Page로 연동하는 작업입니다.\n그 뒤로는 배포에 사용하기 위한 ShellScript에 대해 설명하겠습니다.\n아래의 명령어들을 실행하면, 처음에 만들어둔 2개 Repository에 용도에 맞게 사용하게 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 # 현재 위치 확인 $ pwd /Users/user/Workspaces/blog # blog -\u0026gt; blog 레포지토리 연결 # git remote add origin http://github.com/\u0026lt;username\u0026gt;/technical-blog.git $ git remote add origin http://github.com/korcasus/technical-blog.git # blog/public -\u0026gt; \u0026lt;username\u0026gt;.github.io 연결 # git submodule add -b master http://github.com/\u0026lt;username\u0026gt;/\u0026lt;username\u0026gt;.github.io.git public $ git submodule add -b master http://github.com/korcasus/korcasus.github.io.git public 발행할 Contents를 작성을 완료한 후, Hugo로 Build해주는 과정이 필요로 합니다.\nBuild된 결과물은 public 디렉토리에 생성됩니다. 이를 Github Page에 반영해주는 과정이 필요로 합니다.\n매번 따로 수행하기보다는 배포 스크립트로 만들어 실행하는것이 운영하기 쉽습니다.\n저의 경우에는 아래의 스크립트를 통해, 배포를 합니다. (Reference)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #!/bin/bash echo -e \u0026#34;\\033[0;32mDeploying updates to GitHub...\\033[0m\u0026#34; # Build the project. # hugo -t \u0026lt;여러분의 테마\u0026gt; hugo -t hugo-tranquilpeak-theme # Go To Public folder, sub module commit cd public # Add changes to git. git add . # Commit changes. msg=\u0026#34;rebuilding site `date`\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; # Push source and build repos. git push origin master # Come Back up to the Project Root cd .. # blog 저장소 Commit \u0026amp; Push git add . msg=\u0026#34;rebuilding site `date`\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; git push origin master 1 2 3 4 5 # deploy.sh 실행 파일 권한 부여 $ chmod 777 deploy.sh # 배포 실행 $ ./deploy.sh Post 작성 발행할 글을 작성할때 아래처럼 명령어를 입력하면 Hugo에서 Markdown을 생성해줍니다.\n1 2 3 4 5 6 7 8 9 10 11 12 $ cd $project # hugo new ${content내 directory}/${사용할 파일명}.md $ hugo new post/hugo-blog-2/index.md Content \u0026#34;/Users/user/Workspace/technical-blog/content/post/hugo-blog-2/index.md\u0026#34; created $ cat content/post/hugo-blog-2/index.md --- title: \u0026#34;Hugo Blog 2\u0026#34; date: 2022-11-14T23:58:09+09:00 draft: true --- 주의해야할 점이 2가지 있습니다.\ncontent directory내에 생성된다는 점입니다. 확장자를 md로 해야합니다. 첫번째는 생성되는 경로를 정확히 파악하기 위해 주의해주는 것이 좋습니다.\n두번째는 확장자를 md로 하지않을 경우, 명령어로 post생성할때 에러가 발생합니다.\nHugo에서 archetypes 디렉토리를 포함하고있는데, default.md 파일이 존재합니다.\n명령어를 통해 확장자 md파일을 만들때, 기본값으로 사용됩니다.\n하지만 여기에 포함되지않은 확장자 파일을 만들경우, 기본값으로 사용할 파일이 존재하지않으므로 에러가 발생합니다. default.md 파일을 확인해보면 아래와 같습니다.\n1 2 3 4 5 --- title: \u0026#34;{{ replace .Name \u0026#34;-\u0026#34; \u0026#34; \u0026#34; | title }}\u0026#34; date: {{ .Date }} draft: true --- Utterences(Github 댓글 위젯) 연동 Stack 테마에서는 아래와 같은 댓글 시스템들을 지원합니다. (참고)\nCactus Cusdis Disqus DisqusJS Giscus Gitalk Remark42 Twikoo utterances Vssue Waline 이 중에 OpenSource이고, github과 연동하여 쉽게 사용가능한 Utterences를 사용합니다.\n사용하기 위해서는 별도의 Repository가 추가로 필요합니다. 여기서 사용한 Repo이름은 blog-comments 입니다.\n첨부한 GIF대로 진행하시면 됩니다. 잊지말고 utterances app연동 진행해주세요!\nBlog Post와 Issue Mapping 방식을 다르게 하고싶다면, 선호하시는 것으로 선택하시면 됩니다. 만들어진 Script를 테마에 적용해보도록 하겠습니다.\nblog/themes/${테마}/layouts/partials/comments/provider 경로에 사용하고자 하는 댓글 시스템에 직접 수정방법도 있습니다.\n하지만 이는 추후 테마 업데이트 하는데 문제가 발생할 수 있으므로 제외합니다.\n테마에서 제공하는 config.yaml를 수정해서 댓글 시스템에 적용해보겠습니다.\n프로젝트 루트 디렉토리에 복사해둔 config.yaml을 사용합니다. 아래처럼 comments를 사용할건지, 사용할 provider를 지정해줄 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 params: comments: enabled: true provider: disqus disqusjs: shortname: apiUrl: apiKey: admin: adminLabel: utterances: repo: issueTerm: pathname label: 추출한 Script를 적용하면 아래처럼 됩니다.\n만약 Issue Mapping을 설명한 방법과 다르게 하셨다면 blog/themes/${테마}/layouts/partials/comments/provider/utterances.html 를 확인해서 Script에 맞게끔 값을 입력해주세요.\n1 2 3 4 5 6 7 params: comments: enabled: true provider: utterances utterances: repo: Korcasus/blog-comments issueTerm: title 정상적으로 만들어졌다면 게시글 하단에 아래와 같이 추가되어있는것을 보실 수 있습니다.\n댓글이 잘 작성되는지 확인해보겠습니다.\n로그인후 테스트 댓글을 작성해보았습니다.\n작성된 댓글이 Github Issue로 잘 등록되었습니다.\n정리 여기까지 Hugo로 Static Site Generate, 테마적용, 댓글 시스템 연동까지 진행해보았습니다.\n다음에는 만든 블로그를 고도화(Custom 방법, 구글 검색노출, 구글 analytics) 해보겠습니다.\n감사합니다.\n","date":"2022-11-14T23:11:35+09:00","permalink":"https://korcasus.github.io/p/hugo-blog-%EB%A7%8C%EB%93%A4%EA%B8%B0-1/","title":"Hugo Blog 만들기-1"}]