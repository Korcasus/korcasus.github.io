[{"content":"Introduction Kubernetes에서는 Batch성격의 Application들을 지원하고 있습니다.\nJob, CronJob이라는 형태로 실행할 수 있습니다.\n하지만 실행이력을 관리하거나, 여러 job들의 의존성을 설정하여 실행하기에는 기능이 부족합니다. Argo Workflow와 Argo Event가 이러한 기능들을 충분히 지원하는지 확인하기 위해, 설치하여 테스트를 진행해보았습니다.\nInstall Argo-Event 추후 사용을 고려해서, namespace단위 설치가 아닌 cluster-wide로 설치합니다.\nnamespace 생성 1 kubectl create namespace argo-events Deploy Argo Events, SA, ClusterRoles, Sensor Controller, EventBus Controller and EventSource Controller. 1 2 3 kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/manifests/install.yaml # Install with a validating admission controller kubectl apply -f https://raw.githubusercontent.com/argoproj/argo-events/stable/manifests/install-validating-webhook.yaml Deploy Eventbus 1 kubectl apply -n argo-events -f https://raw.githubusercontent.com/argoproj/argo-events/stable/examples/eventbus/native.yaml Argo-workflow Controller And Server 1 2 kubectl create namespace argo kubectl apply -n argo -f https://github.com/argoproj/argo-workflows/releases/download/v3.3.0/install.yaml getting-started를 보고 설치를 진행하게되면, namespace에 한정되어 설치됩니다.\n이는 argo-event 사용 불편을 초래하기에, 특별한 이유가 없다면 cluster-wide로 설치하는것을 추천합니다.\nInstall CLI 1 2 3 4 5 6 7 8 9 10 11 12 13 14 # Download the binary curl -sLO https://github.com/argoproj/argo-workflows/releases/download/v3.3.0/argo-darwin-amd64.gz # Unzip gunzip argo-darwin-amd64.gz # Make binary executable chmod +x argo-darwin-amd64 # Move binary to path mv ./argo-darwin-amd64 /usr/local/bin/argo # Test installation argo version Port-Forwarding (local) 1 kubectl -n argo port-forward deployment/argo-server 2746:2746 Remote Cluster를 사용할경우 링크를 보고 Routing 설정을 해주면 됩니다. Local에서는 Load Balancer타입으로 진행하면 됩니다.\nAuthorization 링크 방법을 응용해서 기존에 만들어진 token을 획득후, 로그인에 사용하면 됩니다.\nConcept Architecture Event Source 외부에서 오는 이벤트를 처리하기위한 Resource입니다.\nSensor 센서는 Event Dependency Set을 Input으로, Trigger는 Output 정의합니다. EventBus를 통해서 Event를 수신하여 처리하는 Event Dependency Manager로서 행동합니다. 그리고 Trigger를 실행합니다.\nEventbus EventSources와 Sensor를 연결해주는 TransportLayer로서 수행합니다.\nEventSources가 Event를 Publish하는반면, Sensor는 Event를 Subscribe하여 Trigger를 실행합니다.\nTrigger Sensor에 의해서 한번 Event Dependencies가 해결되면, 실행되는 Resource/Workload를 의미합니다.\n테스트한 기능 calendar webhook-workflow webhook-k8s-object cross-namespace webhook-auth RBAC 설정 argo-workflow와 argo-events를 사용한다는 전제하에 설명합니다.\nargo-workflow에서 실행한 pod 상태, 로그를 확인하기 위해서 권한을 주어야합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: workflow-role namespace: argo-events rules: - apiGroups: - \u0026#34;\u0026#34; resources: - pods verbs: - get - watch - patch - apiGroups: - \u0026#34;\u0026#34; resources: - pods/log verbs: - get - watch --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: workflow-role-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: workflow-role subjects: - kind: ServiceAccount name: default Argo Events에서 Trigger로서 Argo-Workflow를 사용하고자 한다면 필요한 설정입니다.\nTrigger로 생성되는 객체가 Workflow이기 때문에, 해당 타입의 api-resource를 사용할수 있도록 권한이 필요합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 apiVersion: v1 kind: ServiceAccount metadata: name: operate-workflow-sa namespace: argo-events --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: operate-workflow-role namespace: argo-events rules: - apiGroups: - argoproj.io verbs: - \u0026#34;*\u0026#34; resources: - workflows - workflowtemplates - cronworkflows - clusterworkflowtemplates --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: operate-workflow-role-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: operate-workflow-role subjects: - kind: ServiceAccount name: operate-workflow-sa webhook-workflow Webhook을 EventSource로 사용하며, Sensor는 argo-workflow를 Trigger로 사용합니다. 이에 따라, 동작을 시키기 위해서는 argo-workflow cluster wide로 사전 설치를 해야합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: argoproj.io/v1alpha1 kind: EventSource metadata: name: webhook spec: service: ports: - port: 12000 targetPort: 12000 webhook: example: port: \u0026#34;12000\u0026#34; endpoint: /example method: POST 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 apiVersion: argoproj.io/v1alpha1 kind: Sensor metadata: name: webhook spec: template: serviceAccountName: operate-workflow-sa dependencies: - name: test-dep eventSourceName: webhook eventName: example triggers: - template: name: webhook-workflow-trigger k8s: operation: create source: resource: apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: webhook- spec: entrypoint: whalesay arguments: parameters: - name: message value: hello world templates: - name: whalesay inputs: parameters: - name: message container: image: docker/whalesay:latest command: [cowsay] args: [\u0026#34;{{inputs.parameters.message}}\u0026#34;] parameters: - src: dependencyName: test-dep dataKey: body dest: spec.arguments.parameters.0.value calendar Timer, CronJob을 모두 구동시킬수 있는 형태의 EventSource입니다.\n1 2 3 4 5 6 7 8 9 apiVersion: argoproj.io/v1alpha1 kind: EventSource metadata: name: calendar spec: calendar: example-with-interval: interval: 10s # schedule: \u0026#34;30 * * * *\u0026#34; 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 apiVersion: argoproj.io/v1alpha1 kind: Sensor metadata: name: calendar spec: template: serviceAccountName: operate-workflow-sa dependencies: - name: test-dep eventSourceName: calendar eventName: example-with-interval triggers: - template: name: calendar-workflow-trigger k8s: operation: create source: resource: apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: generateName: calendar-workflow- spec: entrypoint: whalesay arguments: parameters: - name: message value: hello world templates: - name: whalesay inputs: parameters: - name: message container: image: docker/whalesay:latest command: [cowsay] args: [\u0026#34;{{inputs.parameters.message}}\u0026#34;] parameters: - src: dependencyName: test-dep dataKey: eventTime dest: spec.arguments.parameters.0.value retryStrategy: steps: 3 webhook-k8s-object Webhook-Workflow와 유사합니다. 다른점은 Sensor에서 Trigger하는 대상이 k8s object 라는 점입니다.\nPod, Deployment, Job, CronJob와 같은 Custom Resources를 실행할 수 있습니다.\n가능한 동작은 Create, Update, Patch, Delete입니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 apiVersion: argoproj.io/v1alpha1 kind: EventSource metadata: name: k8s-webhook spec: service: ports: - port: 12000 targetPort: 12000 webhook: example: port: \u0026#34;12000\u0026#34; endpoint: /example method: POST 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion: argoproj.io/v1alpha1 kind: Sensor metadata: name: k8s-webhook spec: template: serviceAccountName: operate-k8s-sa dependencies: - name: test-dep eventSourceName: k8s-webhook eventName: example triggers: - template: name: webhook-pod-trigger k8s: operation: create source: resource: apiVersion: v1 kind: Pod metadata: generateName: hello-world- spec: containers: - name: hello-container args: - \u0026#34;hello-world\u0026#34; command: - cowsay image: \u0026#34;docker/whalesay:latest\u0026#34; parameters: - src: dependencyName: test-dep dataKey: body dest: spec.containers.0.args.0 Kubernetes Object를 배포하기 위해서는 추가적인 RBAC가 필요합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion: v1 kind: ServiceAccount metadata: name: operate-k8s-sa namespace: argo-events --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: operate-k8s-role namespace: argo-events rules: - apiGroups: - \u0026#34;\u0026#34; verbs: - \u0026#34;*\u0026#34; resources: - pods --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: operate-k8s-role-binding roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: operate-k8s-role subjects: - kind: ServiceAccount name: operate-k8s-sa cross-namespace 지금까지 사용한 RBAC는 namespace 한정해서 사용가능합니다. 하지만 이는 사용자들이 모두 사용법을 알아야 하는 단점이 있어, 사용자로서는 불편할 수 밖에 없습니다.\nCross-Namespace로 사용하기 위해서는 기존에 Role로 정의되어있던 내용들을 모두 ClusteRole으로 변경해야 합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 apiVersion: v1 kind: ServiceAccount metadata: name: operate-k8s-cluster-sa namespace: argo-events --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata: name: operate-k8s-clusterrole namespace: argo-events rules: - apiGroups: - \u0026#34;\u0026#34; verbs: - \u0026#34;*\u0026#34; resources: - pods --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: operate-k8s-clusterrole-binding namespace: argo-events roleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: operate-k8s-clusterrole subjects: - kind: ServiceAccount name: operate-k8s-cluster-sa namespace: argo-events 예제로 첨부한 ClusterRole은 pod만 생성가능한데, 필요할 경우 다른 Resource를 추가한뒤 사용하면 됩니다.\nwebhook-auth 보안상의 문제로 Webhook을 호출하기 위해서 인증 토큰을 추가해야할 수 있습니다.\n이 방법은 인증용 토큰을 생성한뒤, k8s object인 Secret으로 등록 후 사용하는 것입니다.\nSecret 객체 이름을 지정해주면 Webhook에서 인증용도로 사용할 수 있습니다.\n1 2 3 echo -n \u0026#39;af3qqs321f2ddwf1e2e67dfda3fs\u0026#39; \u0026gt; ./token.txt kubectl create secret generic my-webhook-token --from-file=my-token=./token.txt 1 2 3 4 5 6 7 8 9 10 11 12 13 apiVersion: argoproj.io/v1alpha1 kind: EventSource metadata: name: secret-webhook spec: webhook: example: port: \u0026#34;12000\u0026#34; endpoint: /example method: POST authSecret: name: my-webhook-token key: my-token 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 apiVersion: argoproj.io/v1alpha1 kind: Sensor metadata: name: k8s-webhook spec: template: serviceAccountName: operate-k8s-sa dependencies: - name: test-dep eventSourceName: secret-webhook eventName: example triggers: - template: name: webhook-pod-trigger k8s: operation: create source: resource: apiVersion: v1 kind: Pod metadata: generateName: hello-world- spec: containers: - name: hello-container args: - \u0026#34;hello-world\u0026#34; command: - cowsay image: \u0026#34;docker/whalesay:latest\u0026#34; parameters: - src: dependencyName: test-dep dataKey: body dest: spec.containers.0.args.0 Summary 테스트 한 내용들을 보시면, 운영적인 요소보다는 기능적인 요소로서 지원여부를 위주로 확인하였습니다.\nBatch를 사용하기위해 필요한 Resource들이 모두 yaml로 정의가 가능하기 때문에, GitOps로 운영이 가능했습니다.\n그리고 Argo Event에서 다양한 Event Source를 지원하기 때문에, Webhook과 Cron을 필요한 만큼 충분히 사용가능 한 것을 확인하였습니다.\n다음 글에서는 운영적인 요소에서 필요한 Needs와 그것들을 확인해 나간 과정에 대해 정리해보겠습니다.\n","date":"2023-09-10T16:46:32+09:00","permalink":"https://korcasus.github.io/p/argo-events-getting-start/","title":"Argo Events Getting Start"},{"content":"Introduction Synology에 Docker Registry를 Container로 사용하고 있었습니다.\n개인 용도로 만든 이미지를 자유롭게 저장할 수 있게 되어 편하게 사용하고있었습니다.\n하지만 어떤 이미지를 저장하고있는지, tag가 어떤게 있는지를 보기가 어렵더라구요.\n물론 API를 사용해서 조회하면 볼수야 있겠지만, 저에게는 불편하게 다가왔습니다.\n그래서 WebUI를 설치하는 방안을 찾아보았습니다.\nCandidate 검색해보기도 하고, 참고했을때, 아래의 2개가 적합해보였습니다.\nhttps://github.com/Joxit/docker-registry-ui https://github.com/goharbor/harbor harbor는 기능이 풍부하고 CNCF로 관리되다보니, 더 좋긴합니다.\n하지만 Registry를 이미 설치해두었고, web-ui만 사용하면 되는거라 Joxit docker regisry ui가 낫겠다 생각했습니다.\nInstall 내부망에서만 사용가능하게끔 설정하였습니다.\n외부망에서는 다른 설정이 추가될 수 있으며, 보안이 취약할 수 있습니다. (이때는 harbor를 사용하는게 더 나을겁니다.)\n공식문서에서는 Recommended Docker Registry Configuration을 안내해주고 있습니다.\n이대로 하면 좋겠지만, 제가 사용하는 환경에 맞게 추가 설정을 해주어야했기 때문에 참고만 하였습니다. 아래의 목록은 고려해야했던 사항들 입니다.\n설치해둔 Registry는 분산설치 된게 아니며, NAS 하나에만 설치되어있다. Registry는 보안설정을 해두었다. Web-ui에서 이미지 삭제가 가능해야한다. Image의 상세정보를 볼 수 있어야한다. 내부망에서 사용하는 host명이 별도로 있다.(CORS설정) 위 사항들을 고려했을때, 아래의 설정을 추가해주어야했습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 web-ui: SINGLE_REGISTRY: true REGISTRY_SECURED: true DELETE_IMAGES: true SHOW_CONTENT_DIGEST: true REGISTRY_URL: \u0026#34;http://xxxxx:1234\u0026#34; registry: REGISTRY_HTTP_HEADERS_Access-Control-Allow-Origin: \u0026#39;[http://xxxxx:1234]\u0026#39; REGISTRY_HTTP_HEADERS_Access-Control-Allow-Methods: \u0026#39;[HEAD,GET,OPTIONS,DELETE]\u0026#39; REGISTRY_HTTP_HEADERS_Access-Control-Allow-Credentials: \u0026#39;[true]\u0026#39; REGISTRY_HTTP_HEADERS_Access-Control-Allow-Headers: \u0026#39;[Authorization,Accept,Cache-Control]\u0026#39; REGISTRY_HTTP_HEADERS_Access-Control-Expose-Headers: \u0026#39;[Docker-Content-Digest]\u0026#39; 여기까지 설정하니, 특별히 문제없이 web-ui가 문제없기 실행되었습니다.\n정상 동작하는지 들어가보니, 메인 페이지는 Image 리스트를 잘 보여주었습니다.\n하지만 이미지 상세정보 페이지에 들어가니, CORS 문제가 발생하고 있었습니다.\nCORS TroubleShooting Console에 출력되는 에러메시지를 보니, Preflight가 웹 브라우저에 의해 거절되어 실패하던 것이었습니다.\n생각해볼 수 있는 원인은 CORS였는데, 설정이 잘못되었다면 메인페이지에서도 에러가 발생해야 했었습니다.\n다른 원인이 있을 수 있겠다 싶어, 공식 문서를 확인해보니 FAQ에 등록되어있었습니다.\n1 2 3 Why OPTIONS (aka preflight requests) and DELETE fails with 401 status code (using Basic Auth) ? This is caused by a bug in docker registry, it returns 401 status requests on preflight requests, this breaks W3C preflight-request specification. I suggest to have your UI on the same domain than your registry e.g. registry.example.com/ui/ or use NGINX_PROXY_PASS_URL or configure a nginx/apache/haproxy in front of your registry that returns 200 on each OPTIONS requests. (see #104, #204, #207, #214, #266). 3가지 방법중 하나를 선택하라고 알려주고 있습니다.\nweb-ui의 domain을 registry와 동일하게 한다. NGINX_PROXY_PASS_URL을 설정한다. nginx에 OPTIONS method에 200응답을 보낸다. 2번이 가장 간단하기도 하고, 다른 방법에 비해 가장 나을 선택지로 보여 진행했습니다.\n설정은 아래처럼 변경하였습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 web-ui: SINGLE_REGISTRY: true REGISTRY_SECURED: true DELETE_IMAGES: true SHOW_CONTENT_DIGEST: true NGINX_PROXY_PASS_URL: \u0026#34;http://xxxxx:1234\u0026#34; registry: REGISTRY_HTTP_HEADERS_Access-Control-Allow-Origin: \u0026#39;[http://xxxxx:1234]\u0026#39; REGISTRY_HTTP_HEADERS_Access-Control-Allow-Methods: \u0026#39;[HEAD,GET,OPTIONS,DELETE]\u0026#39; REGISTRY_HTTP_HEADERS_Access-Control-Allow-Credentials: \u0026#39;[true]\u0026#39; REGISTRY_HTTP_HEADERS_Access-Control-Allow-Headers: \u0026#39;[Authorization,Accept,Cache-Control]\u0026#39; REGISTRY_HTTP_HEADERS_Access-Control-Expose-Headers: \u0026#39;[Docker-Content-Digest]\u0026#39; 이렇게하니 더 이상의 CORS 문제가 발생하지않고, 정상동작하였습니다.\nConclusion NAS에 개인용도로 사용할 registry web-ui를 설치하였습니다.\n간단하게 Registry에 저장된 도커이미지를 관리하기에는 충분해 보입니다.\n저처럼 사적으로 Registry에 저장해둔 도커 이미지를 관리할 니즈가 있으신 분들에게는 추천 드립니다.\n","date":"2023-09-03T21:42:24+09:00","permalink":"https://korcasus.github.io/p/install-docker-registry-ui/","title":"Install Docker Registry Ui for Synology"},{"content":"Introduction Synology에서 여러 응용 프로그램을 설치해서 사용하기 위해서 Docker를 애용합니다.\nDocker로 Jenkins 를 사용중인데, Jenkins에서도 Docker를 사용해야하는 상황이 생겨 진행하게 되었습니다.\nSynology가 아닌 다른 Machine에서도 비슷하게 진행되지만, 이번 설명은 Synology를 기준으로 설명하겠습니다.\n설명한 방법은 Host의 docker sock을 공유하는 방법입니다.\nPrerequisite Jenkins Container Using Official Image Progress 생각보다 어렵지 않습니다. 차근차근히 진행하면 쉽습니다.\nDSM 7.2로 업데이트한 후에 진행한 작업이라, 캡처한 이미지가 차이가 있을 수 있습니다.\n1. Docker sock 공유 Synology에 설치해준 Docker sock과 container에서 사용할 Docker sock을 binding 하는 작업을 해야합니다. 아래의 명령어대로 설정해 주면 됩니다.\n1 2 3 4 docker run -p ${webPort}:8080 -p ${agentPort}:50000 -d -v /var/run/docker.sock:/var/run/docker.sock -v ${hostJenkinsDir}:/var/jenkins_home jenkins/jenkins:lts # 예시로 들면 아래와 같습니다. docker run -p 8080:8080 -p 50000:50000 -d -v /var/run/docker.sock:/var/run/docker.sock -v /docker/jenkins/home:/var/jenkins_home jenkins/jenkins:lts 하지만 Synology에서는 / directory 설정할 수 없기떄문에, SSH로 접속해서 설정해주어야합니다.\nSSH로 접속해서 Docker command를 사용할 준비를 합니다.\n저의 경우에는 위의 이미지처럼 container의 volume을 update할 수 있는 설정이 있습니다.\n그래서 아래의 명령어를 사용해서 volume binding 하였습니다.\n1 docker container update -v /var/run/docker.sock:/var/run/docker.sock -v /docker/jenkins/home:/var/jenkins_home ${containerId} 이렇게 변경한 volume binding은 synology web ui상에서는 보이지 않기 때문에, 명령어를 통해 잘 변경되었는지 확인해야합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 docker container inspect ${containerId} { ..., \u0026#34;Mounts\u0026#34;: [ { \u0026#34;Type\u0026#34;: \u0026#34;bind\u0026#34;, \u0026#34;Source\u0026#34;: \u0026#34;/volume1/docker/jenkins/home\u0026#34;, \u0026#34;Destination\u0026#34;: \u0026#34;/var/jenkins_home\u0026#34;, \u0026#34;Mode\u0026#34;: \u0026#34;rw\u0026#34;, \u0026#34;RW\u0026#34;: true, \u0026#34;Propagation\u0026#34;: \u0026#34;rprivate\u0026#34; }, { \u0026#34;Type\u0026#34;: \u0026#34;bind\u0026#34;, \u0026#34;Source\u0026#34;: \u0026#34;/var/run/docker.sock\u0026#34;, \u0026#34;Destination\u0026#34;: \u0026#34;/var/run/docker.sock\u0026#34;, \u0026#34;Mode\u0026#34;: \u0026#34;rw\u0026#34;, \u0026#34;RW\u0026#34;: true, \u0026#34;Propagation\u0026#34;: \u0026#34;rprivate\u0026#34; } ], ... } 위처럼 docker.sock이 추가되었다면, 정상적으로 된 것입니다.\n혹시 docker container update 명령어를 통해 volume binding을 할 수 없다면, 새로 container를 만드는것이 편하겠습니다.\n직접 하진않았지만 아래의 명령어처럼 입력하면 되지않을까 싶습니다.\n1 2 3 4 docker run --restart ${policy} --name ${jenkins} -p ${webPort}:8080 -p ${agentPort}:50000 -d -v /var/run/docker.sock:/var/run/docker.sock -v ${hostJenkinsDir}:/var/jenkins_home jenkins/jenkins:lts # 예시로 들면 아래와 같습니다. docker run --restart always --name jenkins -p 8080:8080 -p 50000:50000 -d -v /var/run/docker.sock:/var/run/docker.sock -v /docker/jenkins/home:/var/jenkins_home jenkins/jenkins:lts 항상 재시작 하는 옵션을 공식문서를 참고하였습니다.\n본인의 필요에 따라 설정해주세요.\n2. Jenkins에 Docker 설치 Dockerfile을 새로 만들어, Docker를 설치하는 스크립트를 입력해주는 방법도 있습니다. 하지만 새로 이미지를 만들고 관리하는건 번거롭기도하고, volume binding을 해서 데이터가 유지되는 상황이니 설치 스크립트를 저장해두는게 좋다고 생각이 들었습니다.\n(Jenkins를 업그레이드하거나 Container가 삭제된다면, 설치스크립트를 새로 실행해주긴 해야하지만요.)\n아래의 내용의 스크립트를 host의 /docker/jenkins/home/install_jenkins.sh로 만들어 주었습니다.\n1 2 3 #!/bin/bash curl https://get.docker.com/ \u0026gt; dockerinstall \u0026amp;\u0026amp; chmod 777 dockerinstall \u0026amp;\u0026amp; ./dockerinstall chmod 666 /var/run/docker.sock 설치하는데 조금 시간이 걸리긴합니다. 기다리면, 설치가 완료된것을 보실 수 있습니다.\n3. 동작 확인 Jenkins Container와 Web UI에서 Docker command가 잘 되는지 확인해보겠습니다. 우선은 Container에서 확인해보겠습니다.\n1 2 3 4 5 6 7 docker exec -it ${containerId} /bin/bash # 예시 docker exec -it f0f5398c9a62 /bin/bash --- docker container ls --all 위의 명령어들을 실행하면 Container에 접속하였고, 시놀로지에서 구동중인 Container 리스트를 확인할 수 있게됩니다. 구동중인 Container가 없을 수는 없지만, 에러는 발생해서는 안됩니다.\n위에까지만 확인해도 문제는 없을텐데, 저는 혹시나 싶어 더 확인해보았습니다.\n아래처럼 Jenkins pipeline으로 실행해보았습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 pipeline { agent any stages { stage(\u0026#39;Run Container\u0026#39;) { steps { script { // Docker 컨테이너 실행 docker.image(\u0026#34;hello-world:latest\u0026#34;).run() } } } } } 위의 결과까지 나오면, jenkins에서도 docker를 마음껏 사용하실 수 있습니다.\nConclusion Docker로 구동한 Jenkins에서 Docker를 실행할 수 있도록 해보았습니다. Container로 만들어진 Jenkins에서 Host의 Docker에 접근할 수 있게 만든다는게, 편리하긴 하지만 보안상으로는 좋지않아 보입니다. 사용하신다면 Container와 Host의 보안을 좀 더 신경 써서 사용해주시면, 안전할 것 같습니다.\n다른 분들에게 도움이 되면 좋겠네요. 감사합니다.\nReference https://faun.pub/how-to-install-docker-in-jenkins-container-4c49ba40b373 https://github.com/jenkinsci/docker/blob/master/README.md ","date":"2023-08-26T22:30:49+09:00","permalink":"https://korcasus.github.io/p/use-docker-in-jenkins-docker/","title":"Use Docker in Jenkins Docker"},{"content":"Introduction Hugo로 블로그를 만들어서 운영중인데요. 구글 서치콘솔에 sitemap.xml을 제출수 검색에 노출되게끔 설정해두었습니다.\n그런데 몇달이 지난 지금도 색인된 페이지가 몇 개가 되지않습니다.\n글이 적은걸 고려하더라도, 이해하기 어려운 수치여서 살펴보았습니다.\n색인되지 않은 페이지들은 대다수가 categories와 tags로 생성된 페이지 였습니다.\n몇달째 이 상태가 유지된다는건, 색인 대상에서 제외(?) 되는거 아닐까 라는 생각이 들었습니다.\n이러면 정상적인 글까지도 색인 되지않게 되는거라 추가 조치를 취하기로 마음 먹었습니다.\nObjective 구글에서 수집할 페이지들을 보는 페이지(sitemap.xml)에서 불필요한 글들을 대상에서 제외한다.\nHow Hugo는 사용중인 설정파일에만 추가해주면, sitemap을 자동으로 생성해 줍니다.\n(설정에 대한 가이드는 링크에서 확인하실수 있습니다.)\n이 설정대로 사용할 경우, categories와 tags가 sitemap.xml에 포함되어 출력되기 떄문에 custom해 주어야 합니다.\noverride built-in template을 읽어보면, custom 하는 방법을 상세히 안내하고 있습니다. 정리하면 아래와 같이 2개 중 하나의 파일만 만들어주면 됩니다.\n1 2 - layouts/sitemap.xml - layouts/_default/sitemap.xml 저의 경우에는 default로 만드는 것에 왠지 모를 거부감(?)이 느껴져서 layouts/sitemap.xml로 만들었습니다.\n만든 sitemap.xml에서 어떻게 내용을 구성하면 되는지 작성해주면 되는데요. Hugo가 Golang으로 만들어졌기 때문에, HTML을 만들기 위한 golang template language 문법에 맞게 sitemap.xml을 작성하면 됩니다.\n번거로우니까 우선 결론부터 이야기하면 sitemap.xml은 아래의 내용처럼 추가해주시면 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 {{ printf \u0026#34;\u0026lt;?xml version=\\\u0026#34;1.0\\\u0026#34; encoding=\\\u0026#34;utf-8\\\u0026#34; standalone=\\\u0026#34;yes\\\u0026#34;?\u0026gt;\u0026#34; | safeHTML }} \u0026lt;urlset xmlns=\u0026#34;http://www.sitemaps.org/schemas/sitemap/0.9\u0026#34; xmlns:xhtml=\u0026#34;http://www.w3.org/1999/xhtml\u0026#34;\u0026gt; {{ range .Pages }} {{ if not (in .Site.Params.taxonomiesExcludedFromSitemap .Data.Plural) }} \u0026lt;url\u0026gt; \u0026lt;loc\u0026gt;{{ .Permalink }}\u0026lt;/loc\u0026gt;{{ if not .Lastmod.IsZero }} \u0026lt;lastmod\u0026gt;{{ safeHTML ( .Lastmod.Format \u0026#34;2006-01-02T15:04:05-07:00\u0026#34; ) }}\u0026lt;/lastmod\u0026gt;{{ end }}{{ with .Sitemap.ChangeFreq }} \u0026lt;changefreq\u0026gt;{{ . }}\u0026lt;/changefreq\u0026gt;{{ end }}{{ if ge .Sitemap.Priority 0.0 }} \u0026lt;priority\u0026gt;{{ .Sitemap.Priority }}\u0026lt;/priority\u0026gt;{{ end }}{{ if .IsTranslated }}{{ range .Translations }} \u0026lt;xhtml:link rel=\u0026#34;alternate\u0026#34; hreflang=\u0026#34;{{ .Language.Lang }}\u0026#34; href=\u0026#34;{{ .Permalink }}\u0026#34; /\u0026gt;{{ end }} \u0026lt;xhtml:link rel=\u0026#34;alternate\u0026#34; hreflang=\u0026#34;{{ .Language.Lang }}\u0026#34; href=\u0026#34;{{ .Permalink }}\u0026#34; /\u0026gt;{{ end }} \u0026lt;/url\u0026gt; {{ end }} {{ end }} \u0026lt;/urlset\u0026gt; {{ if not (in .Site.Params.taxonomiesExcludedFromSitemap .Data.Plural) }} 라는 부분이 노출제어하는 역할을 수행합니다.\n첫번째 변수(.Site.Params.taxonomiesExcludedFromSitemap)는 Site Variable을 사용합니다. Site Variable은 쉽게 생각하면 Global Variable 역할을 맡는다고 합니다.\n여기서 Params라는 변수게 접근이 가능한데 관련된 내용은 링크로 확인할 수 있습니다.\n.Site.Params 변수를 사용하면 config 파일에 params 에 접근할 수 있습니다.\n이 말은 params에 있는 taxonomiesExcludedFromSitemap 변수에 접근하겠다는 뜻이죠.\nconfig.yaml에 추가한 적이 없기 때문에, 노출제어를 하기 위해서는 추가해주어야 합니다. (config.yaml또는 config.toml에 아래 내용을 추가해주세요)\n1 2 3 4 params: taxonomiesExcludedFromSitemap: - \u0026#34;tags\u0026#34; - \u0026#34;categories\u0026#34; 두번째 변수는 Taxonomy Variable을 사용합니다.\n이걸 찾아볼때 까지 Taxonomy가 무엇인지 몰랐습니다\u0026hellip; 간단히 정리하면 User가 정의하는 Grouping 방법입니다.\n(자세히 알고싶은 분은 링크를 한번 읽어보면 좋을것 같습니다.)\n변수 설명페이지를 보면 .Data.Plural은 분류법의 복수 이름이라고 정의하고 있습니다.\n노출제어하고 싶은 페이지들에 해당되는 변수임을 알 수 있습니다.\nyaml파일 수정과 sitemap.xml추가까지 했다면, 필요한 조치들은 모두 수행하였습니다. local에서 sitemap이 의도한 대로 변경되었는지 확인하고 배포하시면 되겠습니다.\nConclusion 생각보다 어려운 작업은 아니었지만, Hugo의 설정 관련 문서들을 찾기가 조금 어려웠습니다.\n아직은 Document 구성이 익숙하지 않아 그런것 같아, 앞으로 블로그를 운영해나가면서 익숙해져 나가야겠습니다.\n감사합니다.\nReference Hugo Community article ","date":"2023-08-15T15:03:02+09:00","permalink":"https://korcasus.github.io/p/hugo-exclude-pages/","title":"Hugo Exclude Pages"},{"content":"Before to start 맥을 사용하시는분이라면 Keychain에 익숙하실겁니다. 비밀번호를 저장하고 관리해주는 어플리케이션이죠.\n요즘에는 크롬이나 엣지 브라우저에서도 제공하고있습니다.\n저는 웹 뿐만이 아니라, 맥에서도 편하게 사용할수 있는것을 원했기 때문에 다른 툴을 사용하고있습니다.\n더 자세히 이야기하면 길어지기 때문에, 짧게 이야기하면 Valutwarden이라는 Bitwarden의 Compact버전을 synology에 설치해서 사용하고있습니다.\n이번에 Valutwarden이 1.29.0 버전으로 업그레이드 되면서 실시간 동기화가 가능해졌다는 소식에 업그레이드를 결심하였습니다.\nObjective Valutwarden을 1.29.1로 업그레이드하면서, 필요한 설정(ex. 실시간 동기화)들을 추가한다.\nUpgrade Upgrade Guide 찾기 업그레이드 하기전까지 사용하던 docker image를 latest를 사용했기 때문에, 몇버전을 사용하고 있었는지는 모르겠습니다. 하지만 최신버전이 1.29.x버전이기 때문에 minor 버전만 신경써주면 될것으로 생각했습니다.\n공식 Github에 들어가보면 Updating valutwarden image Wiki가 존재합니다. 살펴보니 컨테이너 환경에서 업그레이드를 하는 방법만 설명하였고, 버전이 바뀌면서 변경해야하는 설정에 대해서는 언급이 없습니다.\n저의 경우에는 이럴때 Release Note를 살펴봅니다. 정석은 모든 변경사항을 읽어야하지만, 선 업그레이드 후 변경하기로 마음먹었습니다 😇\n그래도 업그레이드 하는게 1.29.x이니 1.29.0으로 올라오면서 큰 변경이 어떤건지 보았습니다.\n1 2 3 4 5 6 7 - WebSocket notifications now work via the default HTTP port. No need for `WEBSOCKET_ENABLED` and a separate port anymore. - The proxy examples still need to be updated for this. Support for the old websockets port 3012 will remain for the time being. - Mobile Client push notification support, see #3304 thanks @GeekCornerGH! - Web-Vault updated to v2023.5.0 (v2023.5.1 does not add any improvements for us) - The latest Bitwarden Directory Connector can be used now (v2022.11.0) - Storing passkeys is supported, though the clients are not yet released. So, it might be we need to make some changes once they are released. - See: #3593, thanks @GeekCornerGH! 저한테는 3가지가 눈에 들어왔습니다.\nWebSocket notifications now work via the default HTTP port. No need for WEBSOCKET_ENABLED and a separate port anymore. Mobile Client push notification support, see #3304 thanks @GeekCornerGH! Storing passkeys is supported, though the clients are not yet released. So, it might be we need to make some changes once they are released. 1번의 경우에는 폰에서 사용중인 앱에서 변경사항이 발생시, valutwarden 서버에 반영하기 위해서 필수로 설정해야하는 기능입니다. (아니면 서버와 휴대폰앱이 별도의 데이터를 가지게 됩니다.) 이것 때문에 2개의 포트를 열어두고 사용하는게 필요했었는데, 이번 업데이트로 1개로 통합되었다고 하네요.\n공유기에 개방해둔 포트를 줄일수 있을뿐더러, 신경써야되는 환경변수가 하나 줄었습니다.\n2번의 경우에는 실시간 동기화입니다. 이거는 나중에 자세히 보겠습니다.\n3번은 Passkey에 관한 내용입니다. 애플과 구글에서 지원하기 시작한 보안 방법중 하나인데, 고도화된 보안환경을 제공한다고 합니다. 저도 자세히 살펴보진 않아서,\u0026hellip; 설명은 넘어가겠습니다.\n보안에 신경은 항상 써야하기때문에, 지원가능해지면 사용해보고싶네요.\n추가할 Configuration 보기 비밀번호 관리 프로그램을 개인 서버에 올려서 사용한다는건, 책임도 본인이 진다는 뜻입니다.\n따라서 해킹당하지않도록 문제의 소지가 될수 있는 기능은 닫아두는게 좋습니다. (기본은 닫혀있을겁니다.) 그래서 사용하지 않을 기능과 사용할 기능들을 살펴보기 위해서 문서를 읽어보기 시작했습니다.\nGithub wiki를 읽어보면 자세하게 나와있습니다. config.json보다는 환경변수를 사용하는 방법을 권고하네요.\n리스트는 아래와 같습니다.\n여러 관심가는 설정들이 많지만, 저는 2,3,4번 항목이 눈에 들어왔고 반영하기로 하였습니다.\n2번항목은 추가 사용자 가입금지 3번항목은 초대를 통한 사용자 가입금지 4번항목은 admin계정 운영 admin을 별도로 관리하고싶지않았기에, 기본값이 admin계정 비활성화인지 살펴봤습니다. 위에 나열한 내용들을 반영하기 위해서 아래와 같이 설정 해주었습니다.\n1 2 SIGNUPS_ALLOWED : false INVITATIONS_ALLOWED : false 그러고보니 실시간 동기화도 설정이 필요하네요.\n7번 항목에 등록되어있습니다.\n문서를 살펴보면 아시겠지만, 3개 환경변수 설정을 요구합니다.\n1 2 3 - PUSH_ENABLED=true - PUSH_INSTALLATION_ID= - PUSH_INSTALLATION_KEY= ID와 KEY는 Bitwarden 홈페이지에서 발급받으라고 합니다.\nSelf Hosting은 지원하지 않는건가? 싶었는데, 다행히 지원하네요.\nSelf Hosting에서 이용중인 이메일과 Data Region을 설정해줍니다.\nData Region은 뭔가싶겠지만, Wiki에 US로 두고 쓰라고 적혀있네요. Europe은 안될수도있다고\u0026hellip; 그리고 업그레이드 직후에 App에서 실시간 동기화가 안될경우 재설치하라고 가이드하고있습니다.\n업그레이드 완료한 후에, 한번 실시간 동기화 해보시는걸 추천합니다. (저는 직후에 바로해봤는데 잘되더라구요)\nUpgrade 하기 본인 Synology에 Valutwarden을 설치하신 분이라면, 업그레이드 하는 과정은 다 아실거라 판단해서 과정은 생략했습니다.\n업그레이드 완료한 컨테이너 설정값인데, 이전과 큰 차이는 없습니다.\n완료후, docker container log확인과 valutwarden web에 접속해보았습니다.\nlog에는 서버를 실행하는데 문제없었고, web에 접속해보니 에러페이지 뜨지않고 기존 데이터도 잘 있습니다.\n실시간 동기화도 테스트해보았는데, 조금 이상하더라구요. Web에서 변경한 내용은 App에서 별도의 액션 없이 잘 반영되었는데, App에서 변경한 내용은 Web에서 새롭게 로딩해주어야 했습니다. (새로고침했다가 다시 비밀번호 입력했습니다 -ㅅ-)\n큰 이슈까진 아니기 때문에, 업그레이드 잘 되었다고 판단하였습니다.\n후기 Valutwarden은 Bitwarden의 Compact를 추구하는 만큼 꼭 필요한 기능들만 가지고 있습니다. 이런 이유때문에 업그레이드 하더라도 많이 변경하지않아도 되는것이 좋게 느껴졌습니다.\n별도로 운영되기 때문에 업그레이드를 하는데 개인의 노력이 필요할겁니다. 후원을 받긴하지만 오픈소스 환경에서 자발적으로 해주시는 분들에게 감사할 따름이네요.\n저도 언젠가 필요한 기능을 추가하기 위해 Contribute하는 순간이 오겠죠?? 곧 올거라 믿습니다.\n감사합니다.\n","date":"2023-08-14T23:09:01+09:00","permalink":"https://korcasus.github.io/p/upgrade-vaultwarden-1.29.1/","title":"Upgrade Vaultwarden 1.29.1"},{"content":"Introduction ES가 검색엔진이다보니, 설정한 조건에 맞는 데이터를 빠르게 찾아낼 수 있습니다.\n하지만 조건으로 설정하기 위해서는 색인하는 과정이 필요하고, 색인을 하기위해서는 많은 리소스가 필요로 합니다.\n이러한 제약조건으로 인해, 많은 양의 데이터 속에서 조건에 맞는 데이터를 찾아 효율적으로 추출하기는 어렵습니다.\n따라서 다른 방법으로 데이터를 조회해야할텐데, 이러한 상황이라면 대부분의 경우 Hadoop을 사용을 검토합니다.\n만약 Elasticsearch에 저장할 데이터를 전송하는 역할을 팀원 또는 본인이라면, 적절한 방법으로 HDFS에 기록할 수 있습니다.\n하지만 데이터 전송에 대한 제어권이 없고, Elasticsearch만 사용이 가능하다면 어떻게 해야할까요?\n당연히 Elasticsearch에서 데이터 추출을 검토해야할 것입니다.\n이러한 상황에 사용하기 적합한 방법을 설명하고자 합니다.\nSpark에서 library를 사용해, Elasticsearch 데이터를 ETL하는 과정을 Getting Started 정도의 내용으로 설명합니다.\nObjective ES에 색인되어있는 특정 index를 Hive로 ETL한다.\nRequired Spark에 조금의 지식이 필요합니다. Elasticsearch에 조금의 지식이 필요합니다. Environment Scala : 2.12 Spark : 3.x Java : 8 ElasticSearch : 7.x Dependency elasticsearch-hadoop elasticsearch-spark-30_2.12 calcite-core ElasticSearch 8버전으로 사용한다면, Release Note를 보고 적절한 버전을 찾는것을 권장합니다.\nelastic-* 라이브러리는 7.17.0 버전 이상을 사용하는게 좋습니다.\n7.17.0 버전에서 중요한 패치가 이루어졌기 때문입니다.\ncalcite-core는 deserialize할때 사용했던것으로 기억합니다.\n추가하지않으면, Dataframe을 생성하는 과정에 문제가 발생합니다.\nGetting Started Connect 가이드대로 하면 Spark SQL을 사용할 수있습니다. 하지만 Spark SQL은 구조화된 데이터이므로, 동일한 데이터구조를 가지지않을경우 문제가 생길수있습니다.\n구버전은(Spark version \u0026lt; 3.x) 설명하진 않겠습니다.\nSpark3버전부터는 SparkSession을 생성해 사용하게 됩니다.\nSession을 생성하면서, 연결하고자 하는 es의 정보들을 입력해주어야합니다.\nConfiguration을 읽으면 상세한 내용을 확인할 수 있습니다.\nES에 연결하기위해서는 아래의 2개는 필수적으로 작성해주어야 합니다.\nes.nodes es.port 인증이 필요하거나, https연결이 필요하지않을 경우 아래와 같은 설정을 입력해줍니다.\nes.net.http.auth.user es.net.http.auth.pass es.net.ssl 인증과 관련된 내용은 링크를 확인해주세요. 제한된 환경인 WAN환경에 구축된 ES를 사용해야하는 경우 추가해주어야하는 파라미터가 있습니다. 이 파라미터를 추가할 경우, es.nodes 에 명시된 nodes만 사용합니다. 다른 node는 탐색하지 않습니다.\nes.nodes.wan.only 상세 내용은 링크에서 보실수 있습니다. 위와같은 설정 정보들을 모두 사용하면 아래와 같습니다.\n1 2 3 4 5 6 7 8 9 sparkSession = SparkSession.builder() .master(sparkConfiguration.getMaster()) .appName(sparkConfiguration.getAppName()) .config(\u0026#34;spark.es.nodes\u0026#34;, \u0026#34;es-nodes\u0026#34;) .config(\u0026#34;spark.es.port\u0026#34;, \u0026#34;es-port\u0026#34;) .config(\u0026#34;spark.es.net.http.auth.user\u0026#34;, \u0026#34;user\u0026#34;) .config(\u0026#34;spark.es.net.http.auth.pass\u0026#34;, \u0026#34;password\u0026#34;) .config(\u0026#34;spark.es.nodes.wan.only\u0026#34;, true) .config(\u0026#34;spark.es.net.ssl\u0026#34;, true) ElasticSearch hadoop은 Spark가 접근해올 수 있는 방법을 4가지 제공하고있습니다.\nNative RDD Spark Streaming Spark SQL Spark Structured Streaming 위 4가지를 성격에 따라 나누어 보았습니다.\nStructured vs Unstructured Streaming vs Non-Streaming(Batch) Streaming을 고려해야한다면, Spark Streaming이나 Spark Structured Streaming을 검토해볼수있습니다.\n그게아니라면 Native RDD나 Spark SQL을 고려해볼 수 있습니다. ES데이터를 Hive에서 사용할 목적으로 진행하는 프로젝트이므로, Structure 형태를 가질수 있는 Spark SQL로 진행하였습니다. 1.5 이상부터는 아래처럼 Dataframe을 생성할 수 있습니다. (1.5 이전 버전은 가이드 문서를 통해 확인해 주세요.)\n1 val df = sql.read.format(\u0026#34;es\u0026#34;).load(\u0026#34;spark/index\u0026#34;) 예시는 Java도 있긴하지만, Scala로 위주로 설명하는게 대다수입니다.\n유지 보수를 위해 Java Project로 진행해야했기때문에, 사용 예시는 앞으로 Java로 보여드리겠습니다.\n1 2 3 Dataset\u0026lt;Row\u0026gt; df = sparkSession.read() .format(\u0026#34;es\u0026#34;) .load(\u0026#34;template_data\u0026#34;); 위와같은 방법으로 DataFrame 객체를 생성할 수 있습니다.\n만약에 Java DataFrame 객체를 생성해야 한다면, 아래와 같이 할 수 있습니다.\n1 2 3 4 Map\u0026lt;String, String\u0026gt; cfg = Maps.newHashMap(); // cfg는 configuration을 추가하기위한 객체입니다. cfg.put(\u0026#34;es.read.metadata\u0026#34;, \u0026#34;true\u0026#34;); Dataset\u0026lt;Row\u0026gt; df = JavaEsSparkSQL.esDF(sparkSession, \u0026#34;template_data\u0026#34;, cfg); Read from ES 지금까지 설명드린 방법을 통해 DataFrame에 접근할 수 있게 되었습니다.\n이제는 데이터를 읽어와서 사용해보도록 하겠습니다.\n우선은 DataFrame에 어떤 데이터들이 있는지 확인해보겠습니다.\n아래와 같이 printSchema()함수를 사용하면, DataFrame이 어떤 구조를 갖추었는지 알 수 있습니다.\n1 2 3 4 5 6 7 df.printSchema(); root |-- _class: string (nullable = true) |-- column1: long (nullable = true) |-- column2: string (nullable = true) ... index에 설정된 mapping정보를 토대로 DataFrame의 Schema를 구성합니다.\n여기까지 직접 진행해보시면 아시겠지만, document id와 같은 metadata가 없습니다. 이부분은 좀 의아하긴 하지만, 설정값을 추가하면 쉽게 해결할 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Dataset\u0026lt;Row\u0026gt; df = sparkSession.read() .format(\u0026#34;es\u0026#34;) .option(\u0026#34;es.read.metadata\u0026#34;, \u0026#34;true\u0026#34;) .load(\u0026#34;template_data\u0026#34;); df.printSchema(); root |-- _class: string (nullable = true) |-- column1: long (nullable = true) |-- column2: string (nullable = true) ... |-- _metadata: map (nullable = true) | |-- key: string | |-- value: string (valueContainsNull = true) 또 한가지 의아한 점이 있을 겁니다. array 필드는 어떻게 되는거지? 🤔\nmapping에는 array 타입을 지정해줄 수 없기 때문입니다.\n가이드 문서를 살펴봐도 element에 사용되는 데이터 타입을 명시하도록 안내하고 있습니다.\nDataFrame Schema가 잘못 지정되서, 파싱 에러가 발생할 수 있지 않을까? 질문으로 이어집니다.\n이 부분도 마찬가지로 설정으로 해결가능하게끔 만들어져 있습니다.\n저도 이 문제를 겪었다보니, 설정으로 해결하였습니다.\n1 2 3 4 5 Dataset\u0026lt;Row\u0026gt; df = sparkSession.read() .format(\u0026#34;es\u0026#34;) .option(\u0026#34;es.read.metadata\u0026#34;, \u0026#34;true\u0026#34;) .option(\u0026#34;es.read.field.as.array.include\u0026#34;, \u0026#34;productMapping,sellerMapping\u0026#34;) .load(\u0026#34;template_data\u0026#34;); 어떤 필드를 array 로 파싱해야하는지 명시하였습니다.\n이것과 반대로 array로 파싱하면 안되는 필드를 명시할 수도 있습니다.\nes.read.field.as.array.exclude 값을 설정해주면 됩니다.\nES에 저장된 데이터들과 mapping의 데이터 타입이 불일치 하지않는 이상, 여기까지 진행했다면 ES데이터를 파싱하는데 문제가 발생하지 않습니다.\nDataFrame을 직접 Handling할 수도 있지만, 임시적으로 사용할 View를 생성해주면 아래처럼 SQL을 사용할 수도있습니다.\n1 2 df.createTempView(\u0026#34;tmp_front\u0026#34;); Dataset\u0026lt;Row\u0026gt; filtered_df = sparkSession.sql(\u0026#34;select _metadata, \u0026#34; + String.join(\u0026#34;,\u0026#34;, columns) + \u0026#34; from tmp_front\u0026#34;); Customize Field ES에 있는 데이터를 그대로 사용하는것이 아니라, 변환을 거쳐 사용해야할 수도 있습니다.\n예를들면 Serialized Json String이 필드로 존재한다면, Deserialize한 후에 Map으로 사용하는 경우가 있습니다.\n물론 예시는 저의 사례입니다. ㅎ\nProcessing을 하기에 앞서 Spark가 어떻게 실행되는지 간단히 이야기해보겠습니다.\n우리가 작성한 프로그램은 Spark에서 Driver라고 불리며, 다수의 워커 관리 및 운영프로그램의 실행관리 및 모니터링과 같은 역할을 수행합니다.\n데이터 조회와 분산처리의 경우에는 Worker(Executor)가 할당되어 처리하게됩니다. 다수의 Worker가 처리하기 때문에 큰 규모의 데이터를 처리하더라도 빠른시간안에 처리가 가능합니다.\n물론 Worker에서 처리한 데이터를 Driver에서 처리하게 할수는 있지만, 상당히 느려질 것입니다.\nSpark SQL로 만든 DataFrame은 Spark의 Worker에 분산되어 저장되어있습니다.\n따라서 필드의 Processing을 위해서는 다수의 Worker에서 함수를 실행할 수 있어야합니다.\n이를 위해서는 Serializable Interface로 구현해주면 됩니다.\n하지만 이미 만들어진 Class를 Serializable로 변경하는 작업은 쉽지않을 겁니다.\n대안으로 static function으로 만들어서 해결할 수 있습니다.\n예시로 말씀드렸던 Deserialize한 후 컬럼으로 등록하는 과정을 나열하면 아래와 순서대로 되겠습니다.\n필드를 가공할 수 있는 UDF(User Defined Function)을 static function으로 만든다. SparkSession에 UDF 등록 DataFrame의 컬럼으로 추가 위의 3가지 과정을 코드로 작성하면 아래와 같습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 public static UDF1\u0026lt;String, HashMap\u0026lt;String, String\u0026gt;\u0026gt; jsonStringToMap = jsonStr -\u0026gt; { ObjectMapper mapper = new ObjectMapper(); return mapper.readValue(jsonStr, new TypeReference\u0026lt;HashMap\u0026lt;String, String\u0026gt;\u0026gt;() {}); }; @Override public RepeatStatus execute(StepContribution contribution, ChunkContext chunkContext) throws Exception { sparkSession.udf().register(\u0026#34;jsonStringToMap\u0026#34;, jsonStringToMap, DataTypes.createMapType(DataTypes.StringType, DataTypes.StringType)); df.createTempView(\u0026#34;tmp_front\u0026#34;); Dataset\u0026lt;Row\u0026gt; filtered_df = sparkSession.sql(\u0026#34;select _metadata, \u0026#34; + String.join(\u0026#34;,\u0026#34;, columns) + \u0026#34; from tmp_front\u0026#34;); filtered_df = filtered_df.withColumn(\u0026#34;dataFieldMap\u0026#34;, functions.callUDF(\u0026#34;jsonStringToMap\u0026#34;, filtered_df.col(\u0026#34;dataFields\u0026#34;))); } 이렇게 UDF 함수를 각 Worker에서 호출할 수 있게 됨으로서, 분산저장된 DataFrame을 병렬로 Processing할 수 있습니다.\nConclusion 여기까지 Spark를 사용해 Elasticsearch 데이터를 Hive에 연동하는 과정까지 진행해보았습니다. 4가지 방식중 저의 니즈에 맞는 Spark-SQL만 설명드렸지만, 사용하는 방법은 나머지도 비슷비슷합니다. 상황에 맞는 적절한 방식을 선택해서 진행하시면 되겠습니다.\nGuide 문서만 보고 시작을 헀는데, 실력이 부족해서 그런지 삽질 좀 했었습니다. 🫠\n이와 비슷한 일을 하셔야하는 분들에게 도움이 되었으면 해서 기록으로 남겨봅니다.\n감사합니다.\nReference Spark-SQL 연동 공식문서 : https://www.elastic.co/guide/en/elasticsearch/hadoop/7.17/spark.html#spark-sql Configuration 공식문서 : https://www.elastic.co/guide/en/elasticsearch/hadoop/7.17/configuration.html Dependency Elasticsearch-spark-30 : https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-spark-30 Elasticsearch-hadoop : https://mvnrepository.com/artifact/org.elasticsearch/elasticsearch-hadoop ","date":"2023-08-11T00:09:54+09:00","permalink":"https://korcasus.github.io/p/elasticsearch-spark/","title":"Elasticsearch Spark"},{"content":"Previous to Start 안녕하세요. 이번 글은 Twitch에서 공식 제공하는 문서인 Handling Webhook Events를 참고하여 작성하였습니다.\n지금보니 어떤 문서들보다 상세하게 잘 작성되어있는편이지만, 처음에 사용하고자 했을때 헤매었던 경험이 있어 다른사람들에게 도움이 되었으면 바램으로 간단히 Guide문서를 작성해보게되었습니다.\n인증에 대한 방법은 따로 기술하지않았습니다. 추후에 기회가 된다면 기술할 예정입니다.\nOverview Event Subscription은 Twitch에서 발생한 Event를 구독할 수 있는 기능을 말합니다.\n여러가지의 형태들의 Event들이 존재합니다.\n이 중에서 EventSub을 통해 구독받기 원하는 Event를 Twitch 서버에 등록 할 수 있습니다.\n등록해 두었던 Event가 트위치에서 발생하면 Application에 알림을 전송해 줍니다. 예를 들면 아래 종류의 notification을 전달받을 수 있습니다.\n스트리머가 온라인 상태일때 스트리머의 새로운 follower가 생겼을때 스트리머의 새로운 구독자가 생겼을때 시청자가 채널 응원할때 시청자가 포인트를 기부했을때? Twitch는 전송하는데 2가지 방식을 지원합니다.\nWebhook WebSocket 이 문서에서 설명하고자 하는 방식은 Webhook에 해당됩니다.\nRequired 3 API Event Subscription을 사용하여 Event를 수신받고 싶다면 3가지 API(Callback)이 필요로 합니다. 이 Callback들은 무조건 SSL을 사용해야하며, 443 port를 사용합니다.\nRequest Header에 Twitch-Eventsub-Message-Type가 포함되는데, 아래의 3가지 Notification Type을 Value로 가질 수 있습니다.\nwebhook_callback-verification : Callback으로 등록한 Event Handler가 구독 요청한 사람의 소유인지 증명하는데 사용합니다. Event 구독을 하면, 처음으로 수신받는 Event 입니다. notification : Event 데이터를 포함하고 있습니다. 실제로 Event를 처리하는 부분에 해당합니다. revocation : 어떤 사유로, Twitch 서버에서 구독을 취소하게 되면 사용합니다. 이유를 포함하게 됩니다. 위 3개 Type의 Request가 유입되면, 몇초내에 응답해주어야합니다.\n오랜 시간이 걸리면 Timeout으로 처리합니다. 짧은 시간안에 많은 실패가 발생한다면, Subscription 상태는 notification_failures_exceeded 으로 바뀌고 Twitch는 Event Subscription을 해지할 것입니다.\n만약 서버에서 빠른 시간안에 처리가 불가능하다면, 이벤트를 임시저장소에 저장하거나 2XX로 응답한후 처리하는것을 고려해야합니다.\nCommon (Verifying the Event Message) 핸들러에게 전송된 메시지로 어떤 작업을 하기 전에, 악의를 가진사람이 메시를 전송한 것일수도 있으므로 해당 메시지가 Twitch에서 보낸 것인지 검증해야 합니다. 이벤트에 구독할때 이벤트에 Secret을 포함하게 되어있습니다. Twitch에서는 HMAC-SHA256(해시 기반 메시지 인증 코드) 서명을 생성할 때 Secret을 키로 사용합니다. Twitch는 Twitch-Eventsub-Message-Signature 요청 헤더에서 핸들러에게 HMAC 서명을 전달하여, 인증합니다.\nSecret과 Twitch-Eventsub-Message-Id 헤더, Twitch-Eventsub-Message-Timestamp 헤더, Request Body의 값을 연결한 메시지를 사용하여 HMAC 서명을 생성합니다.(순서가 중요합니다)\nTwitch-Eventsub-Message-Signature 헤더에 Twitch가 전송한 HMAC과 내가 만든 HMAC을 비교합니다.\n서명이 일치하지 않으면 4XX 상태 코드를 반환해야합니다.\n아래는 Twitch에서 공식적으로 제공한 서명 생성코드 예제입니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 const crypto = require(\u0026#39;crypto\u0026#39;) // Notification request headers const TWITCH_MESSAGE_ID = \u0026#39;Twitch-Eventsub-Message-Id\u0026#39;.toLowerCase(); const TWITCH_MESSAGE_TIMESTAMP = \u0026#39;Twitch-Eventsub-Message-Timestamp\u0026#39;.toLowerCase(); const TWITCH_MESSAGE_SIGNATURE = \u0026#39;Twitch-Eventsub-Message-Signature\u0026#39;.toLowerCase(); // Prepend this string to the HMAC that\u0026#39;s created from the message const HMAC_PREFIX = \u0026#39;sha256=\u0026#39;; app.use(express.raw({ // Need the raw message body for signature verification type: \u0026#39;application/json\u0026#39; })) . . . let secret = getSecret(); let message = getHmacMessage(req); let hmac = HMAC_PREFIX + getHmac(secret, message); // Signature to compare to Twitch\u0026#39;s if (true === verifyMessage(hmac, req.headers[TWITCH_MESSAGE_SIGNATURE])) { console.log(\u0026#34;signatures match\u0026#34;); // Get JSON object from body, so you can process the message. let notification = JSON.parse(req.body); // Handle notification... } else { console.log(\u0026#39;403\u0026#39;); res.sendStatus(403); } . . . function getSecret() { // TODO: Get your secret from secure storage. This is the secret you passed // when you subscribed to the event. return \u0026#39;\u0026lt;your secret goes here\u0026gt;\u0026#39;; } // Build the message used to get the HMAC. function getHmacMessage(request) { return (request.headers[TWITCH_MESSAGE_ID] + request.headers[TWITCH_MESSAGE_TIMESTAMP] + request.body); } // Get the HMAC. function getHmac(secret, message) { return crypto.createHmac(\u0026#39;sha256\u0026#39;, secret) .update(message) .digest(\u0026#39;hex\u0026#39;); } // Verify whether your signature matches Twitch\u0026#39;s signature. function verifyMessage(hmac, verifySignature) { return crypto.timingSafeEqual(Buffer.from(hmac), Buffer.from(verifySignature)); } 모든 Request에 대해서 이 검증작업이 수행되어야합니다. 보안상으로 중요한 이슈이기 때문에, 잘 만들어 주는게 좋습니다.\nResponding to Challenge Request 이벤트를 정기구독하면 Twitch는 요청에 지정된 이벤트 핸들러를 소유하고 있는지 확인하기 위해 Authentification Challenge를 보냅니다.\n챌린지 메시지는 Twitch-Eventsub-Message-Type 헤더를 webhook_callback_verification으로 설정합니다.\n예를 들면 아래와 같습니다.\n1 Twitch-Eventsub-Message-Type: webhook_callback_verification 요청 본문의 challenge 필드에는 응답해야 하는 challenge 값이 포함되어 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 { \u0026#34;challenge\u0026#34;: \u0026#34;pogchamp-kappa-360noscope-vohiyo\u0026#34;, \u0026#34;subscription\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;f1c2a387-161a-49f9-a165-0f21d7a4e1c4\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;webhook_callback_verification_pending\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;channel.follow\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;cost\u0026#34;: 1, \u0026#34;condition\u0026#34;: { \u0026#34;broadcaster_user_id\u0026#34;: \u0026#34;12826\u0026#34; }, \u0026#34;transport\u0026#34;: { \u0026#34;method\u0026#34;: \u0026#34;webhook\u0026#34;, \u0026#34;callback\u0026#34;: \u0026#34;https://example.com/webhooks/callback\u0026#34; }, \u0026#34;created_at\u0026#34;: \u0026#34;2019-11-16T10:11:12.634234626Z\u0026#34; } } 응답은 200 상태 코드를 반환해야 하며, Response Header에 Content-Type: text/plain이 포함되어야 하고 Response Body는 challenge만 포함해야 합니다. 성공하면 구독이 활성화된 것입니다.\n서버에서 웹 프레임워크를 사용하는 경우 웹 프레임워크가 호환되지 않는 방식으로 응답을 수정하지 않도록 주의하세요.\nProcessing an event 정기구독한 이벤트가 발생하면 Twitch는 이벤트 데이터가 포함된 알림 메시지를 이벤트 처리자에게 보냅니다. 알림 메시지는 Twitch-Eventsub-Message-Type 헤더를 알림으로 설정합니다. 예를 들어 Twitch-Eventsub-Message-Type: notification 로 볼 수 있습니다.\nRequest body에는 subscription와 event에 정보가 포함되어있습니다.\nsubscription.type 필드는 이벤트 유형을 식별할 수 있는 정보를 포함하고 있습니다.\nevent 필드에는 구독한 이벤트 타입에서 실제로 발생한 이벤트의 데이터가 포함됩니다.\n아래는 예시입니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 { \u0026#34;subscription\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;f1c2a387-161a-49f9-a165-0f21d7a4e1c4\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;enabled\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;channel.follow\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;cost\u0026#34;: 1, \u0026#34;condition\u0026#34;: { \u0026#34;broadcaster_user_id\u0026#34;: \u0026#34;12826\u0026#34; }, \u0026#34;transport\u0026#34;: { \u0026#34;method\u0026#34;: \u0026#34;webhook\u0026#34;, \u0026#34;callback\u0026#34;: \u0026#34;https://example.com/webhooks/callback\u0026#34; }, \u0026#34;created_at\u0026#34;: \u0026#34;2019-11-16T10:11:12.634234626Z\u0026#34; }, \u0026#34;event\u0026#34;: { \u0026#34;user_id\u0026#34;: \u0026#34;1337\u0026#34;, \u0026#34;user_login\u0026#34;: \u0026#34;awesome_user\u0026#34;, \u0026#34;user_name\u0026#34;: \u0026#34;Awesome_User\u0026#34;, \u0026#34;broadcaster_user_id\u0026#34;: \u0026#34;12826\u0026#34;, \u0026#34;broadcaster_user_login\u0026#34;: \u0026#34;twitch\u0026#34;, \u0026#34;broadcaster_user_name\u0026#34;: \u0026#34;Twitch\u0026#34;, \u0026#34;followed_at\u0026#34;: \u0026#34;2020-07-15T18:16:11.17106713Z\u0026#34; } } 응답은 2XX 상태 코드를 반환해야 합니다.\n이벤트 처리에 1~2초 이상 걸리는 경우 이벤트를 임시 저장소에 쓰고 2XX로 응답한 후 알림을 처리하는 것이 좋습니다.\n너무 많이 빠르게 응답하지 않으면 정기구독 상태가 notification_failures_exceeded로 변경되고 Twitch에서 구독을 취소해버립니다.\n이벤트 발생시 처리하고 싶은 비지니스 로직은 이 부분에 Request를 처리하며 포함하면 됩니다.\nRevoking your subscription 정기구독은 만료되지 않지만 Twitch는 정기구독을 취소할 수 있습니다. Twitch는 다음과 같은 경우에 정기구독을 취소합니다\n정기구독에 언급된 유저가 더 이상 존재하지 않는 경우. 알림 상태가 user_removed로 설정된 경우. 사용자가 인증 토큰을 해지했거나 단순히 비밀번호를 변경한 경우. 알림의 상태가 authorization_revoked로 설정됩니다. 콜백이 적시에 응답하지 않은 횟수가 너무 많습니다. 알림의 상태가 알림_실패_초과로 설정됩니다. 구독한 구독 유형 및 버전이 더 이상 지원되지 않습니다. 알림의 상태가 version_removed로 설정되어 있습니다. Twitch는 언제든지 정기구독을 취소할 수 있는 권리를 보유합니다. 해지 메시지는 Twitch-Eventsub-Message-Type 헤더를 revocation로 설정합니다.\n예를 들어 아래와 같습니다.\n1 Twitch-Eventsub-Message-Type: revocation 요청 본문의 status 필드에는 Twitch가 정기구독을 취소한 이유가 표시됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 { \u0026#34;subscription\u0026#34;: { \u0026#34;id\u0026#34;: \u0026#34;f1c2a387-161a-49f9-a165-0f21d7a4e1c4\u0026#34;, \u0026#34;status\u0026#34;: \u0026#34;authorization_revoked\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;channel.follow\u0026#34;, \u0026#34;cost\u0026#34;: 1, \u0026#34;version\u0026#34;: \u0026#34;1\u0026#34;, \u0026#34;condition\u0026#34;: { \u0026#34;broadcaster_user_id\u0026#34;: \u0026#34;12826\u0026#34; }, \u0026#34;transport\u0026#34;: { \u0026#34;method\u0026#34;: \u0026#34;webhook\u0026#34;, \u0026#34;callback\u0026#34;: \u0026#34;https://example.com/webhooks/callback\u0026#34; }, \u0026#34;created_at\u0026#34;: \u0026#34;2019-11-16T10:11:12.634234626Z\u0026#34; } } 응답은 2XX 상태 코드를 반환해야 합니다.\nTesting your Handler Twitch의 명령줄 인터페이스(CLI)는 이벤트 핸들러를 테스트하는 데 사용할 수 있는 이벤트 명령을 제공합니다. 이 명령은 SSL이 필요하지 않으며, 손쉽게 테스트할 수 있습니다.\n도전 이벤트(Testing Challenge events) 알림 이벤트(Testing notification events) Testing challenge Events 이벤트에 가입하면 Twitch는 요청에 지정된 이벤트 처리자에게 webhook_callback-verification 알림 이벤트를 보내, Event Handler를 소유하고 있는지 확인합니다.\n다음 CLI 예제는 Challenge Event에 대한 응답을 테스트하는 방법을 보여줍니다.\n실행하기 전에 -F 플래그의 값을 핸들러를 가리키도록 변경하고 -s 플래그의 값을 비밀로 변경하세요.\n비밀 플래그는 선택 사항이지만 항상 지정해야 핸들러에서 메시지가 Twitch에서 왔는지 확인하는 부분이 작동합니다.\n그런 다음 터미널 창을 열고 명령을 실행합니다.\n1 twitch event verify-subscription subscribe -F http://localhost:8080/eventsub/ -s 5f1a6e7cd2e7137ccf9e15b2f43fe63949eb84b1db83c1d5a867dc93429de4e4 모든 작업을 올바르게 수행했다면 출력은 다음과 같습니다.\n1 2 ✔ Valid response. Received challenge 5e784545-5a59-a05a-6ae6-cfaf260532f1 in body ✔ Valid status code. Received status 200 verify-subscription 하위 명령 사용에 대한 자세한 내용은 이벤트 처리기의 챌린지 응답 확인하기를 참조하세요.\nTesting notification events 트리거 하위 명령을 사용하여 이벤트 핸들러에 모의 이벤트를 전송합니다. 명령의 인수는 트리거할 이벤트를 식별합니다.\n실행하기 전에 -F 플래그의 값을 핸들러를 가리키도록 변경하고 -s 플래그의 값을 시크릿으로 변경하세요.\n비밀 플래그는 선택 사항이지만 항상 지정해야 핸들러에서 메시지가 Twitch에서 왔는지 확인하는 부분이 작동합니다.\n그런 다음 터미널 창을 열고 명령을 실행합니다.\n1 twitch event trigger subscribe -F http://localhost:8080/eventsub/ -s 5f1a6e7cd2e7137ccf9e15b2f43fe63949eb84b1db83c1d5a867dc93429de4e4 Conclusion Twtich EventSubscription을 어떻게 사용해야하는지, 어떻게 테스트해야하는지에 대해서 살펴보았습니다. Twitch 공식문서에 잘 설명 되어있기때문에, 덧붙일 내용이 많진 않았습니다.\n하지만 처음보는 사람으로서 어떤 순서로 사용하는지, 어떤 용도로 사용해야하는건지 파악하기 힘들어 정리해보았습니다. (경험이 부족한것일수도있겠지만요\u0026hellip;)\n다른 분들께 도움이 되면 좋겠습니다.\nReference Twitch Event Subscription 공식문서 : https://dev.twitch.tv/docs/eventsub/handling-webhook-events/ ","date":"2023-04-09T14:34:00+09:00","permalink":"https://korcasus.github.io/p/twitch-eventsub/","title":"Twitch Eventsub"},{"content":"How to Build Fat Jar Build Tool을 어떤 종류를 사용하는지에 따라 방법이 달라집니다.\nMaven은 maven-assembly-plugin\nGradle Shadow Plugin 을 사용해야합니다.\n대부분은 위의 2개 Plugin중 하나만 사용하면 해결되지만, Spring Boot는 구동방식이 달라서, 별도의 처리가 필요합니다.\nBoot-Jar Spring에서 공식적으로 권장하는 방법입니다.\n설명에 따르면 Java는 nested-jar파일을 load하는 방법을 제공하지않습니다. 이때문에 압축 풀지않고, 실행해야하는 환경이라면 문제가 될 수 있습니다.\n이 때문에 Shaded Jar(Fat Jar)를 사용해야합니다. 모든 Jars파일로부터 모든 Class를 모아서 Pacakage하여 하나의 Jar파일로 만들어 줍니다.\n실제로 Application에 존재하는 library를 보기에 힘들어지기때문에, Spring은 다른 방법을 제공합니다.\n어떤방식으로 구현되었는지가 궁금하다면 상세한 내용은 공식문서를 참고해주세요.\nConfiguration 1. Module 추가 implementation 'org.springframework.boot:spring-boot-loader'\n위의 모듈은 Spring boot 를 Excutable Jar 또는 War로 만들수 있게 도와줍니다.\n2. Manifest 변경 JarLauncher를 통해 실행되어야하기 때문에, Manifest 변경이 필요합니다.\nMETA-INF/MANIFEST.MF 에 내용이 존재하는데. Build하며 기록되어야 합니다.\n공식문서에는 아래와같이 가이드 하고 있습니다.\n1 2 Main-Class: org.springframework.boot.loader.JarLauncher Start-Class: com.mycompany.project.MyApplication 저의 경우에는 아래와 같이 적용하였습니다.\n1 2 3 4 5 6 jar { manifest { attributes \u0026#34;Main-Class\u0026#34;: \u0026#34;org.springframework.boot.loader.JarLauncher\u0026#34; attributes \u0026#34;Start-Class\u0026#34;: \u0026#34;com.mycompany.team.batch.SparkSpringBatchClient\u0026#34; } } 문제점 java -jar xxx.jar 명령어로 실행할 경우, 정상동작합니다.\n하지만 Spark에서 실행할 경우, 문제가 발생합니다.\n이유는 Dependency version 때문이었습니다.\n재직중인 회사에서 운영중인 Spark는 2.4버전을 사용 중인데, 여기에 포함되어있는 gson 버전이 Application에서 사용해야할 버전보다 낮았습니다. 이때문에 구버전의 Gson으로 사용되면서 Application구동 실패 하였습니다.\n강제로 최신버전의 Gson을 사용할 수 있는 방안을 찾아보았습니다.\n첫번째로는 사용할 Dependency버전을 명시하는 방법이였습니다.\n(참고로 이 방법을 사용하지않아도 비교적 최신 gson을 사용합니다.) 하지만 spark에 내장되어있는 library를 우선으로 사용하기때문에 실패하였습니다.\n두번째 방법은 Spark공식문서에서 제공하는 방법입니다. (참고)\nspark.driver.userClassPathFirst\nspark.executor.userClassPathFirst\n위의 2개 변수는 기본값이 False로 되어있습니다.\n문서에 따르면, 사용자가 옵션으로 추가한 --jars 를 우선시 사용한다고 되어있습니다.\n시험 기능인 문제도 있고, 추가해야할 jar이 증가하면 관리할 포인트가 증가할 수 있습니다. 의도한대로 동작하지도 않았고, 의도했던 바가 아니라 더이상 사용하지 않았습니다.\n이를 해결하려면 relocate(임의로 이름변경?)을 사용해야하는데, Shadow Plugin에서는 지원합니다.\nShadow Plugin 프로젝트 내에 있는 Dependency Classs와 Resource를 하나의 Jar로 만들어 줍니다.\n이로 인해 얻을수 있는 장점은 2가지입니다.\n실행가능한 Jar배포를 만든다. Library에 있는 공통 Dependency를 bundling, relocating하여 classpath conflict를 회피한다. 장점을 좀더 잘 풀어쓴글을 참고하면 좋겠습니다.\nAdd Option For Spring Shadow Plugin으로 Spring Application을 Build하려면 추가 옵션이 필요합니다.\n아래의 내용을 추가합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 import com.github.jengelman.gradle.plugins.shadow.transformers.* shadowJar { // Required for Spring mergeServiceFiles() append \u0026#39;META-INF/spring.handlers\u0026#39; append \u0026#39;META-INF/spring.schemas\u0026#39; append \u0026#39;META-INF/spring.tooling\u0026#39; transform(PropertiesFileTransformer) { paths = [\u0026#39;META-INF/spring.factories\u0026#39; ] mergeStrategy = \u0026#34;append\u0026#34; } } 상세한 내용은 Git Issue를 읽어보면 알 수 있습니다.\nRelocating Packages Classpath에 동일한 Dependency가 존재하여, 의도하고자한 버전을 사용하지 못하는 상황일때 사용하는 기능입니다. Hadoop, Spark에서 빈번히 발생합니다.\n이 아이디어는 간단합니다. 충돌나는 Dependency를 Project에서 찾아낸뒤, 이름(경로)을 바꿔줍니다. 다른 이름의 Dependency를 사용하기때문에 문제가 생기지 않습니다.\n구현방법은 간단합니다.\n1 2 3 4 // Relocating a Package shadowJar { relocate \u0026#39;junit.framework\u0026#39;, \u0026#39;shadow.junit\u0026#39; } juni.framework를 shadow.junit 으로 변경해 줍니다.\n필요에 맞게 아래처럼 적용합니다.\n1 2 relocate \u0026#39;com.google.gson\u0026#39;, \u0026#39;shadow.google.gson\u0026#39; relocate \u0026#39;com.fasterxml.jackson\u0026#39;, \u0026#39;shadow.fasterxml.jackson\u0026#39; Shadow Plugin에서 relocation을 살펴보고싶다면 링크를 참고해주세요.\nClasspath 변경 사실 여기까지 진행하고 나서, 잘 실행하는것을 기대했습니다.\n하지만 java -jar project-all.jar 로 실행해보면, 이상함을 느낄 수 있습니다. Spring자체는 실행되었지만, Main Application이 실행되지 않은 것을 확인 할 수 있습니다.\n찾아보니, 원인을 알기 쉽지 않았습니다.\n다행히 try and error 거치며 알아냈는데, 아래의 링크와 관련있었습니다. https://imperceptiblethoughts.com/shadow/configuration/dependencies/\n기본으로 runtimeClassPath만 포함되서 Build되었고, 아래처럼 compileClassPath를 포함하니 해결되었습니다.\n1 configurations = [project.configurations.compileClasspath, project.configurations.productionRuntimeClasspath] 문제해결이 우선이라, 왜 compileClassPath도 포함해야만 정상동작하는지는 확인해보지 않았습니다.\nConclusion Spring을 사용할때, 어떻게 Fat Jar(Shaded Jar)을 빌드할 수 있는지 살펴보았습니다.\nBoot-Jar은 스프링에서 권장하는 방법이고 간단하다는 장점이 있지만, Dependency 충돌하는 상황이라면 해결할기 어렵다는 문제가 있습니다.\n이런경우는 하둡 클러스터를 사용하는경우에 해당됩니다.\nShadow Plugin을 사용하는 방법은 일반적으로 많이 사용하는 방법입니다.\n하지만 스프링에 적용하려면 스크립트를 추가로 작성해주어야 하지만, Boot-Jar에서 겪었던 문제를 회피할 수 있습니다.\nSpark나 하둡 클러스터를 사용해야하는 경우라면 Shadow Plugin을 사용하는 것을 권장합니다.\n하지만 이와 같은 문제를 부딪힌 상황이 아니라면, 본인의 상황에 맞게 사용하는 것을 권장합니다.\nReference Spring Official Guide : https://docs.spring.io/spring-boot/docs/current/reference/html/executable-jar.html#appendix.executable-jar.alternatives 공식문서 번역 Article : https://wordbe.tistory.com/entry/Spring-Boot-2-Executable-JAR-스프링-부트-실행 Spring fat jar Git Issue for shadow plugin : https://github.com/spring-projects/spring-boot/issues/1828#issuecomment-231104288 전반적인 이해에 도움을 주었던 StackOverflow : https://stackoverflow.com/questions/51206959/classnotfound-when-submit-a-spring-boot-fat-jar-to-spark ","date":"2023-02-20T23:16:51+09:00","permalink":"https://korcasus.github.io/p/spring-fat-jar-for-spark/","title":"Spring Fat-jar for Spark"},{"content":"개요 앞서 설명해왔던 내용과는 반대로 Hive에서 Kafka에 메시지를 전송하는 과정을 설명하겠습니다.\nKafka 입장에서는 Hive가 Producer 역할을 수행하게 됩니다.\n단점 Hive에서 Kafka로 전송할때 필수적인 기능들은 제공하다보니, 일반적인 경우에는 사용하는데 문제 없습니다.\n하지만 일부 기능을 제공하지않는데, 그 중 하나가 Header입니다.\nHeader도 만들어주어야 하는 상황이라면, Hive 말고 다른 Application을 사용하는 것을 권장합니다.\nKafkaWritable Kafka Record 공식문서 kafka metadata 주의사항 Producer에서 Kafka로 메시지를 전송할때는 메타 데이터들을 포함시켜 전송할 수 있습니다.\n포함시키지 않을 경우, Kafka에서 자동으로 생성해주는 것으로 알고 있습니다.\nHive에서도 마찬가지 입니다. __timestamp, __partition, __offset은 kafka에서 기록하는 정보이므로 아래처럼 작성하면 Kafka에서 자동으로 생성해 줄 수 있습니다.\n__timestamp : -1 __partition : null __offset : -1 하지만 __key는 사용할 수 있는 값의 범위는 한정적이다보니 선택지가 제한적이지만, 신중하게 설정해주는 것이 좋습니다.\nKafka는 key를 partition을 결정하는데 사용하므로 메시지 분산에 영향을 미칠수 있습니다. 그리고 cleanup.policy 설정값에 따라 key 활용여부가 결정됩니다.\nKafka에서 key값을 기준으로 동일한 partition에 넣어주는 역할을 수행하고 있으며 , log.cleanup.policy = compact 로 설정할경우 key를 활용하기 때문에 byte array로 넣어주는게 좋습니다.\nInteger to byte array : cast(cast(1 AS STRING) AS BINARY) String to byte array : cast(\u0026lsquo;1\u0026rsquo; AS BINARY) JSON 테스트 케이스 JSON 포맷 메시지를 Kafka에 전송해보도록 하겠습니다.\n토픽 생성 테스트 메시지를 저장해둘 토픽을 생성합니다.\n아래의 예시는 Kafka 설치시 함께 제공되는 스크립트를 사용하여 topic을 만듭니다.\n./kafka-topics.sh --create --topic \u0026quot;${토픽토픽}\u0026quot; --replication-factor 1 --partitions 1 --bootstrap-server \u0026quot;{서버서버}\u0026quot;\n테이블 생성 Kafka에 전송할 topic을 만들어보았으니, 이제는 실제 전송역할을 수행해줄 table을 생성해보겠습니다. Kafka Handler를 사용할 수 있도록 KafkaStorageHandler를 지정해줍니다.\nTBLPROPERTIES에는 앞서 설명드렸던 글과 비슷하게 진행합니다.\n다만, 메시지 전송하는 역할이니 consumer가 아닌 producer로 지정해줍니다.\n아래의 예시를 참고하면 좋겠습니다.\n1 2 3 4 5 6 7 8 9 10 11 CREATE EXTERNAL TABLE test_export_to_kafka( `id` STRING, `value` STRING ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES( \u0026#34;kafka.serde.class\u0026#34; = \u0026#34;org.apache.hadoop.hive.serde2.JsonSerDe\u0026#34;, \u0026#34;kafka.topic\u0026#34; = \u0026#34;${토픽토픽}\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;${서버서버}\u0026#34;, \u0026#34;kafka.producer.security.protocol\u0026#34; = \u0026#34;PLAINTEXT\u0026#34; ); 테스트 데이터 전송 만들어준 테이블에 데이터를 입력해보도록 하겠습니다. 테이블에 명시한 컬럼인 id, value에 대한 값을 필수로 추가합니다. 그 외에는 Kafka MetataData 이며 key, partition, offset, timestamp 순서대로 생각하고 넣어주시면 됩니다.\ninsert into test_export_to_kafka values ('1', '1', null, null, -1, -1);\nONLY LONG TYPE 테스트 케이스 이번에는 JSON이 아닌 PLAIN TEXT Format으로 데이터를 전송해보도록하겠습니다.\n토픽 생성 앞서 설명드렸던 JSON 테스트 케이스와 비슷합니다.\n테스트 메시지를 저장해둘 토픽을 생성합니다.\n아래의 예시는 Kafka 설치시 함께 제공되는 스크립트를 사용하여 topic을 만듭니다.\n./kafka-topics.sh --create --topic \u0026quot;${토픽토픽}\u0026quot; --replication-factor 1 --partitions 1 --bootstrap-server \u0026quot;{서버서버}\u0026quot;\n테이블 생성 Kafka Handler와 TBLPROPERTIES는 앞서 설명드렸던 테스트 케이스와 동일하게 설정합니다.\n다만 이번엔 PLAIN TEXT Format인 만큼, 컬럼은 하나만 만들어 주겠습니다.\n1 2 3 4 5 6 7 8 9 10 CREATE EXTERNAL TABLE test_export_to_kafka_long( id bigint ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES( \u0026#34;kafka.serde.class\u0026#34; = \u0026#34;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\u0026#34;, \u0026#34;kafka.topic\u0026#34; = \u0026#34;${토픽토픽}\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;${서버서버}\u0026#34;, \u0026#34;kafka.producer.security.protocol\u0026#34; = \u0026#34;PLAINTEXT\u0026#34; ); 테스트 데이터 전송 만들어둔 테이블에 데이터를 입력해보도록 하겠습니다.\n테이블에 명시한 컬럼인 id에 대한 값을 필수로 추가합니다. 그 외에는 Kafka MetaData에 대한 정보로 이전과 동일하게 추가해줍니다. insert into test_export_to_kafka_long values (1, null, null, -1, -1);\n동일한 key를 가진 메시지가 동일한 partition에 들어가는 테스트 Producer로 사용할 Application을 만들때 보면, Kafka에 전송할 Message가 어떤 Partition에 저장될지 Partitioner를 지정할 수 있습니다.\n하지만 Hive에서는 별도로 Partitioner를 지정할 수 없습니다.\nDefault Partitioner를 사용하는것으로 생각하고 진행하셔야 합니다.\nMessage의 Key가 동일하다면, 동일한 Partition에 들어가야 합니다.\n하지만 Hive를 통해 전송해보는 것은 처음이기 때문에 동일한 Partition에 전송되는지 확인이 필요하였습니다.\n이를 확인하기 위해 테스트를 진행하였습니다.\n각기 다른 Key를 가진 여러개의 메시지를 전송한 뒤, 앞서 보낸 메시지와 동일한 Key를 가지지만 다른 내용으로 메시지를 전송합니다.\n자세한 내용은 아래의 쿼리를 통해 확인할 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 CREATE EXTERNAL TABLE test_export_to_kafka_partition( id INT, value STRING ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES( \u0026#34;kafka.serde.class\u0026#34; = \u0026#34;org.apache.hadoop.hive.serde2.JsonSerDe\u0026#34;, \u0026#34;kafka.topic\u0026#34; = \u0026#34;${토픽토픽}\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;${서버서버}\u0026#34;, \u0026#34;kafka.producer.security.protocol\u0026#34; = \u0026#34;PLAINTEXT\u0026#34; ); ./kafka-topics.sh --create --topic ${토픽토픽} --bootstrap-server {서버서버} --config \u0026#34;cleanup.policy=compact\u0026#34; --partitions 4 --replication-factor 2 INSERT INTO TABLE test_export_to_kafka_partition VALUES (1, \u0026#39;1\u0026#39;, CAST(\u0026#39;1\u0026#39; AS BINARY), NULL, -1, -1), (\u0026#34;2\u0026#34;, \u0026#34;2\u0026#34;, CAST(\u0026#39;2\u0026#39; AS binary), -1, -1, -1), (\u0026#34;3\u0026#34;, \u0026#34;3\u0026#34;, CAST(\u0026#39;3\u0026#39; AS binary), -1, -1, -1), (\u0026#34;4\u0026#34;, \u0026#34;4\u0026#34;, CAST(\u0026#39;4\u0026#39; AS binary), -1, -1, -1); CREATE TEMPORARY TABLE test_partition(id INT, value STRING); INSERT INTO TABLE test_partition VALUES (2, \u0026#39;2\u0026#39;), (3, \u0026#39;3\u0026#39;), (4, \u0026#39;4\u0026#39;); INSERT INTO table test_export_to_kafka_partition SELECT id, value, CAST(CAST(id AS string) AS binary) AS `_key`, NULL AS `_partition`, -1, -1 FROM test_partition; 결과화면은 첨부하지 않았지만 테스트를 진행하니 동일한 Key값을 가진경우, 동일한 Partition에 저장되는 것을 확인할 수 있었습니다.\n정리 이번 글을 통해 Hive Table 데이터를 Kafka로 전송가능하다는 것을 확인할 수 있었습니다.\n사실 이렇게 데이터를 전송하는게 유일한 방법도 아니고, 다른 좋은 방법들도 존재합니다.\n(예를 들면 HiveServer2에 JDBC로 연결할 수도 있고, Spark를 사용할 수도 있습니다.)\n설명한 내용들을 읽으셨다면 제약이 많다는 것을 느끼셨을 수 있습니다.\n기능 설명에 작성하진 않았지만, 단순히 전송하는 경우라면 문제없지만 다른 테이블과 JOIN해서 바로 전송하게끔 쿼리를 실행한다면 에러가 발생할 수 있습니다.\n이 문제를 회피하기위해서는 임시 테이블을 만들어 데이터를 저장한 뒤, 임시테이블을 사용해 데이터 전송하게끔 쿼리 작성해야합니다.\n이 외에도 제가 파악하지 못한 제약이 있을수도 있습니다.\n이처럼 많은 제약이 존재하지만, 별도로 Application을 만들 필요가 없다는게 큰 장점 입니다.\n이러한 방법도 여러 선택지중 하나로 사용할 수 있다는 것을 알게되는 계기가 되었으면 합니다.\n여기까지 읽어주셔서 감사합니다.\n","date":"2022-12-01T22:45:04+09:00","permalink":"https://korcasus.github.io/p/hive-kafka-integration3/","title":"Write message to kafka using hive"},{"content":"개요 이전 글에서 Kafka와 Hive 연동에 사용하는 설정들에 대해서 알아보았습니다.\n이제부터는 Kafka에서 데이터를 읽어들이고, 쓰는 작업에 대해서 설명하려고 합니다. 이번 글에서는 Kafka에 있는 데이터들을 읽어들이는 것부터 진행해보도록 하겠습니다.\nHive를 Kafka Consumer로 사용하는 방벙리라 생각하시면 이해하기 쉬우실 겁니다.\n차이점이 있다면, 일반 Consumer와는 다르게 Consumer Group으로 지정되지 않습니다.\nJSON Kafka로부터 읽어들일 데이터들이 json format 일 경우를 가정하고, 진행하는 예제입니다.\nSample data 아래와 같은 형태의 데이터를 예시를 기준으로 설명드리도록 하겠습니다.\n필요에 따라 더 많은 필드들을 추가하실 수 있습니다.\n1 2 3 4 { \u0026#34;id\u0026#34;: 12345678, \u0026#34;value:\u0026#34;: \u0026#34;hello\u0026#34; } 테이블 정의 json 스키마와 동일한 구조를 가지는 테이블을 정의해보도록 하겠습니다. id 필드는 정수값을 가지고, value 필드는 문자열을 가지고 있습니다. Hive에서 정수값은 INT 또는 BIGINT Data Type으로 정의할 수 있습니다. 그리고 문자열은 STRING으로 정의합니다.\n이 정보들을 토대로 DDL쿼리를 만들어보면 아래와 같습니다.\n1 2 3 4 5 6 7 8 9 10 11 CREATE EXTERNAL TABLE test_export_to_kafka( `id` BIGINT, `value` STRING ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES( \u0026#34;kafka.serde.class\u0026#34; = \u0026#34;org.apache.hadoop.hive.serde2.JsonSerDe\u0026#34;, \u0026#34;kafka.topic\u0026#34; = \u0026#34;etl.hive.catalog.used\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;{broker서버}\u0026#34;, \u0026#34;kafka.consumer.security.protocol\u0026#34; = \u0026#34;PLAINTEXT\u0026#34; ); test_export_to_kafka은 테이블명으로, 원하는대로 사용할 수 있습니다.\n테이블에서 사용하는 컬럼들은 읽어들일 json 구조대로 작성해주면 됩니다.\n컬럼명은 json key와 동일하게 작성하고, 컬럼의 타입은 json value의 데이터 타입과 동일하게 맞춰주면 됩니다.\nhive 데이터 타입은 json보다 다양하므로, 문서를 참고해 적절한 타입을 선택해서 사용하는 것을 권장합니다. \u0026quot;kafka.serde.class\u0026quot; = \u0026quot;org.apache.hadoop.hive.serde2.JsonSerDe\u0026quot;으로 지정해줌으로서, hive가 json데이터를 deserialize할 수 있게 되었습니다.\n그 외에 특별히 TBLPROPERTIES에서 살펴봐야할 내용은 없습니다.\nkafka.topic만 사용하고 싶은 kafka topic명으로 변경해주면 됩니다.\nREAD 1 2 SELECT * FROM test_export_to_kafka 정상적으로 설정되었다면, topic에 들어있는 메시지가 출력되는것을 확인하실 수 있습니다.\n하지만 설정이 잘못되었다면 에러메시지가 출력되거나, 각 컬럼이 NULL로 출력 될 것입니다.\n여기까지 설명했던 내용과 다르게 설정한 내용이 있지않은지 확인해주세요.\nTEXT JSON처럼 구조화된 데이터가 아니라, PLAIN TEXT를 읽어오는 예제입니다.\nSample data PLAIN TEXT라, 예제도 특별할 것은 없습니다.\n단순히 숫자값을 예시로 사용해보도록 하겠습니다.\n1 2 3 1234 5678 91011 테이블 정의 1 2 3 4 5 6 7 8 9 10 CREATE EXTERNAL TABLE test_export_to_kafka( `id` BIGINT ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES( \u0026#34;kafka.serde.class\u0026#34; = \u0026#34;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\u0026#34;, \u0026#34;kafka.topic\u0026#34; = \u0026#34;etl.hive.catalog.used\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;{서버서버}\u0026#34;, \u0026#34;kafka.consumer.security.protocol\u0026#34; = \u0026#34;PLAINTEXT\u0026#34; ); 앞서 설명했던 JSON 테스트 케이스와 유사합니다.\n다른점이 있다면 Serde와 컬럼을 다르게 지정해준 것입니다.\nSerde의 경우 \u0026quot;kafka.serde.class\u0026quot; = \u0026quot;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\u0026quot; 로 변경하면 됩니다.\nREAD 1 2 SELECT * FROM test_export_to_kafka 정상적으로 설정되었다면, topic에 들어있는 메시지가 출력되는것을 확인하실 수 있습니다.\n하지만 설정이 잘못되었다면 에러메시지가 출력되거나, 각 컬럼이 NULL로 출력 될 것입니다.\n여기까지 설명했던 내용과 다르게 설정한 내용이 있지않은지 확인해주세요.\nKafka Metadata 활용 Kafka에는 메시지와 함께 메타 데이터들이 함께 기록됩니다.\n테이블을 정의할때 메타 데이터들도 자동으로 컬럼으로 추가됩니다. Hive에서 메시지를 읽어올때, 메타 데이터도 함께 가져오게 됩니다.\n메타 데이터가 컬럼으로 정의되다보니, SQL 조건문으로 활용할 수 있습니다.\n예를 들면 특정 시간 이후로 유입된 데이터만 가져오던가, 특정 파티션에 있는 데이터만 가져올 수 있습니다.\n__partition와 __offset의 경우에는 정수 타입을 사용하기 때문에 사용하기 편리합니다.\nex1) __partition = 0 ex2) __offset \u0026gt; 5000 하지만 __timestamp의 경우에는 unix_timestamp를 사용하기에 int64로 기록됩니다.\n이는 일상에서 사용하는 시간 표기방식과 다르고, 변환이 필요하기에 사용하기 불편합니다. 편하게 조건문으로 활용하기 위해서는 아래처럼 변경해서 사용하는것을 추천합니다.\nunix_timestamp라는 함수를 사용 하면 되는데 아래 예제를 보겠습니다.\nunix_timestamp('2022-03-08 00:00:00.000', 'yyyy-MM-dd HH:mm:ss.SSS') * 1000\n첫번째 인자는 변환하고 싶은 시간을 작성합니다. 두번째 인자는 첫번째 인자에 입력한 시간포맷을 작성합니다. 함수를 사용하여 특정 시간을 unix_timestamp로 변경 할 수 있게 되었고, __timestamp 비교 연산을 할 수 있게되었습니다. 하지만 hive로 통해 읽어들인 __timestamp는 마이크로초 단위이기 때문에 단위를 맞춰주는 추가연산이 필요합니다.\n이때문에 1000을 곱하였습니다. 아래는 사용 예제입니다.\nex) unix_timestamp('2022-03-08 00:00:00.000', 'yyyy-MM-dd HH:mm:ss.SSS') * 1000 \u0026lt; __timestamp\n추가 예제\n1 2 3 4 5 6 7 8 9 -- print to UTC timestamp select FROM_UTC_TIMESTAMP(`__timestamp`, \u0026#39;JST\u0026#39;) from test_kafka_product_received limit 1; -- print timestamp, unixtime to timestamp select current_timestamp(), from_unixtime(unix_timestamp(), \u0026#39;yyyy-MM-dd HH:mm:ss.SSS\u0026#39;); -- print converted unixtime select unix_timestamp(\u0026#39;2022-03-07 14:00:00.000\u0026#39;, \u0026#39;yyyy-MM-dd HH:mm:ss.SSS\u0026#39;); select unix_timestamp(\u0026#39;2022-03-07 06:11:41\u0026#39;, \u0026#39;yyyy-MM-dd HH:mm:ss\u0026#39;); 정리 이번 글을 통해 Hive에서 Kafka에 저장된 JSON 또는 PLAIN-TEXT를 읽어서 사용할 수 있게 되었습니다.\nmetadata를 필터링 용도로 사용한다면, Kafka에 원하는 조건을 가진 메시지를 손쉽게 추출해 볼 수 있겠습니다.\n아쉬운 점으로는 Hive에 저장되어있는 다른 테이블들과 join이 되지 않는다는 점입니다. (제가 시도했을때는 인증으로 인한 문제가 발생했습니다.)\n이 때문에 사용용도가 많이 제약되었지만, 개선된다면 활용도가 높을 것으로 보입니다.\n다음 글에서는 Hive데이터들을 Kafka로 전송하는 과정을 기록해보겠습니다.\n감사합니다.\n","date":"2022-12-01T22:45:01+09:00","permalink":"https://korcasus.github.io/p/hive-kafka-integration2/","title":"Read message from kafka using hive"},{"content":"개요 Hive는 Hadoop에 저장된 대용량의 데이터들을 SQL 문법으로 읽고 쓸수있게 해주는 data warehouse software입니다.\n간단하게 Hadoop에서 동작하는 대용량 데이터 처리 Database라고 생각해 볼 수 있을것 같습니다.\n기본으로 HDFS를 지원하며, Hbase 또는 기타 DBMS에 접근할 수 있도록 개방적인 DataSourcing 기능을 제공하고 있습니다. Message Queue로 많이 활용되는 Kafka도 지원합니다. 이번 시리즈에서는 Hive와 Kafka 연동을 위한 설정부터 Message를 읽고 쓰는 과정까지 기록해보려고 합니다.\n이번 글에서는 사용할때 필요한 설정들과 알아야하는 정보들을 위주로 기록합니다. 사용 예시가 궁금하시다면 다음 글을 읽어주세요.\n공식문서를 토대로 작성하였습니다.\n문서에는 작성되어있지않으나, 사용하려면 추가로 알아야할 내용과 직접 테스트한 예제들을 상세히 기록하고자 합니다.\nHive 3.x 버전에서 진행하였습니다.\n설치 사실 별도의 설치는 필요하지않습니다.\n공식문서를 한번 살펴보셨다면 아시겠지만, Hive에 이미 내장 되어있습니다.\n어떻게 할 지 상상이 안될수도 있지만, SQL을 사용하면 Kafka에 연동을 할 수 있습니다.\n테이블 생성 Hive와 Kafka를 연동해주기 위해서는 연결고리를 만들어주어야합니다.\nSpring에서는 Binder와 Binding을 사용하면 손쉽게 연결고리를 만들 수 있듯이, Hive에서는 StorageHandler를 사용하여 만들 수 있습니다.\nStorageHandler는 table을 정의할 때 함께 명시하면 됩니다. 이 외에도 설정을 추가하면 완벽하게 사용하실수 있습니다.\n다만 Hive에서 Kafka와 연동된 테이블을 만들기 위해서는 몇가지 주의해야하는 사항이 있습니다.\n첫번째로는 EXTERNAL 테이블로 생성해야한다는 것입니다.\n우리가 사용할 방식은 Hive에서 Kafka에 접근하여 저장된 데이터들을 사용하는 것이기 때문에, EXTERNAL 키워드를 추가해주어야합니다.\n두번째로는 KafkaStorageHandler 를 명시해야합니다.\n테이블을 정의할 때 STORED BY 'org.apache.hadoop.hive.kafka.KafkaStorageHandler' 가 함께 적혀있어야 합니다.\n적혀 있어야만 Kafka를 연동하여 사용하실 수 있습니다.\n세번째로는 테이블설정을 해주어야합니다.\nTBLPROPERTIES 키워드 사용하면 테이블에 대한 Property를 설정할 수 있게됩니다. 여기에 Kafka 연동에 필요한 Configuration을 작성해야 합니다.\n세가지를 모두 적용하여 DDL을 작성해보면, 아래의 예시처럼 만들어질 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 CREATE EXTERNAL TABLE kafka_table ( `timestamp` TIMESTAMP, `page` STRING, `newPage` BOOLEAN, `added` INT, `deleted` BIGINT, `delta` DOUBLE ) STORED BY \u0026#39;org.apache.hadoop.hive.kafka.KafkaStorageHandler\u0026#39; TBLPROPERTIES ( \u0026#34;kafka.topic\u0026#34; = \u0026#34;test-topic\u0026#34;, \u0026#34;kafka.bootstrap.servers\u0026#34; = \u0026#34;localhost:9092\u0026#34; ); Kafka metadata Kafka에 저장된 Message는 Payload 외에도 Key, Partition, Offset, Timestamp 4개의 metadata가 존재합니다. 이 데이터들도 Hive에서 사용할 수 있습니다.\n앞서 테이블을 정의할 때 사용하였던 KafkaStorageHandler가 자동으로 metadata들을 컬럼으로 등록해 줍니다.\n__key (byte array) __partition (int32) __offset (int64) __timestamp (int64) 활용하는 방안은 실사용 예제를 작성한 다음글에서 자세히 설명드리도록 하겠습니다.\nAvro 포맷 사용시 주의사항 만약 Confluent Connector를 통해 Avro포맷을 사용한다면, 메시지에서 5-bytes를 제거해주어야합니다.\n이 5-bytes중 1-byte는 매직 byte, 4-byte는 schema registry의 schema id에 해당됩니다.\n이는 \u0026quot;avro.serde.type\u0026quot;=\u0026quot;skip\u0026quot; 그리고 \u0026quot;avro.serde.skip.bytes\u0026quot;=\u0026quot;5\u0026quot;를 설정해주면 해결할 수 있습니다.\navro를 사용할 경우, 상세한 내용은 가이드를 꼭 참고하면 좋겠습니다.\nconfluent avro format 지원은 Hive 4.0버전에 추가될 것으로 보입니다. 3.x 버전을 사용하시는 분이라면, 주의해주시면 좋을것 같습니다. (제가 삽질했던거라서요\u0026hellip;)\nSerde(Serializer, Deserializer) Application에서 Kafka로 데이터 전송을 해본 적 있다면, serializer, deserializer를 지정해보신 경험이 있으실 겁니다.\nHive에서도 마찬가지로 여러 serializer, deserializer를 제공하고있습니다.\nSupported Serializers and Deserializers description org.apache.hadoop.hive.serde2.JsonSerDe JSON 포맷 org.apache.hadoop.hive.serde2.OpenCSVSerde CSV 포맷 org.apache.hadoop.hive.serde2.avro.AvroSerDe AVRO 포맷 org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe binary..? org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe Plain Text 지정하는 방법은 생각보다 매우 간단합니다.\nTBLPROPERTIES에 추가하면 손쉽게 적용할 수 있습니다.\n예를 들면 Kafka에서 읽어들일 데이터포맷이 JSON의 경우이거나, 전송할 데이터가 JSON으로 하고 싶다면 아래처럼 추가하면 됩니다. \u0026quot;kafka.serde.class\u0026quot; = \u0026quot;org.apache.hadoop.hive.serde2.JsonSerDe\u0026quot;\n인증 Kafka에도 인증을 해야 데이터 읽고 쓸수 있는 권한을 주는 기능을 제공하고 있습니다.\nKafka 사용하는데 필요한 인증정보들 또한 Hive를 통해 제공할 수 있습니다.\n저의 경우에는 별도로 인증을 추가하지 않고 사용했다보니, 인증없이 사용한 경우로 기록하겠습니다.\n인증을 사용해야하는 상황이라면, 공식 문서를 읽어보고 진행해 주세요.\n인증을 위한 TBLPROPERTIES 값을 추가하지 않은 상태라면, 연결을 시도했을때 에러가 발생합니다.\n별도의 설정을 안해두었다면, 당연히 인증없이도 사용가능해야되는거 아닌가? 라고 생각할 수 있을텐데요. (제가 그랬습니다.) Hive는 기본값으로 id, password를 사용하여 인증을 시도하려 합니다.\n따라서 인증을 사용하지않는 설정을 추가해주어야만 사용이 가능한데요.\n공식문서에 이 문제를 해결하기 위한 방법을 따로 제시하지않아 많이 어려움을 겪었던 부분이었습니다. 결론부터 이야기하면 TBLPROPERTY를 추가하면 해결할 수 있습니다.\nproducer일 경우 : \u0026quot;kafka.producer.security.protocol\u0026quot; = \u0026quot;PLAINTEXT\u0026quot; consumer일 경우 : \u0026quot;kafka.consumer.security.protocol\u0026quot; = \u0026quot;PLAINTEXT\u0026quot; 인증에 상세히 알아보고 싶다면, 문서를 참고하시면 좋을것 같습니다.\n후에 서술하겠지만 설정할때 kafka.를 prefix로 지정해두면, kafka에 설정값이 전달할 수 있습니다.\nKey Serializer 앞서 설명드렸던 Serializer들은 Value Serializer 였습니다.\n예상하던것보다 많이 제공했지만 반대로 Key Serializer는 Byte Serializer밖에 지원하지 않습니다.\n다른 Serializer로 변경해서 사용하고 싶더라도 변경할 수 없게 되어 있습니다.\n왜 이렇게 구현한지는 모르겠지만, 자세한 내용은 코드를 보면 알 수 있습니다.\n궁금하실 수 있을 것 같아, 링크도 첨부합니다.\nproducer consumer 그외 지금까지 작성한 내용들 이외에도 Kafka를 사용할때, 추가하고 싶은 설정이 더 있을 수 있습니다.\n공식문서에서는 설명하진 않지만, Kafka에 사용할 설정값을 전달할 수 있습니다.\nproducer로 사용할 경우 “kafka.producer.{property}” = “{value}” consumer로 사용할 경우 “kafka.consumer.{property}” = “{value}” TBLPROPERTIES에 추가해주면, Kafka에 설정값들이 전달되며 원하는 설정을 적용할 수 있습니다. 어떻게 설정값이 전달될 수 있는지, 구현된 코드가 궁금하다면 아래의 링크를 참고하시면 좋겠습니다.\nproducer consumer 정리 여기까지 Hive에서 Kafka를 Read, Write하기 위해서 필요한 설정들과, 도움이 되는 정보들에 대해 알아 보았습니다.\n다음 글에서는 Kafka에 있는 Message들을 Hive에서 Read하는 과정을 기록해보겠습니다.\n감사합니다.\n","date":"2022-12-01T22:44:01+09:00","permalink":"https://korcasus.github.io/p/hive-kafka-integration1/","title":"Hive Kafka Integration"},{"content":"들어가기에 앞서 안녕하세요.\nSynology DS220+ 구매한 뒤로 Bitwarden, WebDAV, \u0026hellip; 다양한 용도로 사용하고있습니다.\n이전까지는 DuckDNS와 함께 DSM 6.X 사용중이었습니다만, DSM 업데이트 이후로 만들어두었던 자동화 프로세스가 동작하지 않더라구요. 이번 기회에 갖고싶던 Domain을 CloudFlare에서 Domain 구매를 한 뒤, 새롭게 프로세스를 정리하였습니다.\n공식 가이드 대로 진행했던 과정을 순차적으로 정리하려고 합니다.\nDomain 구입과 DDNS 설정에 대한 내용은 기회가 되면 작성하도록 하겠습니다.\n사용한 환경 DSM 7.X Synology DS220+ Domain Provider : CloudFlare Let\u0026rsquo;s Encrypt란? HTTPS를 사용할 때는 CA(인증기관)에서 제공하는 인증서를 사용합니다.\n원하는 도메인으로 인증서를 발급하고자 할때, 업체마다 다른 가격을 지불해야합니다. Let\u0026rsquo;s Encrypt는 비영리 기관으로서 무료로 TLS인증서를 제공합니다.\n장점은 무료로 인증서를 제공받을 수 있지만,\n단점으로는 타업체는 유효기간이 1년인데 비해 짧은 3개월이라는 점입니다.\nacme.sh 는 뭔가요? Let\u0026rsquo;s Encrypt는 ACME 프로토콜을 사용하여 도메인 소유자인지 확인한 뒤에 인증서를 발급합니다. 인증서 발급을 위해서는 ACME 클라이언트 소프트웨어를 선택해야하는데요.\nCertBot 사용을 권장하지만, 상황에 따라 다른 소프트웨어를 선택할 수 있습니다. acme.sh는 bash를 사용해 인증서를 발급해 줄 수 있는 대체 소프트웨어입니다.\n본인의 선호도에 따라 선택할 수 있습니다.\n상세한 내용은 공식문서를 참고해주세요.\nCloudFlare API Token 발급 인증서 발급 절차에 대해 정확하게 알지는 못해도, 파악한 흐름은 아래와 같습니다.\n원하는 Domain의 인증서 발급 요청 (to Let\u0026rsquo;s Encrypt) 해당 Domain의 소유자임을 증명하기 위해 TXT Record값을 응답값으로 받음 CloudFlare API를 사용하여 TXT Record 값을 등록 TXT Record값을 확인한 후, 인증서 발급 위 과정에서 사용되는 CloudFlare API를 사용하기위해 Token 발급을 진행합니다.\nGlobal Key로도 사용가능하지만, 필요이상의 권한을 가지고 있기때문에 진행하진 않습니다.\ntoken을 발급하고자 하는 Domain을 선택하면, 아래의 같은 Home 화면이 나타납니다.\n빨간색 화살표를 클릭합니다.\nAPI Token 생성 버튼을 누르면, 아래의 같은 페이지가 나타나게됩니다.\n우리는 DNS 관련된 수정 권한을 Token을 발행해야하기 때문에, 빨간색 화살표의 버튼을 누릅니다.\n아래의 이미지에서 1번에 권한이 필요한 Domain을 선택합니다.\n그뒤에는 2번을 선택해 바로 생성하도록 합니다.\n정상적으로 진행되었다면, Token값을 확인할 수 있습니다.\n이 값은 더이상 볼 수 없기 때문에 잘 기억해주시기 바랍니다.\nACME 설치 및 인증서 발급 인증서 발급을 진행해줄 Client를 설치하도록 하겠습니다.\n아래의 명령어대로 설치를 진행하도록 합니다.\n계정은 root로 진행합니다.\n1 2 3 4 5 6 7 $ sudo su $ cd ~ $ wget https://github.com/acmesh-official/acme.sh/archive/master.tar.gz $ tar xvf master.tar.gz $ cd acme.sh-master/ $ ./acme.sh --install --nocron --home /usr/local/share/acme.sh --accountemail \u0026#34;email@gmailcom\u0026#34; $ source ~/.profile 이제 인증서 생성을 진행해 볼텐데요.\n아래의 환경 변수들을 만들어 줍니다.\n1 2 3 4 # 발급해둔 API Token export CF_Token=\u0026#34;MY_SECRET_TOKEN_SUCH_SECRET\u0026#34; # CloudFlare Email export CF_Email=\u0026#34;myemail@example.com\u0026#34; 아래의 명령어들을 실행해서 인증서를 생성해보겠습니다.\n1 2 3 4 5 6 $ cd /usr/local/share/acme.sh # 여기에 사용할 Domain을 지정 해줍니다. # Ex) *.my-domain.com $ export CERT_DOMAIN=\u0026#34;your-domain.tld\u0026#34; $ export CERT_DNS=\u0026#34;dns_cf\u0026#34; $ /usr/local/share/acme.sh/acme.sh --issue --home . -d \u0026#34;$CERT_DOMAIN\u0026#34; --dns \u0026#34;$CERT_DNS\u0026#34; 위의 마지막 명렁어를 실행했을때, 에러없이 마무리되었다면 정상적으로 된 것입니다.\n만약 email을 update하라는 문구가 출력된다면 아래처럼 진행해주세요.\n1 $ /usr/local/share/acme.sh//usr/local/share/acme.sh/acme.sh --register-account -m 이메일 --issue --home . -d \u0026#39;*.도메인\u0026#39; --dns \u0026#34;$CERT_DNS\u0026#34; 혹여나 문제가 생긴다면 댓글로 문의주세요.\n기본 인증서 교체 신규로 생성한 인증서를 기본 인증서로 교체해보겠습니다.\n1 2 3 4 5 $ cd /usr/local/share/acme.sh $ export SYNO_Username=\u0026#39;관리자 계정\u0026#39; $ export SYNO_Password=\u0026#39;관리자 비밀번호\u0026#39; $ export SYNO_Certificate=\u0026#34;인증서 설명\u0026#34; $ /usr/local/share/acme.sh/acme.sh --deploy --home . -d \u0026#34;$CERT_DOMAIN\u0026#34; --deploy-hook synology_dsm Success가 출력된다면, 정상적으로 마무리된 것입니다.\n시놀로지 제어판-\u0026gt; 보안 -\u0026gt; 인증서로 들어가서 잘 등록되었는지 확인해봅니다.\n또는 브라우저에서 DSM에 접속한 뒤 새로고침해서 인증서 정보를 확인해주세요.\n만약 OTP를 설정해두셨다면, 위 과정을 진행하면서 SYNO_TOTP_SECRET 값을 설정해주라는 출력과 함께 에러가 발생할 것입니다.\n관리자 계정으로 로그인 할때, OTP 인증을 거치게 설정을 해두었기때문에 당연하다고 볼 수 있습니다. 출력된 메시지대로 값을 지정해주는 것도 방법이겠지만, 중요한 정보이기 때문에 선뜻 작성하기엔 내키지 않습니다. (관리자 뚫리면 어차피 소용없지만, 괜히 그렇더라구요..;;)\n이를 대체할 수 있는 값이 있는데, SYNO_DID 입니다.\nDeviceID값을 의미합니다. 이 값을 얻기위해서는 DSM에 로그인시 Remember this device에 체크한 후, 관리자 계정으로 로그인하면 Cookie로 저장됩니다.\n개발자 모드로 들어가서 이 값을 복사한뒤, 환경변수로 지정해 줍니다.\n위의 설명한 방식은 http 프로토콜을 사용하여 localhost에 인증서 교체 요청을 합니다.\n특별히 문제될건 없지만, 혹~~~~~시나 무조건 https 프로토콜을 사용해야한다!!! 라고 생각하시는 분이 계시다면\n아래의 2개 환경변수를 지정해준뒤, --insecure argument를 추가한뒤 실행해주면 https 프로토콜을 사용하게 됩니다.\n1 2 3 4 # 앞서 지정한 환경변수도 필요로 합니다. $ export SYNO_Scheme=\u0026#34;https\u0026#34; $ export SYNO_Port=\u0026#34;5001\u0026#34; $ /usr/local/share/acme.sh/acme.sh --deploy --insecure --home . -d \u0026#34;$CERT_DOMAIN\u0026#34; --deploy-hook synology_dsm 인증서 갱신 스케줄러 설정 인증서 유효기간이 3개월이기 때문에, 3개월마다 갱신작업을 해주어야합니다.\n하지만 이 작업을 매번 수행하기엔\u0026hellip; 놀랍도록 귀찮습니다.\n만료일에 가까워지면 갱신해야되는 초조함은 덤 입니다.\n이 번거로운 일에서 벗어나기 위해 자동화를 하겠습니다.\n제어한 -\u0026gt; 서비스 -\u0026gt; 작업 스케줄러 -\u0026gt; 생성 -\u0026gt; 예약된 작업 -\u0026gt; 사용자 정의 스크립트를 순서대로 선택합니다.\n이미지로는 아래와 같습니다. 빨간원 숫자 순서대로 진행합니다.\n하나의 팝업창 나타날텐데요.\n이미지처럼 3가지 값을 확인해줍니다.\nDSM6 버전까지는 root로 진행해야 인증서 교체가 가능했는데요.\n7 버전에서 다른계정으로 실행해도 되는지 테스트 해보진 않았습니다.\n다음으로는 언제실행할지 스케줄을 지정할 차례입니다.\n이미지처럼 설정할 경우 매주 토요일 오전 1시에 갱신을 시도하려고 할겁니다.\n선호하는 요일, 시간을 지정해주세요.\n매달 실행할 수 있게 변경 가능하지만, 갱신은 만료 1개월 전부터 가능합니다.\n첫번째이자 마지막 갱신 시도하는 날에 모종의 이유로 실패할 경우, 위에서 진행하셨던걸 다시해주셔야합니다.\n그래서 안전하게 매주 실행하도록 설정했습니다. 다음으로는 실행할 스크립트를 지정하는 단계입니다.\n알림 설정해두신 분이라면, 실행 상세정보를 이메일로 보내기를 체크해주면 좋습니다.\n잘 실행되는걸 메일 오는것으로 확인할 수 있을 테니까요.\n저의경우에는 메일 대신에 텔레그램으로 메시지 오도록 변경했습니다. 이와 관련해서는 따로 포스트 하겠습니다.\n사용자 정의 스크립트는 이미지처럼 인증서를 갱신하는 스크립트를 넣어줍니다.\n간단합니다. 한줄의 커맨드만 입력해주면 됩니다.\n1 /usr/local/share/acme.sh/acme.sh --cron --home /usr/local/share/acme.sh/ 이제 설정 완료하시면 아래의 같은 팝업이 출력됩니다.\n사뿐히 확인버튼 눌러주시면 됩니다.\n알림 외에도 실행한 로그를 확인하고 싶으실수 있는데요.\n아래 이미지처럼 로그 경로를 설정할 수 있습니다. 저의경우에 log 디렉토리로 지정했습니다.\n이와같이 지정했을 경우 스케줄러 로그는 /volume1/log/synoscheduler 경로에 저장됩니다.\n우리가 실행한 작업의 로그는 /volume1/log/synoscheduler/${task_number}/${실행시간} 경로에서 확인하실수 있습니다.\ntask_number는 스케줄러가 몇번째로 만들어졌는지 의미하는 값으로 보입니다.\noutput.log는 실행하며 출력된 결과이고, script.log는 실행한 스크립트를 볼 수 있습니다.\n정리 왜 이렇게 까지 해야되나 궁금해 하실수도 있습니다.\n집에서만 사용하도록 하신경우라면 이 과정 전혀 필요없습니다. 왜냐하면 외부에서 접근이 불가능하니까요.\n하지만 외부에서 접근을 허용하는 경우라면 이야기가 달라집니다.\n설정을 해두신 시점 이후로, 수시로 타국가에서 접근을 시도합니다. 보안 허점을 찾으려고 하는 거죠. http 통신은 보안상 문제가 있기 때문에, 이 상황에서 http를 사용해서 외부에서 접근하면\u0026hellip; 더 이야기 안해도 아실거라 생각합니다.\n이러한 이유로 https를 사용하게된거고, 최소한의 방어수단을 갖춘 셈입니다.\n여튼 여기까지 진행하셨다면, CloudFlare의 Domain에 TLS인증서 기반의 https를 사용할 수 있는 환경을 만드신 겁니다. 진행하느라 고생 많으셨습니다.👏👏\n저도 이번에 새롭게 설정하다보니, 자동 갱신이 잘 되는지는 확인하진 못했습니다.\n2개월 뒤에 내용을 보충 하도록 하겠습니다.\n","date":"2022-11-21T22:23:00+09:00","permalink":"https://korcasus.github.io/p/automate-synology-lets-encrypt-certificate/","title":"Automate Synology Let's Encrypt Certificate"},{"content":"Hugo Blog 만들기-2 개요 지난번 글을 통해, Hugo로 Blog를 만들어 보았습니다.\n이번에는 블로그 운영하는데 있어, 필수라고 생각되는 내용들을 적용해보도록 하겠습니다.\nCustom 설정하기 Stack 테마를 Git SubModule로 등록하여 블로그를 만들었습니다.\n사용하는건 좋지만 입맛에 맞게 수정하려면 테마를 변경해 주어야합니다.\n하지만 직접 수정한다면 테마를 업데이트할때마다 충돌이 발생할테고, SubModule로 등록한 의미가 퇴색 됩니다. 이를 방지하기 위한 방법을 설명하겠습니다.\nthemes/hugo-theme-stack/layouts/partials/header.html 파일을 수정해야하는 상황을 가정해보겠습니다. 이 파일을 layouts/partials/header.html로 복사한뒤, 원하는 대로 수정합니다.\nbuild를 할 경우 복사해온 파일을 사용하게 됩니다.\nasset과 static도 동일합니다.\n테마에 사용된 동일한 디자인의 아이콘을 추가할때 위와같은 방법으로 진행할 수 있습니다.\n좀 더 세심히 수정해보고 싶으시다면, 이 글을 참고하시면 큰 도움이 될것 같습니다.\nGoogle 검색 노출 해당 글을 참고하였습니다 구글 검색 결과에 노출되기 위해서는, 구글 검색엔진에 우리 홈페이지도 검색되게해줘~~ 라고 요청을 해야합니다.\n이 작업은 홈페이지 소유자가 Google Search Console 요청할 수 있습니다.\n홈페이지 형태에 따라 소유자임을 증명하는 방법이 달라집니다.\n시작하기를 누르면 아래와 같은 페이지가 나타납니다. URL 접두어에 Github blog 주소를 작성해줍니다. 도메인을 만들어서 운영중이시라면 도메인으로 생성해주세요.\nURL 접두어로 신청했다면, 해당 페이지의 소유자인지 확인해 주어야 합니다. HTML파일을 다운로드하여, static 디렉토리에 추가해줍니다. build하면 자동으로 public 디렉토리에 포함됩니다.\n그리고 public 디렉토리에 sitemap.xml이 존재해야 합니다.\n만약 없다면, build를 한번 해주시면 됩니다.\n\u0026lt;계정\u0026gt;.github.io/sitemap.xml로 접속했을때, 어떤 결과들이 나온다면 정상적으로 된 겁니다.\n검색 노출되기까지는 시간이 좀 걸리니, 느긋하게 기다린뒤 Search Console에서 확인해 봅시다.\nGoogle Analytics 적용 Github 으로 블로그를 운영할 경우, 평균 방문자 수가 얼마인지 간단한 통계조차도 볼 수 없습니다. 이러한 지표는 블로그 운영하는데 있어 재미(?)를 느끼게 하는데 큰 역할을 한다고 생각합니다.\n이런 제약사항을 해결하고, 블로그에 접속하는 사용자들을 분석(?)해보고 싶어 Google Analytics 를 적용해보았습니다.\n해당 글을 참고하였습니다 진행하시기 전에, 광고차단을 해제해두시는것을 추천합니다. (적용되었는지 확인하는데 문제가 생길 수 있습니다.) Google Analytics 신청 상세한 신청 과정을 설명하자니, 이미 진행해버려 동일하게 설명하긴 어려울 것 같습니다.\n개인정보도 포함되어있구요.\n대신 저보다 더 상세히 작성해주신 분들도 많으니, 참고해서 신청해주시면 됩니다.\n애널리틱스 계정, 속성 및 앱 까지 모두 신청하셨다면 사전 준비는 모두 되었다고 생각하시면 됩니다.\n이제는 analytics가 적용된 script와 GA Tracking ID를 확인해 보겠습니다.\n애널리틱스를 적용할 속성 및 앱의 데이터스트림을 생성해 줍니다.\n생성해준 뒤 웹 스트림 세부정보를 들어가면 아래와 같은 이미지를 보실 수 있습니다.\n측정 ID 아래에 적힌 값이 GA Tracking ID를 의미합니다.\nscript는 빨간색 화살표를 클릭해보면 확인할 수 있습니다.\n클릭해보면 아래와 이미지의 섹션에서 확인할 수 있습니다.\n모자이크 한 부분이 analytics를 적용할 수 있는 script입니다.\nScript와 GA Tracking ID 잘 저장해주세요.\nInternal Template을 사용하여 Google Analytics 활성화 Hugo 공식 Guide에서는 config.yaml에 설정만 해준다면, 손쉽게 적용할수 있다고 안내 합니다. 설정 하는방법은 아래와 같습니다.\n1 googleAnalytics: G-MEASUREMENT_ID blog에 배포한 뒤 실시간 접속자 counting이 되는지 확인해 봅니다. 뒤에서 설명하겠지만, 홈 화면에서 실시간 접속자 수를 확인할 수 있습니다. 이 단계에서 잘 적용되었다면, Script 수동 설치는 진행하지 않아도 됩니다.\nScript 수동 설치 신청하며 복사해두었던 script를 layout에 추가하여 적용합니다.\nscript를 복사하며 잘 살펴보셨다면 아시겠지만 head 태그에 추가해야된다고 안내하고 있습니다.\n사용중인 테마의 head 태그 부분을 확인해보겠습니다. (stack 테마를 기준으로 설명합니다.)\n우선 블로그를 구성하는 가장 상위(?)의 html을 확인해보겠습니다. themes/hugo-theme-stack/layouts/_default/baseof.html 입니다.\n현 기준으로는 아래처럼 구성되어있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 \u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html lang=\u0026#34;{{ .Site.LanguageCode }}\u0026#34; dir=\u0026#34;{{ default `ltr` .Language.LanguageDirection }}\u0026#34;\u0026gt; \u0026lt;head\u0026gt; {{- partial \u0026#34;head/head.html\u0026#34; . -}} {{- block \u0026#34;head\u0026#34; . -}}{{ end }} \u0026lt;/head\u0026gt; \u0026lt;body class=\u0026#34;{{ block `body-class` . }}{{ end }}\u0026#34;\u0026gt; {{- partial \u0026#34;head/colorScheme\u0026#34; . -}} {{/* The container is wider when there\u0026#39;s any activated widget */}} {{- $hasWidget := false -}} {{- range .Site.Params.widgets -}} {{- if gt (len .) 0 -}} {{- $hasWidget = true -}} {{- end -}} {{- end -}} \u0026lt;div class=\u0026#34;container main-container flex on-phone--column {{ if $hasWidget }}extended{{ else }}compact{{ end }}\u0026#34;\u0026gt; {{- block \u0026#34;left-sidebar\u0026#34; . -}} {{ partial \u0026#34;sidebar/left.html\u0026#34; . }} {{- end -}} \u0026lt;main class=\u0026#34;main full-width\u0026#34;\u0026gt; {{- block \u0026#34;main\u0026#34; . }}{{- end }} \u0026lt;/main\u0026gt; {{- block \u0026#34;right-sidebar\u0026#34; . -}}{{ end }} \u0026lt;/div\u0026gt; {{ partial \u0026#34;footer/include.html\u0026#34; . }} \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; head 태그 안에 추가하기위해 head/head.html를 확인해보겠습니다.\nthemes/hugo-theme-stack/layouts/partials/head/head.html을 보시면 됩니다.\n아래처럼 만들어져 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 \u0026lt;meta charset=\u0026#39;utf-8\u0026#39;\u0026gt; \u0026lt;meta name=\u0026#39;viewport\u0026#39; content=\u0026#39;width=device-width, initial-scale=1\u0026#39;\u0026gt; {{- $description := partialCached \u0026#34;data/description\u0026#34; . .RelPermalink -}} \u0026lt;meta name=\u0026#39;description\u0026#39; content=\u0026#39;{{ $description }}\u0026#39;\u0026gt; {{- $title := partialCached \u0026#34;data/title\u0026#34; . .RelPermalink -}} \u0026lt;title\u0026gt;{{ $title }}\u0026lt;/title\u0026gt; \u0026lt;link rel=\u0026#39;canonical\u0026#39; href=\u0026#39;{{ .Permalink }}\u0026#39;\u0026gt; {{- partial \u0026#34;head/style.html\u0026#34; . -}} {{- partial \u0026#34;head/script.html\u0026#34; . -}} {{- partial \u0026#34;head/opengraph/include.html\u0026#34; . -}} {{- range .AlternativeOutputFormats -}} \u0026lt;link rel=\u0026#34;{{ .Rel }}\u0026#34; type=\u0026#34;{{ .MediaType.Type }}\u0026#34; href=\u0026#34;{{ .Permalink | safeURL }}\u0026#34;\u0026gt; {{- end -}} {{ with .Site.Params.favicon }} \u0026lt;link rel=\u0026#34;shortcut icon\u0026#34; href=\u0026#34;{{ . }}\u0026#34; /\u0026gt; {{ end }} {{- template \u0026#34;_internal/google_analytics.html\u0026#34; . -}} {{- partial \u0026#34;head/custom.html\u0026#34; . -}} script를 추가해야되니까 head/script.html에 추가하면 좋을것으로 보이네요. 실제로 파일을 확인해보겠습니다. 경로는 themes/hugo-theme-stack/layouts/partials/head/head.html 입니다.\n열어보면 아시겠지만, 비어있습니다.\n무슨 내용이 있었다면 custom하며 보완해주어야겠지만, 비어있으므로 걱정할 필요가 없습니다.\nlaygouts/partials/head/script.html을 만들어, 복사해둔 스크립트를 추가해주면 됩니다.\n적용 확인 애널리틱스 홈 화면을 들어갑니다.\n정상 적용되었다면, 지난 30분 동안의 사용자에 counting 됩니다.\n정리 첫번째 글로 Hugo로 Static Site Generate, 테마적용, 댓글 시스템 연동까지 진행해보았고 이번 글을 통해 블로그를 고도화(Custom 방법, 구글 검색노출, 구글 analytics) 해보았습니다.\n다음 글은 애드센스와 github action 적용을 주제로 작성할 예정입니다.\n감사합니다.\n","date":"2022-11-15T20:23:45+09:00","permalink":"https://korcasus.github.io/p/hugo-blog-customize/","title":"Hugo Blog Customize하기"},{"content":"Hugo Blog 만들기-1 Hugo란? Go로 구현된 빠르고 현대적인 Static Site Generator입니다.\nVisitor Request가 발생할때마다 동적으로 페이지를 생성하는 시스템과는 다르게, Content를 만들거나 업데이트할때 Build가 됩니다. 사전에 Build된 Page를 보여주기 때문에, viewer에게 최적의 경험을 제공해줄 수 있습니다. Hosting에도 제약이 없으며, CDN에도 문제없이 동작합니다.\nHugo는 Database가 필요없고 Ruby, Python or PHP와 같은 Expensive Runtime에 Dependency가 없습니다.\n특징을 정리해보면 아래와 같습니다.\nBuild가 극단적으로 빠르다. (페이지마다 1ms 이하) cross platform 지원 liveReload지원 강력한 테마 어디서든 Hosting할 수 있다. … 상세한 내용은 Hugo 문서를 참고해주세요.\nHugo 설치 Mac을 주력으로 사용하기 때문에, Mac 기준으로 설명하겠습니다.\n타 운영체제를 사용한다면, Reference를 참고해서 설치를 진행해주세요.\nPrerequisites Git Hugo module Git submodule … GoLang ≥ 1.18 Go를 사용하며, Git을 통해 Submodule을 관리하기때문에 위의 2개를 사전에 설치해주어야합니다.\n1 2 3 4 5 6 # Hombrew Package manager 사용 $ brew install hugo # 정상적으로 설치되었는지 확인 $ hugo version hugo v0.104.3+extended darwin/arm64 BuildDate=unknown Apple Silicon을 사용중이라면, 위처럼 darwin/arm64로 나타나는지 확인해줍니다.\nGithub repository 2개 생성 User Github Page로 사용할 Repository와 Markdown을 기록할 Repository를 구분하기 위한 용도로 나누어 사용하기 위해서 입니다.\n1개의 Repository로 통합해도 되지만, 작성해둔 content를 쉽게 copy해갈 수 있으므로 사전에 방지하기 위해 구분하여 사용하려 합니다.\n저의 경우에는 아래의 2개의 respository를 생성했습니다.\ntechnical-blog .github.io (ex. korcasus.github.io) Hugo로 프로젝트 만들기 Hugo를 사용해서 default project를 만들어줍니다.\n생성되는 프로젝트는 아래처럼 구성됩니다.\n1 2 3 4 5 6 7 8 9 10 11 12 $ hugo new site technical-blog $ tree . ├── archetypes ├── config.toml ├── content ├── data ├── layouts ├── public ├── static └── themes 자세하게 알아보고 사용하고 싶다면 Reference를 한번 읽어보는걸 권장합니다.\n블로그에 이쁜 옷을 입혀줄 차례입니다.\n본인이 직접 테마를 만들것이 아니라면, 공개된 테마를 사용하는게 좋습니다.\n다양한 테마가 존재하므로, 문서에서 찾아보고 본인에 맞는 테마를 사용하도록 합니다.\n저의 경우에는 Stack이 가장 이쁘고, 문서화가 잘 되어있다고 생각했습니다.\n그래서 Stack을 기준으로 블로그 구현한 것을 설명하겠습니다.\n1 2 3 4 5 $ cd technical-blog # 다른 테마의 경우 # git submodule add https://github.com/${테마Repository}.git themes/${테마이름} $ git submodule add https://github.com/CaiJimmy/hugo-theme-stack/ themes/hugo-theme-stack 위에처럼 Submodule로 추가해둔다면, 추후 테마 업데이트 하기가 쉬워집니다.\nHugo에 Module이라는 것도 있는데, 이 방법으로도 대체 가능합니다. 관심있다면 적용해보는것도 좋아보입니다.\n테마가 정상적으로 설치되었다면, 테마의 설정파일을 사용하기 위해 기본 config file을 복사해와야 합니다.\n${project}/themes/hugo-theme-stack/exampleSite/config.yaml 파일을 복사해 ${project}/config.yaml 에 옮겨줍니다.\nGithub Page로 사용하기위해서는 config.yaml의 일부를 변경해주어야 합니다.\n아래의 예시에 주석에 해당되는 부분을 변경해주도록 합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 # Github page로 사용할 URL로 변경해주어야합니다. baseurl: https://korcasus.github.io/ languageCode: en-us theme: hugo-theme-stack paginate: 5 title: Read Write languages: en: languageName: English title: Read Write weight: 1 disqusShortname: hugo-theme-stack ... 추가한 테마가 정상적으로 동작하는지 project 디렉토리에서 build 및 서버 실행을 해보도록 하겠습니다.\n이미 어느정도 blog를 만든뒤에 작성하는거라, 완전 아래와 동일하게 나오진 않지만 비슷하게 출력되었던 것으로 기억합니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 $ hugo server -D Start building sites … hugo v0.104.3+extended darwin/arm64 BuildDate=unknown | EN -------------------+----- Pages | 41 Paginator pages | 2 Non-page files | 4 Static files | 0 Processed images | 18 Aliases | 18 Sitemaps | 1 Cleaned | 0 Built in 1494 ms Watching for changes in /Users/user/Workspace/technical-blog/{archetypes,assets,content,data,layouts,static,themes} Watching for config changes in /Users/user/Workspace/technical-blog/config.yaml, /Users/user/Workspace/technical-blog/themes/hugo-theme-stack/config.yaml Environment: \u0026#34;development\u0026#34; Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop Build된 결과를 보기 위해 http://localhost:1313 으로 접속했을때 페이지가 보인다면, 여기까지는 잘 진행되었다고 판단하시면 됩니다.\nGit Repo 연결 및 ShellScipt 작성 Hugo로 만들어진 Project를 개인(Privarte) Repository에 등록하고, Hugo로 Build된 결과를 Github Page로 연동하는 작업입니다.\n그 뒤로는 배포에 사용하기 위한 ShellScript에 대해 설명하겠습니다.\n아래의 명령어들을 실행하면, 처음에 만들어둔 2개 Repository에 용도에 맞게 사용하게 됩니다.\n1 2 3 4 5 6 7 8 9 10 11 # 현재 위치 확인 $ pwd /Users/user/Workspaces/blog # blog -\u0026gt; blog 레포지토리 연결 # git remote add origin http://github.com/\u0026lt;username\u0026gt;/technical-blog.git $ git remote add origin http://github.com/korcasus/technical-blog.git # blog/public -\u0026gt; \u0026lt;username\u0026gt;.github.io 연결 # git submodule add -b master http://github.com/\u0026lt;username\u0026gt;/\u0026lt;username\u0026gt;.github.io.git public $ git submodule add -b master http://github.com/korcasus/korcasus.github.io.git public 발행할 Contents를 작성을 완료한 후, Hugo로 Build해주는 과정이 필요로 합니다.\nBuild된 결과물은 public 디렉토리에 생성됩니다. 이를 Github Page에 반영해주는 과정이 필요로 합니다.\n매번 따로 수행하기보다는 배포 스크립트로 만들어 실행하는것이 운영하기 쉽습니다.\n저의 경우에는 아래의 스크립트를 통해, 배포를 합니다. (Reference)\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 #!/bin/bash echo -e \u0026#34;\\033[0;32mDeploying updates to GitHub...\\033[0m\u0026#34; # Build the project. # hugo -t \u0026lt;여러분의 테마\u0026gt; hugo -t hugo-tranquilpeak-theme # Go To Public folder, sub module commit cd public # Add changes to git. git add . # Commit changes. msg=\u0026#34;rebuilding site `date`\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; # Push source and build repos. git push origin master # Come Back up to the Project Root cd .. # blog 저장소 Commit \u0026amp; Push git add . msg=\u0026#34;rebuilding site `date`\u0026#34; if [ $# -eq 1 ] then msg=\u0026#34;$1\u0026#34; fi git commit -m \u0026#34;$msg\u0026#34; git push origin master 1 2 3 4 5 # deploy.sh 실행 파일 권한 부여 $ chmod 777 deploy.sh # 배포 실행 $ ./deploy.sh Post 작성 발행할 글을 작성할때 아래처럼 명령어를 입력하면 Hugo에서 Markdown을 생성해줍니다.\n1 2 3 4 5 6 7 8 9 10 11 12 $ cd $project # hugo new ${content내 directory}/${사용할 파일명}.md $ hugo new post/hugo-blog-2/index.md Content \u0026#34;/Users/user/Workspace/technical-blog/content/post/hugo-blog-2/index.md\u0026#34; created $ cat content/post/hugo-blog-2/index.md --- title: \u0026#34;Hugo Blog 2\u0026#34; date: 2022-11-14T23:58:09+09:00 draft: true --- 주의해야할 점이 2가지 있습니다.\ncontent directory내에 생성된다는 점입니다. 확장자를 md로 해야합니다. 첫번째는 생성되는 경로를 정확히 파악하기 위해 주의해주는 것이 좋습니다.\n두번째는 확장자를 md로 하지않을 경우, 명령어로 post생성할때 에러가 발생합니다.\nHugo에서 archetypes 디렉토리를 포함하고있는데, default.md 파일이 존재합니다.\n명령어를 통해 확장자 md파일을 만들때, 기본값으로 사용됩니다.\n하지만 여기에 포함되지않은 확장자 파일을 만들경우, 기본값으로 사용할 파일이 존재하지않으므로 에러가 발생합니다. default.md 파일을 확인해보면 아래와 같습니다.\n1 2 3 4 5 --- title: \u0026#34;{{ replace .Name \u0026#34;-\u0026#34; \u0026#34; \u0026#34; | title }}\u0026#34; date: {{ .Date }} draft: true --- Utterences(Github 댓글 위젯) 연동 Stack 테마에서는 아래와 같은 댓글 시스템들을 지원합니다. (참고)\nCactus Cusdis Disqus DisqusJS Giscus Gitalk Remark42 Twikoo utterances Vssue Waline 이 중에 OpenSource이고, github과 연동하여 쉽게 사용가능한 Utterences를 사용합니다.\n사용하기 위해서는 별도의 Repository가 추가로 필요합니다. 여기서 사용한 Repo이름은 blog-comments 입니다.\n첨부한 GIF대로 진행하시면 됩니다. 잊지말고 utterances app연동 진행해주세요!\nBlog Post와 Issue Mapping 방식을 다르게 하고싶다면, 선호하시는 것으로 선택하시면 됩니다. 만들어진 Script를 테마에 적용해보도록 하겠습니다.\nblog/themes/${테마}/layouts/partials/comments/provider 경로에 사용하고자 하는 댓글 시스템에 직접 수정방법도 있습니다.\n하지만 이는 추후 테마 업데이트 하는데 문제가 발생할 수 있으므로 제외합니다.\n테마에서 제공하는 config.yaml를 수정해서 댓글 시스템에 적용해보겠습니다.\n프로젝트 루트 디렉토리에 복사해둔 config.yaml을 사용합니다. 아래처럼 comments를 사용할건지, 사용할 provider를 지정해줄 수 있습니다.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 params: comments: enabled: true provider: disqus disqusjs: shortname: apiUrl: apiKey: admin: adminLabel: utterances: repo: issueTerm: pathname label: 추출한 Script를 적용하면 아래처럼 됩니다.\n만약 Issue Mapping을 설명한 방법과 다르게 하셨다면 blog/themes/${테마}/layouts/partials/comments/provider/utterances.html 를 확인해서 Script에 맞게끔 값을 입력해주세요.\n1 2 3 4 5 6 7 params: comments: enabled: true provider: utterances utterances: repo: Korcasus/blog-comments issueTerm: title 정상적으로 만들어졌다면 게시글 하단에 아래와 같이 추가되어있는것을 보실 수 있습니다.\n댓글이 잘 작성되는지 확인해보겠습니다.\n로그인후 테스트 댓글을 작성해보았습니다.\n작성된 댓글이 Github Issue로 잘 등록되었습니다.\n정리 여기까지 Hugo로 Static Site Generate, 테마적용, 댓글 시스템 연동까지 진행해보았습니다.\n다음에는 만든 블로그를 고도화(Custom 방법, 구글 검색노출, 구글 analytics) 해보겠습니다.\n감사합니다.\n","date":"2022-11-14T23:11:35+09:00","permalink":"https://korcasus.github.io/p/hugo-blog-%EB%A7%8C%EB%93%A4%EA%B8%B0-1/","title":"Hugo Blog 만들기-1"}]